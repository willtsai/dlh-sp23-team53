{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of Context-aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs\n",
    "\n",
    "**UIUC, CS598 DL4H, Spring 2023**\n",
    "\n",
    "**Authors:** Shiyu (Sherry) Li and Wei-Lun (Will) Tsai; {shiyuli2, wltsai2}@illinois.edu\n",
    "\n",
    "**Original paper:** Chang Lu, Tian Han, and Yue Ning. 2022. [Context-aware Health Event Prediction\n",
    "via Transition Functions on Dynamic Disease\n",
    "Graphs.](https://arxiv.org/pdf/2112.05195.pdf) Proceedings of the AAAI\n",
    "Conference on Artificial Intelligence, 36(4):4567â€“4574.\n",
    "\n",
    "**Original codebase:** [github.com/LuChang-CS/Chet](https://github.com/LuChang-CS/Chet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess the data\n",
    "\n",
    "As a part of the initial setup described in the [README](https://github.com/willtsai/uiuc-cs598-dlh/blob/main/README.md), we have downloaded the raw data and placed it in the `data` directory. We will now preprocess the data to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_preprocess import pre_process\n",
    "\n",
    "pre_process(dataset_names=['mimic3','mimic4'], data_saved=False)\n",
    "print('***processing complete***')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Set hyperparameters and seed\n",
    "\n",
    "We keep the same hyperparameters and seed as the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Keep the same hyperparameters and seed as the original paper\n",
    "code_size = 48\n",
    "graph_size = 32\n",
    "hidden_size = 150  # rnn hidden size\n",
    "t_attention_size = 32\n",
    "t_output_size = hidden_size\n",
    "batch_size = 32\n",
    "seed = 6669\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Config for hardware to use\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model\n",
    "\n",
    "Here we implement the model and its layers as described in the paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Optimized dynamic graph layer\n",
    "\n",
    "Here we define the optimized dynamic graph layer for the model. This layer performs the following steps:\n",
    "- Aggregate global/local context with the optimized graph layer with the embedding matrices\n",
    "- Calculate hidden embeddings for diagnoses and neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, adj, code_num, code_size, graph_size):\n",
    "        super().__init__()\n",
    "        self.embedding =  nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, code_size)))\n",
    "        self.adj = adj \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(code_size, graph_size)\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, code_x, neighbor):\n",
    "        # embedding matrices for for diseases appearing in current diagnoses\n",
    "        # M_embedding_matrices = self.embedding(code_x)\n",
    "        # embedding matrices for for diseases appearing in direct neighbors\n",
    "        # N_embedding_matrices = self.embedding(neighbor)\n",
    "        # static adjacency matrix\n",
    "        # keep these unsqueeze for now, may need change if we change the data loader\n",
    "        center_codes = torch.unsqueeze(code_x, dim=-1)\n",
    "        neighbor_codes = torch.unsqueeze(neighbor, dim=-1)\n",
    "\n",
    "        center_embeddings = center_codes * self.embedding\n",
    "        neighbor_embeddings = neighbor_codes * self.embedding\n",
    "\n",
    "        adj_mul_center = torch.matmul(self.adj, center_embeddings)\n",
    "        adj_mul_neighbor = torch.matmul(self.adj, neighbor_embeddings)\n",
    "\n",
    "        # All the calculation here are using the memory-efficient calculation as proved by the author in Subgraphs' Adjacency Matrix Calculation\n",
    "        # aggregated diagnosis local context and diagnosis global context\n",
    "        aggregated_diagnosis_embedding = center_embeddings + center_codes * adj_mul_center + center_codes * adj_mul_neighbor\n",
    "        # aggregated neighbor global context\n",
    "        aggregated_neighbor_embedding = neighbor_embeddings + neighbor_codes * adj_mul_neighbor + neighbor_codes * adj_mul_center\n",
    "\n",
    "        # hidden embeddings of diagnoses and neighbors\n",
    "        hidden_diagnosis_embedding = self.LeakyReLU(self.fc(aggregated_diagnosis_embedding))\n",
    "        hidden_neighbor_embedding = self.LeakyReLU(self.fc(aggregated_neighbor_embedding))\n",
    "        return hidden_diagnosis_embedding, hidden_neighbor_embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Transition functions layer\n",
    "\n",
    "Here we define the transition functions layer for the model. The hidden embeddings from the optimized dynamic graph layer are used as inputs to this layer. This layer includes GRU, M-GRU (customized GRU for matrices), and single headed attention functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SingleHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, attention_size):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        self.query_dense = nn.Linear(query_size, attention_size)\n",
    "        self.key_dense = nn.Linear(key_size, attention_size)\n",
    "        self.value_dense = nn.Linear(query_size, value_size)\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        query = self.query_dense(q)\n",
    "        key = self.key_dense(k)\n",
    "        value = self.value_dense(v)\n",
    "        attention = torch.matmul(query, key.T) / math.sqrt(self.attention_size)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        output = torch.matmul(attention, value)\n",
    "        return output\n",
    "    \n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, code_num, code_size, graph_size, hidden_size, t_attention_size, t_output_size):\n",
    "        super().__init__()\n",
    "        self.unrelated_embedding = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, graph_size)))\n",
    "        self.gru = nn.GRUCell(input_size=graph_size, hidden_size=hidden_size)\n",
    "        self.attention = SingleHeadAttentionLayer(graph_size, graph_size, t_output_size, t_attention_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        self.code_num = code_num\n",
    "        self.hidden_size = hidden_size\n",
    "        self.code_size = code_size\n",
    "\n",
    "    def forward(self, t, co_embeddings, divided, no_embeddings, hidden_state=None):\n",
    "        m_p, m_en, m_eu = divided[:, 0], divided[:, 1], divided[:, 2]\n",
    "        mp_idx, men_idx, meu_idx = torch.where(m_p > 0)[0], torch.where(m_en > 0)[0], torch.where(m_eu > 0)[0]\n",
    "        h_new = torch.zeros((self.code_num, self.hidden_size), dtype=co_embeddings.dtype).to(co_embeddings.device)\n",
    "        output_mp = 0\n",
    "        output_meneu = 0\n",
    "\n",
    "        if len(mp_idx) > 0:\n",
    "            h = hidden_state[mp_idx] if hidden_state is not None else None\n",
    "            h_p = self.gru(co_embeddings[mp_idx], h)\n",
    "            h_new[mp_idx] = h_p\n",
    "            output_mp, _ = torch.max(h_p, dim=-2)\n",
    "        if t == 0 or len(men_idx) + len(meu_idx) == 0:\n",
    "            output = output_mp\n",
    "        else:\n",
    "            q = torch.vstack([no_embeddings[men_idx], self.unrelated_embedding[meu_idx]])\n",
    "            v = torch.vstack([co_embeddings[men_idx], co_embeddings[meu_idx]])\n",
    "            h_tilda = self.activation(self.attention(q, q, v))\n",
    "            h_new[men_idx] = h_tilda[:len(men_idx)]\n",
    "            h_new[meu_idx] = h_tilda[len(men_idx):]\n",
    "            output_meneu, _ = torch.max(h_tilda, dim=-2)\n",
    "            if len(mp_idx) == 0:\n",
    "                output = output_meneu\n",
    "            else:\n",
    "                output, _ = torch.max(torch.vstack([output_mp, output_meneu]), dim=-2)\n",
    "\n",
    "        return output, h_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Embedding layer\n",
    "\n",
    "Here we define the embedding layer for the model, combined with the dot product attention activation for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingWithAttentionLayer(nn.Module):\n",
    "    def __init__(self, value_size, attention_size):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        # define context vector\n",
    "        self.context = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(attention_size, 1)))\n",
    "        self.linear = nn.Linear(value_size, attention_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max pooling\n",
    "        t = self.linear(x)\n",
    "        # calculate attention score\n",
    "        score = torch.softmax(torch.matmul(t, self.context).squeeze(), dim=-1)\n",
    "        # final hidden embedding\n",
    "        output = torch.sum(x * torch.unsqueeze(score, dim=-1), dim=-2)\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Model and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.activation(self.dropout(self.linear(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still need further editing to integrate\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, code_num, code_size,\n",
    "                 adj, graph_size, hidden_size, t_attention_size, t_output_size,\n",
    "                 output_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.graph_layer = GraphLayer(adj, code_num, code_size, graph_size)\n",
    "        self.transition_layer = TransitionLayer(code_num, code_size, graph_size, hidden_size, t_attention_size, t_output_size)\n",
    "        self.attention = EmbeddingWithAttentionLayer(hidden_size, 32)\n",
    "        self.classifier = Classifier(hidden_size, output_size, dropout_rate)\n",
    "\n",
    "    def forward(self, code_x, divided, neighbors, lens):\n",
    "        output = []\n",
    "        for code_x_i, divided_i, neighbor_i, len_i in zip(code_x, divided, neighbors, lens):\n",
    "            no_embeddings_i_prev = None\n",
    "            output_i = []\n",
    "            h_t = None\n",
    "            for t, (c_it, d_it, n_it, len_it) in enumerate(zip(code_x_i, divided_i, neighbor_i, range(len_i))):\n",
    "                co_embeddings, no_embeddings = self.graph_layer(c_it, n_it)\n",
    "                output_it, h_t = self.transition_layer(t, co_embeddings, d_it, no_embeddings_i_prev, h_t)\n",
    "                no_embeddings_i_prev = no_embeddings\n",
    "                output_i.append(output_it)\n",
    "            output_i = self.attention(torch.vstack(output_i))\n",
    "            output.append(output_i)\n",
    "        output = torch.vstack(output)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define functions for training and evaluation\n",
    "\n",
    "#### 4.1 Historical hot function\n",
    "\n",
    "We re-use the `historical_hot()` function directly from the [original codebase](https://github.com/LuChang-CS/Chet/blob/master/train.py). The function will be used later in model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_hot(code_x, code_num, lens):\n",
    "    result = np.zeros((len(code_x), code_num), dtype=int)\n",
    "    for i, (x, l) in enumerate(zip(code_x, lens)):\n",
    "        result[i] = x[l - 1]\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data loader function\n",
    "\n",
    "We create a data_loader() function to load the data needed for training and evaluating the model. The function is based on the [data loding code](https://github.com/LuChang-CS/Chet/blob/master/train.py#L45-L52) from the original authors and also re-uses several of the data loading helper functions from [`utils.py`](https://github.com/LuChang-CS/Chet/blob/master/utils.py) in the original codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import load_adj\n",
    "from utils import EHRDataset\n",
    "from utils import MultiStepLRScheduler\n",
    "\n",
    "def data_loader(task, dataset_path):\n",
    "    print('from {} for task {}:'.format(dataset_path, task))\n",
    "    print('loading code adjacency matrix ...')\n",
    "    code_adj = load_adj(dataset_path, device=device)\n",
    "    code_num = len(code_adj)\n",
    "    print('loading train data ...')\n",
    "    train_data = EHRDataset(os.path.join(dataset_path, \"train/\"), label=task, batch_size=batch_size, shuffle=True, device=device)\n",
    "    print('loading valid data ...')\n",
    "    valid_data = EHRDataset(os.path.join(dataset_path, \"valid/\"), label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "    print('loading test data ...')\n",
    "    test_data = EHRDataset(os.path.join(dataset_path, \"test/\"), label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "\n",
    "    return {\n",
    "        'dataset_name': dataset_path.split('/')[1],\n",
    "        'code_adj': code_adj, \n",
    "        'code_num': code_num, \n",
    "        'train_data': train_data, \n",
    "        'valid_data': valid_data, \n",
    "        'test_data': test_data, \n",
    "        }\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Model training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils import format_time\n",
    "\n",
    "def train_chet(path, task, output_size, evaluate_fn, code_adj, code_num, dropout_rate, \n",
    "               train_data, valid_data, init_lr, lrs, milestones, epochs, test_historical):\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    \n",
    "    # Keep the same model param storage path as the original paper\n",
    "    param_path = os.path.join('data', 'params', path, task)\n",
    "    if not os.path.exists(param_path):\n",
    "        os.makedirs(param_path)\n",
    "\n",
    "    # Keep the same model, optimizer, and scheduler as the original paper,\n",
    "    # but slightly modified to leverage the new config dict\n",
    "    model = Model(code_num=code_num, code_size=code_size,\n",
    "                    adj=code_adj, graph_size=graph_size, hidden_size=hidden_size, t_attention_size=t_attention_size,\n",
    "                    t_output_size=t_output_size,\n",
    "                    output_size=output_size, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = MultiStepLRScheduler(optimizer, epochs, init_lr, milestones, lrs)\n",
    "\n",
    "    # Keep the same param printing code as the original paper\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(pytorch_total_params)\n",
    "\n",
    "    # Keep the same training loop code as the original paper, but note that\n",
    "    # the train, valid, and test data will change based on the task and\n",
    "    # dataset of the current loop\n",
    "    epoch_lrs, valid_losses, mean_losses, time_costs, f1_scores, auc_or_topks = [], [], [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch %d / %d:' % (epoch + 1, epochs))\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_num = 0\n",
    "        steps = len(train_data)\n",
    "        st = time.time()\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.lrs[epoch]\n",
    "        for step in range(len(train_data)):\n",
    "            optimizer.zero_grad()\n",
    "            code_x, visit_lens, divided, y, neighbors = train_data[step]\n",
    "            output = model(code_x, divided, neighbors, visit_lens).squeeze()\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * output_size * len(code_x)\n",
    "            total_num += len(code_x)\n",
    "\n",
    "            end_time = time.time()\n",
    "            remaining_time = format_time((end_time - st) / (step + 1) * (steps - step - 1))\n",
    "            print('\\r    Step %d / %d, LR: %s, remaining time: %s, loss: %.4f'\n",
    "                % (step + 1, steps, current_lr, remaining_time, total_loss / total_num), end='')\n",
    "        train_data.on_epoch_end()\n",
    "        et = time.time()\n",
    "        time_cost = format_time(et - st)\n",
    "        mean_loss = total_loss / total_num\n",
    "        print('\\r    Step %d / %d, LR: %s, time cost: %s, loss: %.4f' % (steps, steps, current_lr, time_cost, mean_loss))\n",
    "        valid_loss, f1_score, auc_or_topk = evaluate_fn(model, valid_data, loss_fn, output_size, test_historical)\n",
    "        torch.save(model.state_dict(), os.path.join(param_path, '%d.pt' % epoch))\n",
    "        epoch_lrs.append(current_lr)\n",
    "        valid_losses.append(valid_loss)\n",
    "        mean_losses.append(mean_loss)\n",
    "        time_costs.append(time_cost)\n",
    "        f1_scores.append(f1_score)\n",
    "        auc_or_topks.append(auc_or_topk)\n",
    "        \n",
    "    return {\n",
    "        'model': model,\n",
    "        'epoch_lrs': epoch_lrs,\n",
    "        'valid_losses': valid_losses,\n",
    "        'mean_losses': mean_losses,\n",
    "        'time_costs': time_costs,\n",
    "        'f1_scores': f1_scores,\n",
    "        'auc_or_topks': auc_or_topks,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Model evaluation function\n",
    "We create a `test()` function to evaluate the model on the `test_data`. We re-use the `evaluate_codes()` and `evaluate_hf()` functions directly from the original [metrics.py](https://github.com/LuChang-CS/Chet/blob/master/metrics.py) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(evaluate_fn, model, test_data, loss_fn, output_size, test_historical):\n",
    "    print(\"Evaluating model on test data...\")\n",
    "    model.eval()\n",
    "    test_loss, f1_score, auc_or_topk = evaluate_fn(model, test_data, loss_fn, output_size, historical=test_historical)\n",
    "    print(\"Test loss: %s, F1 score: %s, AUC or TopK: %s\" % (test_loss, f1_score, auc_or_topk))\n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'f1_score': f1_score,\n",
    "        'auc_or_topk': auc_or_topk,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load the preprocessed data\n",
    "\n",
    "Here we load the preprocessed data using the data_loader() function defined above, for both the MIMIC-III and MIMIC-IV datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Add data analysis here to explain each sub datasets eg: code_x, visit_lens, divided, y, neighbors, code_adj. Also can give an example by printing some data\n",
    "# Todo: Add some initial data analysis. 1) number/ratio of heart failure patients. 2) some statistic for neighbors\n",
    "# Possible Todo: build our own data builder using torch.utils.data.DataLoader as HWs\n",
    "\n",
    "tasks = ['h', 'm']\n",
    "mimic4_standard_path = \"data/mimic4/standard/\"\n",
    "mimic3_standard_path = \"data/mimic3/standard/\"\n",
    "mimic3_datasets, mimic4_datasets = {}, {}\n",
    "for task in tasks:\n",
    "    mimic3_datasets[task] = data_loader(task, mimic3_standard_path)\n",
    "    mimic4_datasets[task] = data_loader(task, mimic4_standard_path)\n",
    "print(\"data loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import evaluate_codes, evaluate_hf\n",
    "\n",
    "epochs = 20 # epochs = 200 in original paper\n",
    "\n",
    "train_results = {}\n",
    "test_results = {}\n",
    "\n",
    "for task in tasks:\n",
    "    dropout_rate_ = 0.45 if task == 'm' else 0.0\n",
    "    lrs_ = [1e-3, 1e-5] if task == 'm' else [1e-3, 1e-4, 1e-5]\n",
    "    milestones_ = [15, 17] if task == 'm' else [2,3,4] # [20, 30] [2, 3, 20]\n",
    "    evaluate_fn_ = evaluate_codes if task == 'm' else evaluate_hf\n",
    "    for dataset in [mimic3_datasets, mimic4_datasets]:\n",
    "        output_size_ = dataset[task]['code_num'] if task == 'm' else 1\n",
    "        print('training for %s task on %s dataset:' % (task, dataset[task]['dataset_name']))\n",
    "        train_data_ = dataset[task]['train_data']\n",
    "        valid_data_ = dataset[task]['valid_data']\n",
    "        test_data_ = dataset[task]['test_data']\n",
    "        train_results[dataset[task]['dataset_name'] + \"-\" + task] = train_chet(\n",
    "            path=dataset[task]['dataset_name'],task=task, output_size=output_size_, \n",
    "            evaluate_fn=evaluate_fn_, code_adj=dataset[task]['code_adj'], code_num=dataset[task]['code_num'], \n",
    "            dropout_rate=dropout_rate_, train_data=train_data_, valid_data=valid_data_, \n",
    "            init_lr=0.01, lrs=lrs_, milestones=milestones_, epochs=epochs, \n",
    "            test_historical=historical_hot(valid_data_.code_x, dataset[task]['code_num'], valid_data_.visit_lens)\n",
    "            )\n",
    "        test_results[dataset[task]['dataset_name'] + \"-\" + task] = test(\n",
    "            evaluate_fn=evaluate_fn_, model=train_results[dataset[task]['dataset_name'] + \"-\" + task]['model'], \n",
    "            test_data=test_data_, loss_fn=torch.nn.BCELoss(), output_size=output_size_,\n",
    "            test_historical=historical_hot(test_data_.code_x, dataset[task]['code_num'], test_data_.visit_lens)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the results here\n",
    "\n",
    "print(\"test_results = \", test_results)\n",
    "print(\"train_results = \", train_results)\n",
    "\n",
    "# import matplotlib.pylab as plt\n",
    "# plt.plot(x, y)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 ('chet_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7306b834e6e06fbd654ec785d993585f279ffa0a060b5378ceb65f5750836ec3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
