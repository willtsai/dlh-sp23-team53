{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of Context-aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs\n",
    "\n",
    "**UIUC, CS598 DL4H, Spring 2023**\n",
    "\n",
    "**Authors:** Shiyu (Sherry) Li and Wei-Lun (Will) Tsai; {shiyuli2, wltsai2}@illinois.edu\n",
    "\n",
    "**Original paper:** Chang Lu, Tian Han, and Yue Ning. 2022. [Context-aware Health Event Prediction\n",
    "via Transition Functions on Dynamic Disease\n",
    "Graphs.](https://arxiv.org/pdf/2112.05195.pdf) Proceedings of the AAAI\n",
    "Conference on Artificial Intelligence, 36(4):4567â€“4574.\n",
    "\n",
    "**Original codebase:** [github.com/LuChang-CS/Chet](https://github.com/LuChang-CS/Chet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess the data\n",
    "\n",
    "As a part of the initial setup described in the [README](https://github.com/willtsai/uiuc-cs598-dlh/blob/main/README.md), we have downloaded the raw data and placed it in the `data` directory. We will now preprocess the data to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing the csv file of admission ...\n",
      "\t58976 in 58976 rows\n",
      "parsing csv file of diagnosis ...\n",
      "\t651047 in 651047 rows\n",
      "calibrating patients by admission ...\n",
      "calibrating admission by patients ...\n",
      "saving parsed data ...\n",
      "patient num: 7493\n",
      "max admission num: 42\n",
      "mean admission num: 2.66\n",
      "max code num in an admission: 39\n",
      "mean code num in an admission: 13.06\n",
      "encoding code ...\n",
      "There are 4880 codes\n",
      "generating code levels ...\n",
      "\t100%00%\n",
      "There are 6000 train, 493 valid, 1000 test samples\n",
      "generating code code adjacent matrix ...\n",
      "\t6000 / 6000\n",
      "building train codes features and labels ...\n",
      "\t6000 / 6000\n",
      "building valid codes features and labels ...\n",
      "\t493 / 493\n",
      "building test codes features and labels ...\n",
      "\t1000 / 1000\n",
      "generating train neighbors ...\n",
      "\t6000 / 6000\n",
      "generating valid neighbors ...\n",
      "\t493 / 493\n",
      "generating test neighbors ...\n",
      "\t1000 / 1000\n",
      "generating train middles ...\n",
      "\t6000 / 6000\n",
      "generating valid middles ...\n",
      "\t493 / 493\n",
      "generating test middles ...\n",
      "\t1000 / 1000\n",
      "building train heart failure labels ...\n",
      "building valid heart failure labels ...\n",
      "building test heart failure labels ...\n",
      "saving encoded data ...\n",
      "saving standard data ...\n",
      "\tsaving training data\n",
      "\tsaving valid data\n",
      "\tsaving test data\n",
      "loading ICD-10 to ICD-9 map ...\n",
      "loading patients anchor year ...\n",
      "parsing the csv file of admission ...\n",
      "\tselecting valid admission ...\n",
      "\t\t431231 in 431231 rows\n",
      "\t\tremaining 221815 rows\n",
      "\t221815 in 221815 rows\n",
      "parsing csv file of diagnosis ...\n",
      "\tmapping ICD-10 to ICD-9 ...\n",
      "\t\t4756326 in 4756326 rows\n",
      "\t4756326 in 4756326 rows\n",
      "calibrating patients by admission ...\n",
      "calibrating admission by patients ...\n",
      "saving parsed data ...\n",
      "patient num: 10000\n",
      "max admission num: 93\n",
      "mean admission num: 3.79\n",
      "max code num in an admission: 39\n",
      "mean code num in an admission: 13.51\n",
      "encoding code ...\n",
      "There are 6037 codes\n",
      "generating code levels ...\n",
      "\t100%00%\n",
      "There are 8000 train, 1000 valid, 1000 test samples\n",
      "generating code code adjacent matrix ...\n",
      "\t8000 / 8000\n",
      "building train codes features and labels ...\n",
      "\t8000 / 8000\n",
      "building valid codes features and labels ...\n",
      "\t1000 / 1000\n",
      "building test codes features and labels ...\n",
      "\t1000 / 1000\n",
      "generating train neighbors ...\n",
      "\t8000 / 8000\n",
      "generating valid neighbors ...\n",
      "\t1000 / 1000\n",
      "generating test neighbors ...\n",
      "\t1000 / 1000\n",
      "generating train middles ...\n",
      "\t8000 / 8000\n",
      "generating valid middles ...\n",
      "\t1000 / 1000\n",
      "generating test middles ...\n",
      "\t1000 / 1000\n",
      "building train heart failure labels ...\n",
      "building valid heart failure labels ...\n",
      "building test heart failure labels ...\n",
      "saving encoded data ...\n",
      "saving standard data ...\n",
      "\tsaving training data\n",
      "\tsaving valid data\n",
      "\tsaving test data\n",
      "***processing complete***\n"
     ]
    }
   ],
   "source": [
    "from run_preprocess import pre_process\n",
    "\n",
    "pre_process(dataset_names=['mimic3','mimic4'], data_saved=False)\n",
    "print('***processing complete***')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Set hyperparameters and seed\n",
    "\n",
    "We keep the same hyperparameters and seed as the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Keep the same hyperparameters and seed as the original paper\n",
    "code_size = 48\n",
    "graph_size = 32\n",
    "hidden_size = 150  # rnn hidden size\n",
    "t_attention_size = 32\n",
    "t_output_size = hidden_size\n",
    "batch_size = 32\n",
    "seed = 6669\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Config for hardware to use\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "Here we implement the model and its layers as described in the paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Optimized dynamic graph layer\n",
    "\n",
    "Here we define the optimized dynamic graph layer for the model. This layer performs the following steps:\n",
    "- Aggregate global/local context with the optimized graph layer with the embedding matrices\n",
    "- Calculate hidden embeddings for diagnoses and neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, adj, code_num, code_size, graph_size):\n",
    "        super().__init__()\n",
    "        self.embedding =  nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, code_size)))\n",
    "        self.adj = adj \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(code_size, graph_size)\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, code_x, neighbor):\n",
    "        # embedding matrices for for diseases appearing in current diagnoses\n",
    "        # M_embedding_matrices = self.embedding(code_x)\n",
    "        # embedding matrices for for diseases appearing in direct neighbors\n",
    "        # N_embedding_matrices = self.embedding(neighbor)\n",
    "        # static adjacency matrix\n",
    "        # keep these unsqueeze for now, may need change if we change the data loader\n",
    "        center_codes = torch.unsqueeze(code_x, dim=-1)\n",
    "        neighbor_codes = torch.unsqueeze(neighbor, dim=-1)\n",
    "\n",
    "        center_embeddings = center_codes * self.embedding\n",
    "        neighbor_embeddings = neighbor_codes * self.embedding\n",
    "\n",
    "        adj_mul_center = torch.matmul(self.adj, center_embeddings)\n",
    "        adj_mul_neighbor = torch.matmul(self.adj, neighbor_embeddings)\n",
    "\n",
    "        # All the calculation here are using the memory-efficient calculation as proved by the author in Subgraphs' Adjacency Matrix Calculation\n",
    "        # aggregated diagnosis local context and diagnosis global context\n",
    "        aggregated_diagnosis_embedding = center_embeddings + center_codes * adj_mul_center + center_codes * adj_mul_neighbor\n",
    "        # aggregated neighbor global context\n",
    "        aggregated_neighbor_embedding = neighbor_embeddings + neighbor_codes * adj_mul_neighbor + neighbor_codes * adj_mul_center\n",
    "\n",
    "        # hidden embeddings of diagnoses and neighbors\n",
    "        hidden_diagnosis_embedding = self.LeakyReLU(self.fc(aggregated_diagnosis_embedding))\n",
    "        hidden_neighbor_embedding = self.LeakyReLU(self.fc(aggregated_neighbor_embedding))\n",
    "        return hidden_diagnosis_embedding, hidden_neighbor_embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Transition functions layer\n",
    "\n",
    "Here we define the transition functions layer for the model. The hidden embeddings from the optimized dynamic graph layer are used as inputs to this layer. This layer includes GRU, M-GRU (customized GRU for matrices), and single headed attention functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SingleHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, attention_size):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        self.query_dense = nn.Linear(query_size, attention_size)\n",
    "        self.key_dense = nn.Linear(key_size, attention_size)\n",
    "        self.value_dense = nn.Linear(query_size, value_size)\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        query = self.query_dense(q)\n",
    "        key = self.key_dense(k)\n",
    "        value = self.value_dense(v)\n",
    "        attention = torch.matmul(query, key.T) / math.sqrt(self.attention_size)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        output = torch.matmul(attention, value)\n",
    "        return output\n",
    "    \n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, code_num, code_size, graph_size, hidden_size, t_attention_size, t_output_size):\n",
    "        super().__init__()\n",
    "        self.unrelated_embedding = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, graph_size)))\n",
    "        self.gru = nn.GRUCell(input_size=graph_size, hidden_size=hidden_size)\n",
    "        self.attention = SingleHeadAttentionLayer(graph_size, graph_size, t_output_size, t_attention_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        self.code_num = code_num\n",
    "        self.hidden_size = hidden_size\n",
    "        self.code_size = code_size\n",
    "\n",
    "    def forward(self, t, co_embeddings, divided, no_embeddings, hidden_state=None):\n",
    "        m_p, m_en, m_eu = divided[:, 0], divided[:, 1], divided[:, 2]\n",
    "        mp_idx, men_idx, meu_idx = torch.where(m_p > 0)[0], torch.where(m_en > 0)[0], torch.where(m_eu > 0)[0]\n",
    "        h_new = torch.zeros((self.code_num, self.hidden_size), dtype=co_embeddings.dtype).to(co_embeddings.device)\n",
    "        output_mp = 0\n",
    "        output_meneu = 0\n",
    "\n",
    "        if len(mp_idx) > 0:\n",
    "            h = hidden_state[mp_idx] if hidden_state is not None else None\n",
    "            h_p = self.gru(co_embeddings[mp_idx], h)\n",
    "            h_new[mp_idx] = h_p\n",
    "            output_mp, _ = torch.max(h_p, dim=-2)\n",
    "        if t == 0 or len(men_idx) + len(meu_idx) == 0:\n",
    "            output = output_mp\n",
    "        else:\n",
    "            q = torch.vstack([no_embeddings[men_idx], self.unrelated_embedding[meu_idx]])\n",
    "            v = torch.vstack([co_embeddings[men_idx], co_embeddings[meu_idx]])\n",
    "            h_tilda = self.activation(self.attention(q, q, v))\n",
    "            h_new[men_idx] = h_tilda[:len(men_idx)]\n",
    "            h_new[meu_idx] = h_tilda[len(men_idx):]\n",
    "            output_meneu, _ = torch.max(h_tilda, dim=-2)\n",
    "            if len(mp_idx) == 0:\n",
    "                output = output_meneu\n",
    "            else:\n",
    "                output, _ = torch.max(torch.vstack([output_mp, output_meneu]), dim=-2)\n",
    "\n",
    "        return output, h_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Embedding layer\n",
    "\n",
    "Here we define the embedding layer for the model, combined with the dot product attention activation for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingWithAttentionLayer(nn.Module):\n",
    "    def __init__(self, value_size, attention_size):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        # define context vector\n",
    "        self.context = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(attention_size, 1)))\n",
    "        self.linear = nn.Linear(value_size, attention_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max pooling\n",
    "        t = self.linear(x)\n",
    "        # calculate attention score\n",
    "        score = torch.softmax(torch.matmul(t, self.context).squeeze(), dim=-1)\n",
    "        # final hidden embedding\n",
    "        output = torch.sum(x * torch.unsqueeze(score, dim=-1), dim=-2)\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Model and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.activation(self.dropout(self.linear(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still need further editing to integrate\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, code_num, code_size,\n",
    "                 adj, graph_size, hidden_size, t_attention_size, t_output_size,\n",
    "                 output_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.graph_layer = GraphLayer(adj, code_num, code_size, graph_size)\n",
    "        self.transition_layer = TransitionLayer(code_num, code_size, graph_size, hidden_size, t_attention_size, t_output_size)\n",
    "        self.attention = EmbeddingWithAttentionLayer(hidden_size, 32)\n",
    "        self.classifier = Classifier(hidden_size, output_size, dropout_rate)\n",
    "\n",
    "    def forward(self, code_x, divided, neighbors, lens):\n",
    "        output = []\n",
    "        for code_x_i, divided_i, neighbor_i, len_i in zip(code_x, divided, neighbors, lens):\n",
    "            no_embeddings_i_prev = None\n",
    "            output_i = []\n",
    "            h_t = None\n",
    "            for t, (c_it, d_it, n_it, len_it) in enumerate(zip(code_x_i, divided_i, neighbor_i, range(len_i))):\n",
    "                co_embeddings, no_embeddings = self.graph_layer(c_it, n_it)\n",
    "                output_it, h_t = self.transition_layer(t, co_embeddings, d_it, no_embeddings_i_prev, h_t)\n",
    "                no_embeddings_i_prev = no_embeddings\n",
    "                output_i.append(output_it)\n",
    "            output_i = self.attention(torch.vstack(output_i))\n",
    "            output.append(output_i)\n",
    "        output = torch.vstack(output)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define functions for training and evaluation\n",
    "\n",
    "### 4.1 Historical hot function\n",
    "\n",
    "We re-use the `historical_hot()` function directly from the [original codebase](https://github.com/LuChang-CS/Chet/blob/master/train.py). The function will be used later in model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_hot(code_x, code_num, lens):\n",
    "    result = np.zeros((len(code_x), code_num), dtype=int)\n",
    "    for i, (x, l) in enumerate(zip(code_x, lens)):\n",
    "        result[i] = x[l - 1]\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data loader function\n",
    "\n",
    "We create a data_loader() function to load the data needed for training and evaluating the model. The function is based on the [data loding code](https://github.com/LuChang-CS/Chet/blob/master/train.py#L45-L52) from the original authors and also re-uses several of the data loading helper functions from [`utils.py`](https://github.com/LuChang-CS/Chet/blob/master/utils.py) in the original codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import load_adj\n",
    "from utils import EHRDataset\n",
    "from utils import MultiStepLRScheduler\n",
    "\n",
    "def data_loader(task, dataset_path):\n",
    "    print('from {} for task {}:'.format(dataset_path, task))\n",
    "    print('loading code adjacency matrix ...')\n",
    "    code_adj = load_adj(dataset_path, device=device)\n",
    "    code_num = len(code_adj)\n",
    "    print('loading train data ...')\n",
    "    train_data = EHRDataset(os.path.join(dataset_path, \"train/\"), label=task, batch_size=batch_size, shuffle=True, device=device)\n",
    "    print('loading valid data ...')\n",
    "    valid_data = EHRDataset(os.path.join(dataset_path, \"valid/\"), label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "    print('loading test data ...')\n",
    "    test_data = EHRDataset(os.path.join(dataset_path, \"test/\"), label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "\n",
    "    return {\n",
    "        'dataset_name': dataset_path.split('/')[1],\n",
    "        'code_adj': code_adj, \n",
    "        'code_num': code_num, \n",
    "        'train_data': train_data, \n",
    "        'valid_data': valid_data, \n",
    "        'test_data': test_data, \n",
    "        }\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils import format_time\n",
    "\n",
    "def train_chet(path, task, output_size, evaluate_fn, code_adj, code_num, dropout_rate, \n",
    "               train_data, valid_data, init_lr, lrs, milestones, epochs, test_historical):\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    \n",
    "    # Keep the same model param storage path as the original paper\n",
    "    param_path = os.path.join('data', 'params', path, task)\n",
    "    if not os.path.exists(param_path):\n",
    "        os.makedirs(param_path)\n",
    "\n",
    "    # Keep the same model, optimizer, and scheduler as the original paper,\n",
    "    # but slightly modified to leverage the new config dict\n",
    "    model = Model(code_num=code_num, code_size=code_size,\n",
    "                    adj=code_adj, graph_size=graph_size, hidden_size=hidden_size, t_attention_size=t_attention_size,\n",
    "                    t_output_size=t_output_size,\n",
    "                    output_size=output_size, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = MultiStepLRScheduler(optimizer, epochs, init_lr, milestones, lrs)\n",
    "\n",
    "    # Keep the same param printing code as the original paper\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(pytorch_total_params)\n",
    "\n",
    "    # Keep the same training loop code as the original paper, but note that\n",
    "    # the train, valid, and test data will change based on the task and\n",
    "    # dataset of the current loop\n",
    "    epoch_lrs, valid_losses, mean_losses, time_costs, f1_scores, auc_or_topks = [], [], [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch %d / %d:' % (epoch + 1, epochs))\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_num = 0\n",
    "        steps = len(train_data)\n",
    "        st = time.time()\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.lrs[epoch]\n",
    "        for step in range(len(train_data)):\n",
    "            optimizer.zero_grad()\n",
    "            code_x, visit_lens, divided, y, neighbors = train_data[step]\n",
    "            output = model(code_x, divided, neighbors, visit_lens).squeeze()\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * output_size * len(code_x)\n",
    "            total_num += len(code_x)\n",
    "\n",
    "            end_time = time.time()\n",
    "            remaining_time = format_time((end_time - st) / (step + 1) * (steps - step - 1))\n",
    "            print('\\r    Step %d / %d, LR: %s, remaining time: %s, loss: %.4f'\n",
    "                % (step + 1, steps, current_lr, remaining_time, total_loss / total_num), end='')\n",
    "        train_data.on_epoch_end()\n",
    "        et = time.time()\n",
    "        time_cost = format_time(et - st)\n",
    "        mean_loss = total_loss / total_num\n",
    "        print('\\r    Step %d / %d, LR: %s, time cost: %s, loss: %.4f' % (steps, steps, current_lr, time_cost, mean_loss))\n",
    "        valid_loss, f1_score, auc_or_topk = evaluate_fn(model, valid_data, loss_fn, output_size, test_historical)\n",
    "        torch.save(model.state_dict(), os.path.join(param_path, '%d.pt' % epoch))\n",
    "        epoch_lrs.append(current_lr)\n",
    "        valid_losses.append(valid_loss)\n",
    "        mean_losses.append(mean_loss)\n",
    "        time_costs.append(time_cost)\n",
    "        f1_scores.append(f1_score)\n",
    "        auc_or_topks.append(auc_or_topk)\n",
    "        \n",
    "    return {\n",
    "        'model': model,\n",
    "        'epoch_lrs': epoch_lrs,\n",
    "        'valid_losses': valid_losses,\n",
    "        'mean_losses': mean_losses,\n",
    "        'time_costs': time_costs,\n",
    "        'f1_scores': f1_scores,\n",
    "        'auc_or_topks': auc_or_topks,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model evaluation function\n",
    "We create a `test()` function to evaluate the model on the `test_data`. We re-use the `evaluate_codes()` and `evaluate_hf()` functions directly from the original [metrics.py](https://github.com/LuChang-CS/Chet/blob/master/metrics.py) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(evaluate_fn, model, test_data, loss_fn, output_size, test_historical):\n",
    "    print(\"Evaluating model on test data...\")\n",
    "    model.eval()\n",
    "    test_loss, f1_score, auc_or_topk = evaluate_fn(model, test_data, loss_fn, output_size, historical=test_historical)\n",
    "    print(\"Test loss: %s, F1 score: %s, AUC or TopK: %s\" % (test_loss, f1_score, auc_or_topk))\n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'f1_score': f1_score,\n",
    "        'auc_or_topk': auc_or_topk,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the preprocessed data\n",
    "\n",
    "Here we load the preprocessed data using the data_loader() function defined above, for both the MIMIC-III and MIMIC-IV datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from data/mimic3/standard/ for task h:\n",
      "loading code adjacency matrix ...\n",
      "loading train data ...\n",
      "loading valid data ...\n",
      "loading test data ...\n",
      "from data/mimic4/standard/ for task h:\n",
      "loading code adjacency matrix ...\n",
      "loading train data ...\n",
      "loading valid data ...\n",
      "loading test data ...\n",
      "from data/mimic3/standard/ for task m:\n",
      "loading code adjacency matrix ...\n",
      "loading train data ...\n",
      "loading valid data ...\n",
      "loading test data ...\n",
      "from data/mimic4/standard/ for task m:\n",
      "loading code adjacency matrix ...\n",
      "loading train data ...\n",
      "loading valid data ...\n",
      "loading test data ...\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "# Todo: Add data analysis here to explain each sub datasets eg: code_x, visit_lens, divided, y, neighbors, code_adj. Also can give an example by printing some data\n",
    "# Todo: Add some initial data analysis. 1) number/ratio of heart failure patients. 2) some statistic for neighbors\n",
    "# Possible Todo: build our own data builder using torch.utils.data.DataLoader as HWs\n",
    "\n",
    "tasks = ['h', 'm']\n",
    "mimic4_standard_path = \"data/mimic4/standard/\"\n",
    "mimic3_standard_path = \"data/mimic3/standard/\"\n",
    "mimic3_datasets, mimic4_datasets = {}, {}\n",
    "for task in tasks:\n",
    "    mimic3_datasets[task] = data_loader(task, mimic3_standard_path)\n",
    "    mimic4_datasets[task] = data_loader(task, mimic4_standard_path)\n",
    "print(\"data loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for h task on mimic3 dataset:\n",
      "486845\n",
      "Epoch 1 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 4m14.7s, loss: 0.498181967\n",
      "    Evaluation: loss: 0.4678 --- auc: 0.8587 --- f1_score: 0.7360\n",
      "Epoch 2 / 20:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m14.9s, loss: 0.408181333\n",
      "    Evaluation: loss: 0.4485 --- auc: 0.8599 --- f1_score: 0.7446\n",
      "Epoch 3 / 20:\n",
      "    Step 188 / 188, LR: 0.0001, time cost: 4m12.2s, loss: 0.388787001\n",
      "    Evaluation: loss: 0.4497 --- auc: 0.8596 --- f1_score: 0.7466\n",
      "Epoch 4 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m9.6s, loss: 0.3866866779\n",
      "    Evaluation: loss: 0.4499 --- auc: 0.8595 --- f1_score: 0.7466\n",
      "Epoch 5 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m8.5s, loss: 0.3863863362\n",
      "    Evaluation: loss: 0.4500 --- auc: 0.8596 --- f1_score: 0.7466\n",
      "Epoch 6 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m5.5s, loss: 0.3860860097\n",
      "    Evaluation: loss: 0.4501 --- auc: 0.8596 --- f1_score: 0.7466\n",
      "Epoch 7 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m5.1s, loss: 0.3858858869\n",
      "    Evaluation: loss: 0.4502 --- auc: 0.8595 --- f1_score: 0.7466\n",
      "Epoch 8 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m7.6s, loss: 0.3855855448\n",
      "    Evaluation: loss: 0.4502 --- auc: 0.8595 --- f1_score: 0.7486\n",
      "Epoch 9 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m6.8s, loss: 0.3852852623\n",
      "    Evaluation: loss: 0.4504 --- auc: 0.8595 --- f1_score: 0.7486\n",
      "Epoch 10 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m6.5s, loss: 0.3850850069\n",
      "    Evaluation: loss: 0.4504 --- auc: 0.8594 --- f1_score: 0.7507\n",
      "Epoch 11 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m5.8s, loss: 0.3847847598\n",
      "    Evaluation: loss: 0.4505 --- auc: 0.8595 --- f1_score: 0.7473\n",
      "Epoch 12 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m5.6s, loss: 0.3844844052\n",
      "    Evaluation: loss: 0.4506 --- auc: 0.8595 --- f1_score: 0.7403\n",
      "Epoch 13 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m5.9s, loss: 0.3842842592\n",
      "    Evaluation: loss: 0.4507 --- auc: 0.8596 --- f1_score: 0.7403\n",
      "Epoch 14 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m6.1s, loss: 0.3839839494\n",
      "    Evaluation: loss: 0.4507 --- auc: 0.8594 --- f1_score: 0.7403\n",
      "Epoch 15 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m6.7s, loss: 0.3836836227\n",
      "    Evaluation: loss: 0.4508 --- auc: 0.8594 --- f1_score: 0.7403\n",
      "Epoch 16 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m6.5s, loss: 0.3833833105\n",
      "    Evaluation: loss: 0.4509 --- auc: 0.8595 --- f1_score: 0.7368\n",
      "Epoch 17 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m5.7s, loss: 0.3831831176\n",
      "    Evaluation: loss: 0.4510 --- auc: 0.8595 --- f1_score: 0.7368\n",
      "Epoch 18 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m5.9s, loss: 0.3828828123\n",
      "    Evaluation: loss: 0.4511 --- auc: 0.8595 --- f1_score: 0.7368\n",
      "Epoch 19 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m6.8s, loss: 0.3825825174\n",
      "    Evaluation: loss: 0.4511 --- auc: 0.8595 --- f1_score: 0.7368\n",
      "Epoch 20 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m6.5s, loss: 0.3822822351\n",
      "    Evaluation: loss: 0.4512 --- auc: 0.8593 --- f1_score: 0.7368\n",
      "Evaluating model on test data...\n",
      "    Evaluation: loss: 0.4676 --- auc: 0.8448 --- f1_score: 0.7124\n",
      "Test loss: 0.46755924212932587, F1 score: 0.7124463519313305, AUC or TopK: 0.844755521692903\n",
      "training for h task on mimic4 dataset:\n",
      "579405\n",
      "Epoch 1 / 20:\n",
      "    Step 250 / 250, LR: 0.01, time cost: 14m58.0s, loss: 0.26899710\n",
      "    Evaluation: loss: 0.2777 --- auc: 0.9266 --- f1_score: 0.7299\n",
      "Epoch 2 / 20:\n",
      "    Step 250 / 250, LR: 0.001, time cost: 15m0.5s, loss: 0.204040104\n",
      "    Evaluation: loss: 0.2413 --- auc: 0.9312 --- f1_score: 0.7650\n",
      "Epoch 3 / 20:\n",
      "    Step 250 / 250, LR: 0.0001, time cost: 15m15.4s, loss: 0.19322748\n",
      "    Evaluation: loss: 0.2413 --- auc: 0.9311 --- f1_score: 0.7663\n",
      "Epoch 4 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m15.2s, loss: 0.19166492\n",
      "    Evaluation: loss: 0.2413 --- auc: 0.9311 --- f1_score: 0.7663\n",
      "Epoch 5 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m14.3s, loss: 0.19144090\n",
      "    Evaluation: loss: 0.2414 --- auc: 0.9311 --- f1_score: 0.7663\n",
      "Epoch 6 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m14.1s, loss: 0.19122673\n",
      "    Evaluation: loss: 0.2414 --- auc: 0.9311 --- f1_score: 0.7663\n",
      "Epoch 7 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m14.4s, loss: 0.19111720\n",
      "    Evaluation: loss: 0.2415 --- auc: 0.9311 --- f1_score: 0.7663\n",
      "Epoch 8 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m17.6s, loss: 0.19099844\n",
      "    Evaluation: loss: 0.2416 --- auc: 0.9310 --- f1_score: 0.7663\n",
      "Epoch 9 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m17.0s, loss: 0.19077715\n",
      "    Evaluation: loss: 0.2417 --- auc: 0.9310 --- f1_score: 0.7663\n",
      "Epoch 10 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m15.2s, loss: 0.19066118\n",
      "    Evaluation: loss: 0.2418 --- auc: 0.9310 --- f1_score: 0.7663\n",
      "Epoch 11 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m17.1s, loss: 0.19044812\n",
      "    Evaluation: loss: 0.2418 --- auc: 0.9310 --- f1_score: 0.7663\n",
      "Epoch 12 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m16.0s, loss: 0.19033885\n",
      "    Evaluation: loss: 0.2419 --- auc: 0.9310 --- f1_score: 0.7663\n",
      "Epoch 13 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m17.9s, loss: 0.19011375\n",
      "    Evaluation: loss: 0.2420 --- auc: 0.9309 --- f1_score: 0.7663\n",
      "Epoch 14 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m16.9s, loss: 0.18999430\n",
      "    Evaluation: loss: 0.2421 --- auc: 0.9309 --- f1_score: 0.7663\n",
      "Epoch 15 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m18.4s, loss: 0.18988757\n",
      "    Evaluation: loss: 0.2422 --- auc: 0.9309 --- f1_score: 0.7663\n",
      "Epoch 16 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m19.9s, loss: 0.18966027\n",
      "    Evaluation: loss: 0.2423 --- auc: 0.9309 --- f1_score: 0.7663\n",
      "Epoch 17 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 15m19.9s, loss: 0.18955238\n",
      "    Evaluation: loss: 0.2424 --- auc: 0.9308 --- f1_score: 0.7629\n",
      "Epoch 18 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 43m0.6s, loss: 0.189393752\n",
      "    Evaluation: loss: 0.2425 --- auc: 0.9308 --- f1_score: 0.7629\n",
      "Epoch 19 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 2h1m36.9s, loss: 0.189228777\n",
      "    Evaluation: loss: 0.2425 --- auc: 0.9308 --- f1_score: 0.7629\n",
      "Epoch 20 / 20:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 2h25m32.2s, loss: 0.189000154\n",
      "    Evaluation: loss: 0.2426 --- auc: 0.9307 --- f1_score: 0.7629\n",
      "Evaluating model on test data...\n",
      "    Evaluation: loss: 0.2312 --- auc: 0.9340 --- f1_score: 0.7529\n",
      "Test loss: 0.2312017776966095, F1 score: 0.7528735632183907, AUC or TopK: 0.9339847904167586\n",
      "training for m task on mimic3 dataset:\n",
      "1223574\n",
      "Epoch 1 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 8m5.6s, loss: 1609.9511511158\n",
      "    Evaluation: loss: 136.0098 --- f1_score: 0.1081 --- top_k_recall: 0.1709, 0.2634, 0.3205, 0.3688  --- occurred: 0.0965, 0.1275, 0.1450, 0.1583  --- not occurred: 0.0744, 0.1360, 0.1754, 0.2104\n",
      "Epoch 2 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 6m51.1s, loss: 1559.561212231\n",
      "    Evaluation: loss: 140.7598 --- f1_score: 0.1186 --- top_k_recall: 0.2014, 0.2842, 0.3525, 0.3918  --- occurred: 0.1029, 0.1291, 0.1538, 0.1727  --- not occurred: 0.0985, 0.1551, 0.1987, 0.2191\n",
      "Epoch 3 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 45m3.5s, loss: 1558.4279796684\n",
      "    Evaluation: loss: 131.7789 --- f1_score: 0.1284 --- top_k_recall: 0.2064, 0.2883, 0.3499, 0.3956  --- occurred: 0.1085, 0.1330, 0.1556, 0.1694  --- not occurred: 0.0978, 0.1553, 0.1943, 0.2263\n",
      "Epoch 4 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 49m1.8s, loss: 1558.59414189182\n",
      "    Evaluation: loss: 130.0533 --- f1_score: 0.1394 --- top_k_recall: 0.2202, 0.2998, 0.3650, 0.4098  --- occurred: 0.1115, 0.1403, 0.1637, 0.1782  --- not occurred: 0.1088, 0.1595, 0.2013, 0.2316\n",
      "Epoch 5 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 53m12.1s, loss: 1557.958442716\n",
      "    Evaluation: loss: 123.2420 --- f1_score: 0.1432 --- top_k_recall: 0.2225, 0.3098, 0.3746, 0.4270  --- occurred: 0.1179, 0.1533, 0.1739, 0.1902  --- not occurred: 0.1046, 0.1564, 0.2007, 0.2368\n",
      "Epoch 6 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 56m29.0s, loss: 1556.8983322395\n",
      "    Evaluation: loss: 127.2800 --- f1_score: 0.1583 --- top_k_recall: 0.2247, 0.3295, 0.3935, 0.4389  --- occurred: 0.1163, 0.1580, 0.1829, 0.1983  --- not occurred: 0.1084, 0.1715, 0.2106, 0.2406\n",
      "Epoch 7 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 51m35.6s, loss: 1556.1850096915\n",
      "    Evaluation: loss: 110.6640 --- f1_score: 0.1646 --- top_k_recall: 0.2338, 0.3260, 0.3853, 0.4346  --- occurred: 0.1261, 0.1619, 0.1836, 0.1963  --- not occurred: 0.1077, 0.1641, 0.2017, 0.2383\n",
      "Epoch 8 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 1h7m59.0s, loss: 1556.21020899\n",
      "    Evaluation: loss: 120.8498 --- f1_score: 0.1723 --- top_k_recall: 0.2343, 0.3271, 0.3926, 0.4376  --- occurred: 0.1329, 0.1691, 0.1880, 0.2022  --- not occurred: 0.1014, 0.1580, 0.2046, 0.2354\n",
      "Epoch 9 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 1h18m16.7s, loss: 1554.97612456\n",
      "    Evaluation: loss: 121.5457 --- f1_score: 0.1732 --- top_k_recall: 0.2420, 0.3361, 0.4019, 0.4483  --- occurred: 0.1371, 0.1720, 0.1915, 0.2038  --- not occurred: 0.1049, 0.1641, 0.2104, 0.2445\n",
      "Epoch 10 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 59m20.1s, loss: 1554.7189914604\n",
      "    Evaluation: loss: 111.6923 --- f1_score: 0.1792 --- top_k_recall: 0.2467, 0.3367, 0.4038, 0.4503  --- occurred: 0.1402, 0.1740, 0.1954, 0.2083  --- not occurred: 0.1065, 0.1627, 0.2084, 0.2420\n",
      "Epoch 11 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 1h25m28.4s, loss: 1554.24259966\n",
      "    Evaluation: loss: 108.3710 --- f1_score: 0.1882 --- top_k_recall: 0.2492, 0.3426, 0.4080, 0.4531  --- occurred: 0.1437, 0.1782, 0.1981, 0.2092  --- not occurred: 0.1055, 0.1644, 0.2098, 0.2439\n",
      "Epoch 12 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 33m52.5s, loss: 1553.702223036\n",
      "    Evaluation: loss: 101.8001 --- f1_score: 0.1939 --- top_k_recall: 0.2574, 0.3520, 0.4126, 0.4608  --- occurred: 0.1428, 0.1809, 0.2003, 0.2140  --- not occurred: 0.1146, 0.1711, 0.2123, 0.2468\n",
      "Epoch 13 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 32m0.4s, loss: 1553.0004040003\n",
      "    Evaluation: loss: 112.6662 --- f1_score: 0.1973 --- top_k_recall: 0.2558, 0.3473, 0.4069, 0.4555  --- occurred: 0.1465, 0.1810, 0.2024, 0.2172  --- not occurred: 0.1093, 0.1663, 0.2045, 0.2384\n",
      "Epoch 14 / 20:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 4m9.0s, loss: 1552.6827827977\n",
      "    Evaluation: loss: 99.5372 --- f1_score: 0.2007 --- top_k_recall: 0.2582, 0.3488, 0.4157, 0.4639  --- occurred: 0.1506, 0.1853, 0.2086, 0.2233  --- not occurred: 0.1076, 0.1635, 0.2071, 0.2406\n",
      "Epoch 15 / 20:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m11.5s, loss: 1550.593636658\n",
      "    Evaluation: loss: 99.0152 --- f1_score: 0.2095 --- top_k_recall: 0.2689, 0.3646, 0.4272, 0.4764  --- occurred: 0.1561, 0.1927, 0.2139, 0.2258  --- not occurred: 0.1128, 0.1719, 0.2134, 0.2507\n",
      "Epoch 16 / 20:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m10.5s, loss: 1550.160404897\n",
      "    Evaluation: loss: 99.7823 --- f1_score: 0.2131 --- top_k_recall: 0.2669, 0.3617, 0.4266, 0.4726  --- occurred: 0.1573, 0.1961, 0.2147, 0.2273  --- not occurred: 0.1096, 0.1656, 0.2118, 0.2452\n",
      "Epoch 17 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m12.2s, loss: 1549.714444891\n",
      "    Evaluation: loss: 99.7574 --- f1_score: 0.2132 --- top_k_recall: 0.2666, 0.3619, 0.4265, 0.4725  --- occurred: 0.1571, 0.1960, 0.2146, 0.2272  --- not occurred: 0.1095, 0.1659, 0.2119, 0.2453\n",
      "Epoch 18 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m13.0s, loss: 1549.616161958\n",
      "    Evaluation: loss: 99.6831 --- f1_score: 0.2132 --- top_k_recall: 0.2666, 0.3620, 0.4268, 0.4730  --- occurred: 0.1573, 0.1959, 0.2148, 0.2271  --- not occurred: 0.1093, 0.1662, 0.2120, 0.2459\n",
      "Epoch 19 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m12.5s, loss: 1549.309191042\n",
      "    Evaluation: loss: 99.5622 --- f1_score: 0.2130 --- top_k_recall: 0.2665, 0.3621, 0.4271, 0.4723  --- occurred: 0.1573, 0.1958, 0.2150, 0.2271  --- not occurred: 0.1093, 0.1664, 0.2121, 0.2451\n",
      "Epoch 20 / 20:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m12.1s, loss: 1549.919393500\n",
      "    Evaluation: loss: 99.5924 --- f1_score: 0.2132 --- top_k_recall: 0.2667, 0.3621, 0.4272, 0.4727  --- occurred: 0.1574, 0.1958, 0.2151, 0.2272  --- not occurred: 0.1093, 0.1664, 0.2120, 0.2456\n",
      "Evaluating model on test data...\n",
      "    Evaluation: loss: 98.6097 --- f1_score: 0.2124 --- top_k_recall: 0.2579, 0.3503, 0.4139, 0.4597  --- occurred: 0.1591, 0.1920, 0.2118, 0.2240  --- not occurred: 0.0989, 0.1583, 0.2021, 0.2356\n",
      "Test loss: 98.60973766684532, F1 score: 0.21236173706720454, AUC or TopK: [0.25794787771677297, 0.3502976917188601, 0.4139315274235037, 0.4596504614437993]\n",
      "training for m task on mimic4 dataset:\n",
      "1490841\n",
      "Epoch 1 / 20:\n",
      "    Step 250 / 250, LR: 0.01, time cost: 15m13.1s, loss: 1960.84588843\n",
      "    Evaluation: loss: 140.3669 --- f1_score: 0.1106 --- top_k_recall: 0.1856, 0.2644, 0.3155, 0.3555  --- occurred: 0.1153, 0.1506, 0.1726, 0.1899  --- not occurred: 0.0703, 0.1138, 0.1429, 0.1656\n",
      "Epoch 2 / 20:\n",
      "    Step 189 / 250, LR: 0.01, remaining time: 3m45.2s, loss: 1919.3601"
     ]
    }
   ],
   "source": [
    "from metrics import evaluate_codes, evaluate_hf\n",
    "\n",
    "epochs = 20 # epochs = 200 in original paper\n",
    "\n",
    "train_results = {}\n",
    "test_results = {}\n",
    "\n",
    "for task in tasks:\n",
    "    dropout_rate_ = 0.45 if task == 'm' else 0.0\n",
    "    lrs_ = [1e-3, 1e-5] if task == 'm' else [1e-3, 1e-4, 1e-5]\n",
    "    milestones_ = [15, 17] if task == 'm' else [2,3,4] # [20, 30] [2, 3, 20]\n",
    "    evaluate_fn_ = evaluate_codes if task == 'm' else evaluate_hf\n",
    "    for dataset in [mimic3_datasets, mimic4_datasets]:\n",
    "        output_size_ = dataset[task]['code_num'] if task == 'm' else 1\n",
    "        print('training for %s task on %s dataset:' % (task, dataset[task]['dataset_name']))\n",
    "        train_data_ = dataset[task]['train_data']\n",
    "        valid_data_ = dataset[task]['valid_data']\n",
    "        test_data_ = dataset[task]['test_data']\n",
    "        train_results[dataset[task]['dataset_name']] = train_chet(\n",
    "            path=dataset[task]['dataset_name'],task=task, output_size=output_size_, \n",
    "            evaluate_fn=evaluate_fn_, code_adj=dataset[task]['code_adj'], code_num=dataset[task]['code_num'], \n",
    "            dropout_rate=dropout_rate_, train_data=train_data_, valid_data=valid_data_, \n",
    "            init_lr=0.01, lrs=lrs_, milestones=milestones_, epochs=epochs, \n",
    "            test_historical=historical_hot(valid_data_.code_x, dataset[task]['code_num'], valid_data_.visit_lens)\n",
    "            )\n",
    "        test_results[dataset[task]['dataset_name'] + \"-\" + task] = test(\n",
    "            evaluate_fn=evaluate_fn_, model=train_results[dataset[task]['dataset_name']]['model'], \n",
    "            test_data=test_data_, loss_fn=torch.nn.BCELoss(), output_size=output_size_,\n",
    "            test_historical=historical_hot(test_data_.code_x, dataset[task]['code_num'], test_data_.visit_lens)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mimic4-m': {'test_loss': 110.60927788607776, 'f1_score': 0.20657817674236958, 'auc_or_topk': [0.2553506488846398, 0.34513551889938804, 0.4047283347085497, 0.4411470453157537]}}\n"
     ]
    }
   ],
   "source": [
    "# analyze the results here\n",
    "\n",
    "# test_results = {}\n",
    "# test_results[\"mimic3\" + \"-\" + \"h\"] = \"test\"\n",
    "\n",
    "print(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 ('chet_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7306b834e6e06fbd654ec785d993585f279ffa0a060b5378ceb65f5750836ec3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
