{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of Context-aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs\n",
    "\n",
    "**UIUC, CS598 DL4H, Spring 2023**\n",
    "\n",
    "**Authors:** Shiyu (Sherry) Li and Wei-Lun (Will) Tsai; {shiyuli2, wltsai2}@illinois.edu\n",
    "\n",
    "**Original paper:** Chang Lu, Tian Han, and Yue Ning. 2022. [Context-aware Health Event Prediction\n",
    "via Transition Functions on Dynamic Disease\n",
    "Graphs.](https://arxiv.org/pdf/2112.05195.pdf) Proceedings of the AAAI\n",
    "Conference on Artificial Intelligence, 36(4):4567â€“4574.\n",
    "\n",
    "**Original codebase:** [github.com/LuChang-CS/Chet](https://github.com/LuChang-CS/Chet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Set hyperparameters and seed\n",
    "\n",
    "We keep the same hyperparameters and seed as the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Keep the same hyperparameters and seed as the original paper\n",
    "code_size = 48\n",
    "graph_size = 32\n",
    "hidden_size = 150  # rnn hidden size\n",
    "t_attention_size = 32\n",
    "t_output_size = hidden_size\n",
    "batch_size = 32\n",
    "seed = 6669\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Config for hardware to use\n",
    "use_cuda = True\n",
    "use_mps = True\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available() and use_mps:\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data\n",
    "\n",
    "As a part of the initial setup described in the [README](https://github.com/willtsai/uiuc-cs598-dlh/blob/main/README.md), we have downloaded the raw data and placed it in the `data` directory. We will now preprocess the data to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing the csv file of admission ...\n",
      "\t58976 in 58976 rows\n",
      "parsing csv file of diagnosis ...\n",
      "\t651047 in 651047 rows\n",
      "calibrating patients by admission ...\n",
      "calibrating admission by patients ...\n",
      "saving parsed data ...\n",
      "patient num: 7493\n",
      "max admission num: 42\n",
      "mean admission num: 2.66\n",
      "max code num in an admission: 39\n",
      "mean code num in an admission: 13.06\n",
      "encoding code ...\n",
      "There are 4880 codes\n",
      "generating code levels ...\n",
      "\t100%00%\n",
      "There are 6000 train, 493 valid, 1000 test samples\n",
      "generating code code adjacent matrix ...\n",
      "\t6000 / 6000\n",
      "building train codes features and labels ...\n",
      "\t6000 / 6000\n",
      "building valid codes features and labels ...\n",
      "\t493 / 493\n",
      "building test codes features and labels ...\n",
      "\t1000 / 1000\n",
      "generating train neighbors ...\n",
      "\t6000 / 6000\n",
      "generating valid neighbors ...\n",
      "\t493 / 493\n",
      "generating test neighbors ...\n",
      "\t1000 / 1000\n",
      "generating train middles ...\n",
      "\t6000 / 6000\n",
      "generating valid middles ...\n",
      "\t493 / 493\n",
      "generating test middles ...\n",
      "\t1000 / 1000\n",
      "building train heart failure labels ...\n",
      "building valid heart failure labels ...\n",
      "building test heart failure labels ...\n",
      "saving encoded data ...\n",
      "saving standard data ...\n",
      "\tsaving training data\n",
      "\tsaving valid data\n",
      "\tsaving test data\n",
      "loading ICD-10 to ICD-9 map ...\n",
      "loading patients anchor year ...\n",
      "parsing the csv file of admission ...\n",
      "\tselecting valid admission ...\n",
      "\t\t431231 in 431231 rows\n",
      "\t\tremaining 221815 rows\n",
      "\t221815 in 221815 rows\n",
      "parsing csv file of diagnosis ...\n",
      "\tmapping ICD-10 to ICD-9 ...\n",
      "\t\t4756326 in 4756326 rows\n",
      "\t4756326 in 4756326 rows\n",
      "calibrating patients by admission ...\n",
      "calibrating admission by patients ...\n",
      "saving parsed data ...\n",
      "patient num: 10000\n",
      "max admission num: 93\n",
      "mean admission num: 3.79\n",
      "max code num in an admission: 39\n",
      "mean code num in an admission: 13.51\n",
      "encoding code ...\n",
      "There are 6037 codes\n",
      "generating code levels ...\n",
      "\t100%00%\n",
      "There are 8000 train, 1000 valid, 1000 test samples\n",
      "generating code code adjacent matrix ...\n",
      "\t8000 / 8000\n",
      "building train codes features and labels ...\n",
      "\t8000 / 8000\n",
      "building valid codes features and labels ...\n",
      "\t1000 / 1000\n",
      "building test codes features and labels ...\n",
      "\t1000 / 1000\n",
      "generating train neighbors ...\n",
      "\t8000 / 8000\n",
      "generating valid neighbors ...\n",
      "\t1000 / 1000\n",
      "generating test neighbors ...\n",
      "\t1000 / 1000\n",
      "generating train middles ...\n",
      "\t8000 / 8000\n",
      "generating valid middles ...\n",
      "\t1000 / 1000\n",
      "generating test middles ...\n",
      "\t1000 / 1000\n",
      "building train heart failure labels ...\n",
      "building valid heart failure labels ...\n",
      "building test heart failure labels ...\n",
      "saving encoded data ...\n",
      "saving standard data ...\n",
      "\tsaving training data\n",
      "\tsaving valid data\n",
      "\tsaving test data\n",
      "***processing complete***\n"
     ]
    }
   ],
   "source": [
    "from run_preprocess import pre_process\n",
    "\n",
    "pre_process(dataset_names=['mimic3','mimic4'], data_saved=False)\n",
    "print('***processing complete***')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "Here we implement the model and its layers as described in the paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Optimized dynamic graph layer\n",
    "\n",
    "Here we define the optimized dynamic graph layer for the model. This layer performs the following steps:\n",
    "- Aggregate global/local context with the optimized graph layer with the embedding matrices\n",
    "- Calculate hidden embeddings for diagnoses and neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, adj, code_num, code_size, graph_size):\n",
    "        super().__init__()\n",
    "        self.embedding =  nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, code_size)))\n",
    "        self.adj = adj \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(code_size, graph_size)\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, code_x, neighbor):\n",
    "        # embedding matrices for for diseases appearing in current diagnoses\n",
    "        # M_embedding_matrices = self.embedding(code_x)\n",
    "        # embedding matrices for for diseases appearing in direct neighbors\n",
    "        # N_embedding_matrices = self.embedding(neighbor)\n",
    "        # static adjacency matrix\n",
    "        # keep these unsqueeze for now, may need change if we change the data loader\n",
    "        center_codes = torch.unsqueeze(code_x, dim=-1)\n",
    "        neighbor_codes = torch.unsqueeze(neighbor, dim=-1)\n",
    "\n",
    "        center_embeddings = center_codes * self.embedding\n",
    "        neighbor_embeddings = neighbor_codes * self.embedding\n",
    "\n",
    "        adj_mul_center = torch.matmul(self.adj, center_embeddings)\n",
    "        adj_mul_neighbor = torch.matmul(self.adj, neighbor_embeddings)\n",
    "\n",
    "        # All the calculation here are using the memory-efficient calculation as proved by the author in Subgraphs' Adjacency Matrix Calculation\n",
    "        # aggregated diagnosis local context and diagnosis global context\n",
    "        aggregated_diagnosis_embedding = center_embeddings + center_codes * adj_mul_center + center_codes * adj_mul_neighbor\n",
    "        # aggregated neighbor global context\n",
    "        aggregated_neighbor_embedding = neighbor_embeddings + neighbor_codes * adj_mul_neighbor + neighbor_codes * adj_mul_center\n",
    "\n",
    "        # hidden embeddings of diagnoses and neighbors\n",
    "        hidden_diagnosis_embedding = self.LeakyReLU(self.fc(aggregated_diagnosis_embedding))\n",
    "        hidden_neighbor_embedding = self.LeakyReLU(self.fc(aggregated_neighbor_embedding))\n",
    "        return hidden_diagnosis_embedding, hidden_neighbor_embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Transition functions layer\n",
    "\n",
    "Here we define the transition functions layer for the model. The hidden embeddings from the optimized dynamic graph layer are used as inputs to this layer. This layer includes GRU, M-GRU (customized GRU for matrices), and single headed attention functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SingleHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, attention_size):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        self.query_dense = nn.Linear(query_size, attention_size)\n",
    "        self.key_dense = nn.Linear(key_size, attention_size)\n",
    "        self.value_dense = nn.Linear(query_size, value_size)\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        query = self.query_dense(q)\n",
    "        key = self.key_dense(k)\n",
    "        value = self.value_dense(v)\n",
    "        attention = torch.matmul(query, key.T) / math.sqrt(self.attention_size)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        output = torch.matmul(attention, value)\n",
    "        return output\n",
    "    \n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, code_num, code_size, graph_size, hidden_size, t_attention_size, t_output_size):\n",
    "        super().__init__()\n",
    "        self.unrelated_embedding = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, graph_size)))\n",
    "        self.gru = nn.GRUCell(input_size=graph_size, hidden_size=hidden_size)\n",
    "        self.attention = SingleHeadAttentionLayer(graph_size, graph_size, t_output_size, t_attention_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        self.code_num = code_num\n",
    "        self.hidden_size = hidden_size\n",
    "        self.code_size = code_size\n",
    "\n",
    "    def forward(self, t, co_embeddings, divided, no_embeddings, hidden_state=None):\n",
    "        m_p, m_en, m_eu = divided[:, 0], divided[:, 1], divided[:, 2]\n",
    "        mp_idx, men_idx, meu_idx = torch.where(m_p > 0)[0], torch.where(m_en > 0)[0], torch.where(m_eu > 0)[0]\n",
    "        h_new = torch.zeros((self.code_num, self.hidden_size), dtype=co_embeddings.dtype).to(co_embeddings.device)\n",
    "        output_mp = 0\n",
    "        output_meneu = 0\n",
    "\n",
    "        if len(mp_idx) > 0:\n",
    "            h = hidden_state[mp_idx] if hidden_state is not None else None\n",
    "            h_p = self.gru(co_embeddings[mp_idx], h)\n",
    "            h_new[mp_idx] = h_p\n",
    "            output_mp, _ = torch.max(h_p, dim=-2)\n",
    "        if t == 0 or len(men_idx) + len(meu_idx) == 0:\n",
    "            output = output_mp\n",
    "        else:\n",
    "            q = torch.vstack([no_embeddings[men_idx], self.unrelated_embedding[meu_idx]])\n",
    "            v = torch.vstack([co_embeddings[men_idx], co_embeddings[meu_idx]])\n",
    "            h_tilda = self.activation(self.attention(q, q, v))\n",
    "            h_new[men_idx] = h_tilda[:len(men_idx)]\n",
    "            h_new[meu_idx] = h_tilda[len(men_idx):]\n",
    "            output_meneu, _ = torch.max(h_tilda, dim=-2)\n",
    "            if len(mp_idx) == 0:\n",
    "                output = output_meneu\n",
    "            else:\n",
    "                output, _ = torch.max(torch.vstack([output_mp, output_meneu]), dim=-2)\n",
    "\n",
    "        return output, h_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Embedding layer\n",
    "\n",
    "Here we define the embedding layer for the model, combined with the dot product attention activation for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingWithAttentionLayer(nn.Module):\n",
    "    def __init__(self, value_size, attention_size):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        # define context vector\n",
    "        self.context = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(attention_size, 1)))\n",
    "        self.linear = nn.Linear(value_size, attention_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max pooling\n",
    "        t = self.linear(x)\n",
    "        # calculate attention score\n",
    "        score = torch.softmax(torch.matmul(t, self.context).squeeze(), dim=-1)\n",
    "        # final hidden embedding\n",
    "        output = torch.sum(x * torch.unsqueeze(score, dim=-1), dim=-2)\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Model and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.activation(self.dropout(self.linear(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still need further editing to integrate\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, code_num, code_size,\n",
    "                 adj, graph_size, hidden_size, t_attention_size, t_output_size,\n",
    "                 output_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.graph_layer = GraphLayer(adj, code_num, code_size, graph_size)\n",
    "        self.transition_layer = TransitionLayer(code_num, code_size, graph_size, hidden_size, t_attention_size, t_output_size)\n",
    "        self.attention = EmbeddingWithAttentionLayer(hidden_size, 32)\n",
    "        self.classifier = Classifier(hidden_size, output_size, dropout_rate)\n",
    "\n",
    "    def forward(self, code_x, divided, neighbors, lens):\n",
    "        output = []\n",
    "        for code_x_i, divided_i, neighbor_i, len_i in zip(code_x, divided, neighbors, lens):\n",
    "            no_embeddings_i_prev = None\n",
    "            output_i = []\n",
    "            h_t = None\n",
    "            for t, (c_it, d_it, n_it, len_it) in enumerate(zip(code_x_i, divided_i, neighbor_i, range(len_i))):\n",
    "                co_embeddings, no_embeddings = self.graph_layer(c_it, n_it)\n",
    "                output_it, h_t = self.transition_layer(t, co_embeddings, d_it, no_embeddings_i_prev, h_t)\n",
    "                no_embeddings_i_prev = no_embeddings\n",
    "                output_i.append(output_it)\n",
    "            output_i = self.attention(torch.vstack(output_i))\n",
    "            output.append(output_i)\n",
    "        output = torch.vstack(output)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define functions for training and evaluation\n",
    "\n",
    "### 4.1 Historical hot function\n",
    "\n",
    "We re-use the `historical_hot()` function directly from the [original codebase](https://github.com/LuChang-CS/Chet/blob/master/train.py). The function will be used later in model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_hot(code_x, code_num, lens):\n",
    "    result = np.zeros((len(code_x), code_num), dtype=int)\n",
    "    for i, (x, l) in enumerate(zip(code_x, lens)):\n",
    "        result[i] = x[l - 1]\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data loader function\n",
    "\n",
    "We create a data_loader() function to load the data needed for training and evaluating the model. The function is based on the [data loding code](https://github.com/LuChang-CS/Chet/blob/master/train.py#L45-L52) from the original authors and also re-uses several of the data loading helper functions from [`utils.py`](https://github.com/LuChang-CS/Chet/blob/master/utils.py) in the original codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import load_adj\n",
    "from utils import EHRDataset\n",
    "from utils import MultiStepLRScheduler\n",
    "\n",
    "def data_loader(task, dataset_path):\n",
    "    print('from {} for task {}:'.format(dataset_path, task))\n",
    "    print('loading code adjacency matrix ...')\n",
    "    code_adj = load_adj(dataset_path, device=device)\n",
    "    code_num = len(code_adj)\n",
    "    print('loading train data ...')\n",
    "    train_data = EHRDataset(os.path.join(dataset_path, \"train/\"), label=task, batch_size=batch_size, shuffle=True, device=device)\n",
    "    print('loading valid data ...')\n",
    "    valid_data = EHRDataset(os.path.join(dataset_path, \"valid/\"), label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "    print('loading test data ...')\n",
    "    test_data = EHRDataset(os.path.join(dataset_path, \"test/\"), label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "\n",
    "    return {\n",
    "        'dataset_name': dataset_path.split('/')[1],\n",
    "        'code_adj': code_adj, \n",
    "        'code_num': code_num, \n",
    "        'train_data': train_data, \n",
    "        'valid_data': valid_data, \n",
    "        'test_data': test_data, \n",
    "        }\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils import format_time\n",
    "\n",
    "def train_chet(path, task, output_size, evaluate_fn, code_adj, code_num, dropout_rate, \n",
    "               train_data, valid_data, init_lr, lrs, milestones, epochs):\n",
    "    test_historical = historical_hot(valid_data.code_x, code_num, valid_data.visit_lens)\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    \n",
    "    # Keep the same model param storage path as the original paper\n",
    "    param_path = os.path.join('data', 'params', path, task)\n",
    "    if not os.path.exists(param_path):\n",
    "        os.makedirs(param_path)\n",
    "\n",
    "    # Keep the same model, optimizer, and scheduler as the original paper,\n",
    "    # but slightly modified to leverage the new config dict\n",
    "    model = Model(code_num=code_num, code_size=code_size,\n",
    "                    adj=code_adj, graph_size=graph_size, hidden_size=hidden_size, t_attention_size=t_attention_size,\n",
    "                    t_output_size=t_output_size,\n",
    "                    output_size=output_size, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = MultiStepLRScheduler(optimizer, epochs, init_lr, milestones, lrs)\n",
    "\n",
    "    # Keep the same param printing code as the original paper\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(pytorch_total_params)\n",
    "\n",
    "    # Keep the same training loop code as the original paper, but note that\n",
    "    # the train, valid, and test data will change based on the task and\n",
    "    # dataset of the current loop\n",
    "    epoch_lrs, valid_losses, mean_losses, time_costs, f1_scores, auc_or_topks = [], [], [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch %d / %d:' % (epoch + 1, epochs))\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_num = 0\n",
    "        steps = len(train_data)\n",
    "        st = time.time()\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.lrs[epoch]\n",
    "        for step in range(len(train_data)):\n",
    "            optimizer.zero_grad()\n",
    "            code_x, visit_lens, divided, y, neighbors = train_data[step]\n",
    "            output = model(code_x, divided, neighbors, visit_lens).squeeze()\n",
    "            loss = loss_fn(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * output_size * len(code_x)\n",
    "            total_num += len(code_x)\n",
    "\n",
    "            end_time = time.time()\n",
    "            remaining_time = format_time((end_time - st) / (step + 1) * (steps - step - 1))\n",
    "            print('\\r    Step %d / %d, LR: %s, remaining time: %s, loss: %.4f'\n",
    "                % (step + 1, steps, current_lr, remaining_time, total_loss / total_num), end='')\n",
    "        train_data.on_epoch_end()\n",
    "        et = time.time()\n",
    "        time_cost = format_time(et - st)\n",
    "        mean_loss = total_loss / total_num\n",
    "        print('\\r    Step %d / %d, LR: %s, time cost: %s, loss: %.4f' % (steps, steps, current_lr, time_cost, mean_loss))\n",
    "        valid_loss, f1_score, auc_or_topk = evaluate_fn(model, valid_data, loss_fn, output_size, test_historical)\n",
    "        torch.save(model.state_dict(), os.path.join(param_path, '%d.pt' % epoch))\n",
    "        epoch_lrs.append(current_lr)\n",
    "        valid_losses.append(valid_loss)\n",
    "        mean_losses.append(mean_loss)\n",
    "        time_costs.append(time_cost)\n",
    "        f1_scores.append(f1_score)\n",
    "        auc_or_topks.append(auc_or_topk)\n",
    "        \n",
    "    return {\n",
    "        'model': model,\n",
    "        'epoch_lrs': epoch_lrs,\n",
    "        'valid_losses': valid_losses,\n",
    "        'mean_losses': mean_losses,\n",
    "        'time_costs': time_costs,\n",
    "        'f1_scores': f1_scores,\n",
    "        'auc_or_topks': auc_or_topks,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model evaluation function\n",
    "We create a `test()` function to evaluate the model on the `test_data`. We re-use the `evaluate_codes()` and `evaluate_hf()` functions directly from the original [metrics.py](https://github.com/LuChang-CS/Chet/blob/master/metrics.py) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(evaluate_fn, model, test_data, code_num, loss_fn, output_size):\n",
    "    print(\"Evaluating model on test data...\")\n",
    "    model.eval()\n",
    "    historical = historical_hot(test_data.code_x, code_num, test_data.visit_lens)\n",
    "    test_loss, f1_score, auc_or_topk = evaluate_fn(model, test_data, loss_fn, output_size, historical=historical)\n",
    "    print(\"Test loss: %s, F1 score: %s, AUC or TopK: %s\" % (test_loss, f1_score, auc_or_topk))\n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'f1_score': f1_score,\n",
    "        'auc_or_topk': auc_or_topk,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the preprocessed data\n",
    "\n",
    "Here we load the preprocessed data using the data_loader() function defined above, for both the MIMIC-III and MIMIC-IV datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from data/mimic3/standard/ for task m:\n",
      "loading code adjacency matrix ...\n",
      "loading train data ...\n",
      "loading valid data ...\n",
      "loading test data ...\n",
      "from data/mimic4/standard/ for task m:\n",
      "loading code adjacency matrix ...\n",
      "loading train data ...\n",
      "loading valid data ...\n",
      "loading test data ...\n",
      "from data/mimic3/standard/ for task h:\n",
      "loading code adjacency matrix ...\n",
      "loading train data ...\n",
      "loading valid data ...\n",
      "loading test data ...\n",
      "from data/mimic4/standard/ for task h:\n",
      "loading code adjacency matrix ...\n",
      "loading train data ...\n",
      "loading valid data ...\n",
      "loading test data ...\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "# Todo: Add data analysis here to explain each sub datasets eg: code_x, visit_lens, divided, y, neighbors, code_adj. Also can give an example by printing some data\n",
    "# Todo: Add some initial data analysis. 1) number/ratio of heart failure patients. 2) some statistic for neighbors\n",
    "# Possible Todo: build our own data builder using torch.utils.data.DataLoader as HWs\n",
    "\n",
    "tasks = ['m', 'h']\n",
    "mimic4_standard_path = \"data/mimic4/standard/\"\n",
    "mimic3_standard_path = \"data/mimic3/standard/\"\n",
    "mimic3_datasets, mimic4_datasets = [], []\n",
    "for task in tasks:\n",
    "    mimic3_datasets.append(data_loader(task, mimic3_standard_path))\n",
    "    mimic4_datasets.append(data_loader(task, mimic4_standard_path))\n",
    "print(\"data loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for m task on mimic3 dataset:\n",
      "1223574\n",
      "Epoch 1 / 200:\n",
      "    Step 188 / 188, LR: 0.01, time cost: 4m56.9s, loss: 425855.461717714\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 2 / 200:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m55.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 3 / 200:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m39.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 4 / 200:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m39.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 5 / 200:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m35.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 6 / 200:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m35.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 7 / 200:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m35.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 8 / 200:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m35.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 9 / 200:\n",
      "    Step 188 / 188, LR: 0.001, time cost: 4m36.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 10 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 11 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m41.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 12 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 13 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 14 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 15 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 16 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 17 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 18 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 19 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 20 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 21 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 22 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 23 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 24 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 25 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 26 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 27 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 28 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 29 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 30 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 31 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 32 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 33 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 34 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 35 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 36 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 37 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 38 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 39 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m37.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 40 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 41 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 42 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 43 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 44 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 45 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 46 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 47 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 48 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 49 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 50 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 51 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 52 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 53 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 54 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 55 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 56 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 57 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 58 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 59 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 60 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 61 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 62 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 63 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 64 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 65 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 66 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 67 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 68 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 69 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 70 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 71 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 72 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 73 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 74 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m31.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 75 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 76 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 77 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 78 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 79 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 80 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 81 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 82 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 83 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 84 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 85 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 86 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 87 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 88 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 89 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 90 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 91 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 92 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 93 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 94 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 95 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 96 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 97 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m37.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 98 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 99 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 100 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 101 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m37.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 102 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 103 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 104 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 105 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 106 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 107 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 108 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 109 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 110 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 111 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 112 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m52.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 113 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m56.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 114 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m45.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 115 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m39.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 116 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m38.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 117 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 118 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 119 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 120 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 121 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 122 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 123 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 124 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 125 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 126 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 127 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 128 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 129 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 130 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 131 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 132 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 133 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 134 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 135 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 136 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 137 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 138 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 139 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 140 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 141 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 142 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m38.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 143 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 144 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 145 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 146 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 147 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 148 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 149 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m37.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 150 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 151 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 152 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m37.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 153 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 154 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 155 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 156 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 157 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 158 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m37.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 159 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 160 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 161 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 162 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 163 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m37.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 164 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 165 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 166 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 167 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 168 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 169 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.9s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 170 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m37.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 171 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 172 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 173 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 174 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 175 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 176 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 177 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 178 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 179 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 180 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 181 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 182 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.8s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 183 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 184 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 185 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 186 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.7s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 187 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.6s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 188 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 189 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 190 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m36.0s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 191 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 192 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 193 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 194 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 195 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m32.2s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 196 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.3s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 197 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m34.1s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 198 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m33.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 199 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m31.4s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Epoch 200 / 200:\n",
      "    Step 188 / 188, LR: 1e-05, time cost: 4m35.5s, loss: 488000.000000000\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0609 --- top_k_recall: 0.0788, 0.1648, 0.2051, 0.2222  --- occurred: 0.0469, 0.0915, 0.1101, 0.1150  --- not occurred: 0.0320, 0.0733, 0.0951, 0.1072\n",
      "Evaluating model on test data...\n",
      "    Evaluation: loss: 488000.0000 --- f1_score: 0.0611 --- top_k_recall: 0.0832, 0.1707, 0.2109, 0.2299  --- occurred: 0.0484, 0.0953, 0.1123, 0.1178  --- not occurred: 0.0348, 0.0754, 0.0986, 0.1120\n",
      "Test loss: 488000.0, F1 score: 0.06114362130312985, AUC or TopK: [0.0831914575016104, 0.17072478903938623, 0.21091925386452154, 0.2298691450240484]\n",
      "training for m task on mimic4 dataset:\n",
      "1490841\n",
      "Epoch 1 / 200:\n",
      "    Step 250 / 250, LR: 0.01, time cost: 22m4.4s, loss: 589433.9260600255\n",
      "    Evaluation: loss: 603700.0000 --- f1_score: 0.0422 --- top_k_recall: 0.0431, 0.0772, 0.1619, 0.2127  --- occurred: 0.0247, 0.0394, 0.0901, 0.1188  --- not occurred: 0.0184, 0.0378, 0.0719, 0.0939\n",
      "Epoch 2 / 200:\n",
      "    Step 250 / 250, LR: 0.001, time cost: 22m26.8s, loss: 603700.000000000\n",
      "    Evaluation: loss: 603700.0000 --- f1_score: 0.0422 --- top_k_recall: 0.0431, 0.0772, 0.1619, 0.2127  --- occurred: 0.0247, 0.0394, 0.0901, 0.1188  --- not occurred: 0.0184, 0.0378, 0.0719, 0.0939\n",
      "Epoch 3 / 200:\n",
      "    Step 250 / 250, LR: 0.001, time cost: 22m28.4s, loss: 603700.000000000\n",
      "    Evaluation: loss: 603700.0000 --- f1_score: 0.0422 --- top_k_recall: 0.0431, 0.0772, 0.1619, 0.2127  --- occurred: 0.0247, 0.0394, 0.0901, 0.1188  --- not occurred: 0.0184, 0.0378, 0.0719, 0.0939\n",
      "Epoch 4 / 200:\n",
      "    Step 250 / 250, LR: 0.001, time cost: 22m31.8s, loss: 603700.000000000\n",
      "    Evaluation: loss: 603700.0000 --- f1_score: 0.0422 --- top_k_recall: 0.0431, 0.0772, 0.1619, 0.2127  --- occurred: 0.0247, 0.0394, 0.0901, 0.1188  --- not occurred: 0.0184, 0.0378, 0.0719, 0.0939\n",
      "Epoch 5 / 200:\n",
      "    Step 250 / 250, LR: 0.001, time cost: 22m29.9s, loss: 603700.000000000\n",
      "    Evaluation: loss: 603700.0000 --- f1_score: 0.0422 --- top_k_recall: 0.0431, 0.0772, 0.1619, 0.2127  --- occurred: 0.0247, 0.0394, 0.0901, 0.1188  --- not occurred: 0.0184, 0.0378, 0.0719, 0.0939\n",
      "Epoch 6 / 200:\n",
      "    Step 250 / 250, LR: 0.001, time cost: 22m40.0s, loss: 603700.000000000\n",
      "    Evaluation: loss: 603700.0000 --- f1_score: 0.0422 --- top_k_recall: 0.0431, 0.0772, 0.1619, 0.2127  --- occurred: 0.0247, 0.0394, 0.0901, 0.1188  --- not occurred: 0.0184, 0.0378, 0.0719, 0.0939\n",
      "Epoch 7 / 200:\n",
      "    Step 250 / 250, LR: 0.001, time cost: 22m34.2s, loss: 603700.000000000\n",
      "    Evaluation: loss: 603700.0000 --- f1_score: 0.0422 --- top_k_recall: 0.0431, 0.0772, 0.1619, 0.2127  --- occurred: 0.0247, 0.0394, 0.0901, 0.1188  --- not occurred: 0.0184, 0.0378, 0.0719, 0.0939\n",
      "Epoch 8 / 200:\n",
      "    Step 250 / 250, LR: 0.001, time cost: 22m41.3s, loss: 603700.000000000\n",
      "    Evaluation: loss: 603700.0000 --- f1_score: 0.0422 --- top_k_recall: 0.0431, 0.0772, 0.1619, 0.2127  --- occurred: 0.0247, 0.0394, 0.0901, 0.1188  --- not occurred: 0.0184, 0.0378, 0.0719, 0.0939\n",
      "Epoch 9 / 200:\n",
      "    Step 250 / 250, LR: 0.001, time cost: 22m39.6s, loss: 603700.000000000\n",
      "    Evaluation: loss: 603700.0000 --- f1_score: 0.0422 --- top_k_recall: 0.0431, 0.0772, 0.1619, 0.2127  --- occurred: 0.0247, 0.0394, 0.0901, 0.1188  --- not occurred: 0.0184, 0.0378, 0.0719, 0.0939\n",
      "Epoch 10 / 200:\n",
      "    Step 250 / 250, LR: 1e-05, time cost: 22m42.4s, loss: 603700.000000000\n",
      "    Evaluation: loss: 603700.0000 --- f1_score: 0.0422 --- top_k_recall: 0.0431, 0.0772, 0.1619, 0.2127  --- occurred: 0.0247, 0.0394, 0.0901, 0.1188  --- not occurred: 0.0184, 0.0378, 0.0719, 0.0939\n",
      "Epoch 11 / 200:\n",
      "    Step 66 / 250, LR: 1e-05, remaining time: 15m50.0s, loss: 603700.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m train_results \u001b[39m=\u001b[39m {}\n\u001b[1;32m     15\u001b[0m test_results \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 16\u001b[0m train_results[dataset[t][\u001b[39m'\u001b[39m\u001b[39mdataset_name\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m train_chet(\n\u001b[1;32m     17\u001b[0m     path\u001b[39m=\u001b[39;49mdataset[t][\u001b[39m'\u001b[39;49m\u001b[39mdataset_name\u001b[39;49m\u001b[39m'\u001b[39;49m],task\u001b[39m=\u001b[39;49mtask, output_size\u001b[39m=\u001b[39;49moutput_size_, \n\u001b[1;32m     18\u001b[0m     evaluate_fn\u001b[39m=\u001b[39;49mevaluate_fn_, code_adj\u001b[39m=\u001b[39;49mdataset[t][\u001b[39m'\u001b[39;49m\u001b[39mcode_adj\u001b[39;49m\u001b[39m'\u001b[39;49m], code_num\u001b[39m=\u001b[39;49mdataset[t][\u001b[39m'\u001b[39;49m\u001b[39mcode_num\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     19\u001b[0m     dropout_rate\u001b[39m=\u001b[39;49mdropout_rate_, train_data\u001b[39m=\u001b[39;49mdataset[t][\u001b[39m'\u001b[39;49m\u001b[39mtrain_data\u001b[39;49m\u001b[39m'\u001b[39;49m], valid_data\u001b[39m=\u001b[39;49mdataset[t][\u001b[39m'\u001b[39;49m\u001b[39mvalid_data\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     20\u001b[0m     init_lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, lrs\u001b[39m=\u001b[39;49mlrs_, milestones\u001b[39m=\u001b[39;49mmilestones_\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m test_results[dataset[t][\u001b[39m'\u001b[39m\u001b[39mdataset_name\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m test(\n\u001b[1;32m     23\u001b[0m     evaluate_fn\u001b[39m=\u001b[39mevaluate_fn_, model\u001b[39m=\u001b[39mtrain_results[dataset[t][\u001b[39m'\u001b[39m\u001b[39mdataset_name\u001b[39m\u001b[39m'\u001b[39m]][\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     24\u001b[0m     code_num\u001b[39m=\u001b[39mdataset[t][\u001b[39m'\u001b[39m\u001b[39mcode_num\u001b[39m\u001b[39m'\u001b[39m], test_data\u001b[39m=\u001b[39mdataset[t][\u001b[39m'\u001b[39m\u001b[39mtest_data\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     25\u001b[0m     loss_fn\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCELoss(), output_size\u001b[39m=\u001b[39moutput_size_\n\u001b[1;32m     26\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m, in \u001b[0;36mtrain_chet\u001b[0;34m(path, task, output_size, evaluate_fn, code_adj, code_num, dropout_rate, train_data, valid_data, init_lr, lrs, milestones)\u001b[0m\n\u001b[1;32m     43\u001b[0m output \u001b[39m=\u001b[39m model(code_x, divided, neighbors, visit_lens)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     44\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, y)\n\u001b[0;32m---> 45\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     46\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     47\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m output_size \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(code_x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from metrics import evaluate_codes, evaluate_hf\n",
    "\n",
    "epochs = 200 # epochs = 200 in original paper\n",
    "\n",
    "for task in tasks:\n",
    "    t = 0 if task == 'm' else 1\n",
    "    dropout_rate_ = 0.45 if task == 'm' else 0.0\n",
    "    lrs_ = [1e-3, 1e-5] if task == 'm' else [1e-3, 1e-4, 1e-5]\n",
    "    milestones_ = [20, 30] if task == 'm' else [2, 3, 20] # [2, 10] [2,3,5]\n",
    "    evaluate_fn_ = evaluate_codes if task == 'm' else evaluate_hf\n",
    "    for dataset in [mimic3_datasets, mimic4_datasets]:\n",
    "        output_size_ = dataset[t]['code_num'] if task == 'm' else 1\n",
    "        print('training for %s task on %s dataset:' % (task, dataset[0]['dataset_name']))\n",
    "        train_results = {}\n",
    "        test_results = {}\n",
    "        train_results[dataset[t]['dataset_name']] = train_chet(\n",
    "            path=dataset[t]['dataset_name'],task=task, output_size=output_size_, \n",
    "            evaluate_fn=evaluate_fn_, code_adj=dataset[t]['code_adj'], code_num=dataset[t]['code_num'], \n",
    "            dropout_rate=dropout_rate_, train_data=dataset[t]['train_data'], valid_data=dataset[t]['valid_data'], \n",
    "            init_lr=0.01, lrs=lrs_, milestones=milestones_, epochs=epochs,\n",
    "            )\n",
    "        test_results[dataset[t]['dataset_name']] = test(\n",
    "            evaluate_fn=evaluate_fn_, model=train_results[dataset[t]['dataset_name']]['model'], \n",
    "            code_num=dataset[t]['code_num'], test_data=dataset[t]['test_data'], \n",
    "            loss_fn=torch.nn.BCELoss(), output_size=output_size_,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# analyze the results here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 ('chet_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7306b834e6e06fbd654ec785d993585f279ffa0a060b5378ceb65f5750836ec3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
