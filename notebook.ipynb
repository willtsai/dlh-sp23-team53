{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# set device\n",
    "device = torch.device('cpu')\n",
    "# set task\n",
    "task = 'h' \n",
    "# set batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient num: 7493\n",
      "max admission num: 42\n",
      "mean admission num: 2.66\n",
      "max code num in an admission: 39\n",
      "mean code num in an admission: 13.06\n",
      "encoding code ...\n",
      "There are 4880 codes\n",
      "generating code levels ...\n",
      "\t100%00%\n",
      "There are 5712 train, 781 valid, 1000 test samples\n",
      "generating code code adjacent matrix ...\n",
      "\t5712 / 5712\n",
      "building train codes features and labels ...\n",
      "\t5712 / 5712\n",
      "building valid codes features and labels ...\n",
      "\t781 / 781\n",
      "building test codes features and labels ...\n",
      "\t1000 / 1000\n",
      "generating train neighbors ...\n",
      "\t5712 / 5712\n",
      "generating valid neighbors ...\n",
      "\t781 / 781\n",
      "generating test neighbors ...\n",
      "\t1000 / 1000\n",
      "generating train middles ...\n",
      "\t5712 / 5712\n",
      "generating valid middles ...\n",
      "\t781 / 781\n",
      "generating test middles ...\n",
      "\t1000 / 1000\n",
      "building train heart failure labels ...\n",
      "building valid heart failure labels ...\n",
      "building test heart failure labels ...\n",
      "saving encoded data ...\n",
      "saving standard data ...\n",
      "\tsaving training data\n",
      "\tsaving valid data\n",
      "\tsaving test data\n"
     ]
    }
   ],
   "source": [
    "# run preprocessor\n",
    "from run_preprocess import preprocess\n",
    "preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Todo: Add data analysis here to explain each sub datasets eg: code_x, visit_lens, divided, y, neighbors, code_adj. Also can give an example by printing some data\n",
    "# Todo: Add some initial data analysis. 1) number/ratio of heart failure patients. 2) some statistic for neighbors\n",
    "# Possible Todo: build our own data builder using torch.utils.data.DataLoader as HWs\n",
    "\n",
    "from preprocess import load_sparse\n",
    "from utils import load_adj, EHRDataset\n",
    "DATA_PATH = \"data/mimic3/\"\n",
    "\n",
    "code_adj = load_adj(os.path.join(DATA_PATH, 'standard'), device=device)\n",
    "train_data = EHRDataset(os.path.join(DATA_PATH, 'standard/train'), label=task, batch_size=batch_size, shuffle=True, device=device)\n",
    "valid_data = EHRDataset(os.path.join(DATA_PATH, 'standard/valid'), label=task, batch_size=batch_size, shuffle=False, device=device)\n",
    "test_data = EHRDataset(os.path.join(DATA_PATH, 'standard/test'), label=task, batch_size=batch_size, shuffle=False, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLayer(nn.Module):\n",
    "    def __init__(self, adj, code_num, code_size, graph_size):\n",
    "        super().__init__()\n",
    "        self.embedding =  nn.Embedding(code_num, code_size)\n",
    "        self.adj = adj \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(code_size, graph_size)\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, code_x, neighbor):\n",
    "        # embedding matrices for for diseases appearing in current diagnoses\n",
    "        M_embedding_matrices = self.embedding(code_x)\n",
    "        # embedding matrices for for diseases appearing in direct neighbors\n",
    "        N_embedding_matrices = self.embedding(neighbor)\n",
    "        # static adjacency matrix\n",
    "        # keep these unsqueeze for now, may need change if we change the data loader\n",
    "        center_codes = torch.unsqueeze(code_x, dim=-1)\n",
    "        neighbor_codes = torch.unsqueeze(neighbor, dim=-1)\n",
    "\n",
    "        center_embeddings = center_codes * M_embedding_matrices\n",
    "        neighbor_embeddings = neighbor_codes * N_embedding_matrices\n",
    "\n",
    "        adj_mul_center = torch.matmul(self.adj, center_embeddings)\n",
    "        adj_mul_neighbor = torch.matmul(self.adj, neighbor_embeddings)\n",
    "\n",
    "        # All the calculation here are using the memory-efficient calculation as proved by the author in Subgraphs' Adjacency Matrix Calculation\n",
    "        # aggregated diagnosis local context and diagnosis global context\n",
    "        aggregated_diagnosis_embedding = center_embeddings + center_codes * adj_mul_center + center_codes * adj_mul_neighbor\n",
    "        # aggregated neighbor global context\n",
    "        aggregated_neighbor_embedding = neighbor_embeddings + neighbor_codes * adj_mul_neighbor + neighbor_codes * adj_mul_center\n",
    "\n",
    "        # hidden embeddings of diagnoses and neighbors\n",
    "        hidden_diagnosis_embedding = self.LeakyReLU(self.fc(aggregated_diagnosis_embedding))\n",
    "        hidden_neighbor_embedding = self.LeakyReLU(self.fc(aggregated_neighbor_embedding))\n",
    "        return hidden_diagnosis_embedding, hidden_neighbor_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithAttentionLayer(nn.Module):\n",
    "    def __init__(self, value_size, attention_size):\n",
    "        super().__init__()\n",
    "        self.attention_size = attention_size\n",
    "        # define context vector\n",
    "        self.context = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(attention_size, 1)))\n",
    "        self.linear = nn.Linear(value_size, attention_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max pooling\n",
    "        t = self.linear(x)\n",
    "        # calculate attention score\n",
    "        score = torch.softmax(torch.matmul(t, self.context).squeeze(), dim=-1)\n",
    "        # final hidden embedding\n",
    "        output = torch.matmul(self.context, torch.transpose(t))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.activation(self.dropout(self.linear(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still need further editing to integrate\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, code_num, code_size,\n",
    "                 adj, graph_size, hidden_size, t_attention_size, t_output_size,\n",
    "                 output_size, dropout_rate, activation):\n",
    "        super().__init__()\n",
    "        self.graph_layer = GraphLayer(adj, code_size, graph_size)\n",
    "        self.transition_layer = TransitionLayer(code_num, graph_size, hidden_size, t_attention_size, t_output_size)\n",
    "        self.attention = EmbeddingWithAttentionLayer(hidden_size, 32)\n",
    "        self.classifier = Classifier(hidden_size, output_size, dropout_rate, activation)\n",
    "\n",
    "    def forward(self, code_x, divided, neighbors, lens):\n",
    "        output = []\n",
    "        for code_x_i, divided_i, neighbor_i, len_i in zip(code_x, divided, neighbors, lens):\n",
    "            no_embeddings_i_prev = None\n",
    "            output_i = []\n",
    "            h_t = None\n",
    "            for t, (c_it, d_it, n_it, len_it) in enumerate(zip(code_x_i, divided_i, neighbor_i, range(len_i))):\n",
    "                co_embeddings, no_embeddings = self.graph_layer(c_it, n_it, c_embeddings, n_embeddings)\n",
    "                output_it, h_t = self.transition_layer(t, co_embeddings, d_it, no_embeddings_i_prev, u_embeddings, h_t)\n",
    "                no_embeddings_i_prev = no_embeddings\n",
    "                output_i.append(output_it)\n",
    "            output_i = self.attention(torch.vstack(output_i))\n",
    "            output.append(output_i)\n",
    "        output = torch.vstack(output)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 ('chet_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "7306b834e6e06fbd654ec785d993585f279ffa0a060b5378ceb65f5750836ec3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
