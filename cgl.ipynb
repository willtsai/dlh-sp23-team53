{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of Chet baseline model for comparison: CGL model (Lu et al., 2021)\n",
    "\n",
    "**UIUC, CS598 DL4H, Spring 2023**\n",
    "\n",
    "**Authors:** Shiyu (Sherry) Li and Wei-Lun (Will) Tsai; {shiyuli2, wltsai2}@illinois.edu\n",
    "\n",
    "**Original paper:** Chang Lu, Chandan K. Reddy, Prithwish Chakraborty, Samantha\n",
    "Kleinberg, and Yue Ning. 2021. [Collaborative Graph Learning with Auxiliary Text\n",
    "for Temporal Event Prediction in Healthcare](https://arxiv.org/pdf/2105.07542.pdf) In \n",
    "*Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21*,\n",
    "pages 3529â€“3535. Interna- tional Joint Conferences on Artificial Intelligence\n",
    "Organization. Main Track.\n",
    "\n",
    "**Original codebase:** [github.com/LuChang-CS/CGL](https://github.com/LuChang-CS/CGL)\n",
    "\n",
    "We re-used the CLG model code from the original repository, but with the following modifications:\n",
    "1. Combined the code from the original classes `layers.py`, `model.py`, `loss.py`, `metrics.py`, `utils.py`, `train_codes.py`, and `train_codes.py` into a single notebook `cgl.ipynb` for ease of sequential execution.\n",
    "1. Modified the model definition and training code to remove the use of clinical notes in the CGL baseline to ensure a fair baseline comparison, as specified in the Chet paper (Lu et al., 2022).\n",
    "1. Modified the training code to use the same preprocessed data and train/valid/test splits as the Chet paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model architecture\n",
    "\n",
    "### 1.1. Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Dense, BatchNormalization, GRUCell\n",
    "from keras.layers import Concatenate, Activation, RNN, StackedRNNCells\n",
    "from keras.initializers import GlorotUniform\n",
    "\n",
    "\n",
    "class HierarchicalEmbedding(Layer):\n",
    "    def __init__(self, code_levels, code_num_in_levels, code_dims, name='hierarchical_embedding'):\n",
    "        super().__init__(name=name)\n",
    "        self.level_num = len(code_num_in_levels)\n",
    "        self.code_levels = code_levels  # (leaf code num * level_num)\n",
    "        self.level_embeddings = [self.add_weight(name='hier_emb_level_%d' % level,\n",
    "                                                 shape=(code_num, code_dim),\n",
    "                                                 initializer=GlorotUniform(),\n",
    "                                                 trainable=True)\n",
    "                                 for level, (code_num, code_dim) in enumerate(zip(code_num_in_levels, code_dims))]\n",
    "\n",
    "    def call(self, inputs=None):\n",
    "        \"\"\"\n",
    "            return: (code_num, embedding_size)\n",
    "        \"\"\"\n",
    "        embeddings = [tf.nn.embedding_lookup(self.level_embeddings[level], self.code_levels[:, level])\n",
    "                      for level in range(self.level_num)]\n",
    "        embeddings = Concatenate()(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class PatientEmbedding(Layer):\n",
    "    def __init__(self, patient_num, patient_dim, name='patient_embedding'):\n",
    "        super().__init__(name=name)\n",
    "        self.patient_embeddings = self.add_weight(name='p_emb',\n",
    "                                                  shape=(patient_num, patient_dim),\n",
    "                                                  initializer=GlorotUniform(),\n",
    "                                                  trainable=True)\n",
    "\n",
    "    def call(self, inputs=None):\n",
    "        return self.patient_embeddings\n",
    "\n",
    "\n",
    "class GraphConvBlock(Layer):\n",
    "    def __init__(self, node_type, dim, adj, name='graph_conv_block'):\n",
    "        super().__init__(name=name)\n",
    "        self.node_type = node_type\n",
    "        self.adj = adj\n",
    "        self.dense = Dense(dim, activation=None, name=name + '_dense')\n",
    "        self.activation = Activation('relu', name=name + '_activation')\n",
    "        self.bn = BatchNormalization(name=name + 'bn')\n",
    "\n",
    "    def call(self, embedding, embedding_neighbor, weight_decay=None):\n",
    "        output = embedding + tf.matmul(self.adj, embedding_neighbor)\n",
    "        if self.node_type == 'code':\n",
    "            assert weight_decay is not None\n",
    "            output += tf.matmul(weight_decay, embedding)\n",
    "        output = self.dense(output)\n",
    "        output = self.bn(output)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def norm_no_nan(x):\n",
    "    return tf.math.divide_no_nan(x, tf.reduce_sum(x, axis=-1, keepdims=True))\n",
    "\n",
    "\n",
    "class GraphConvolution(Layer):\n",
    "    def __init__(self, patient_dim, code_dim,\n",
    "                 patient_code_adj, code_code_adj,\n",
    "                 patient_hidden_dims, code_hidden_dims, name='graph_convolution'):\n",
    "        super().__init__(name=name)\n",
    "        self.patient_code_adj = norm_no_nan(patient_code_adj)  # (patient_num, code_num)\n",
    "        self.code_patient_adj = norm_no_nan(tf.transpose(patient_code_adj))  # (code_num, patient_num)\n",
    "        self.code_code_adj = code_code_adj  # (code_num, code_num)\n",
    "\n",
    "        self.patient_blocks = [\n",
    "            GraphConvBlock('patient', dim, self.patient_code_adj, name='patient_graph_block_%d' % layer)\n",
    "            for layer, dim in enumerate(patient_hidden_dims)]\n",
    "        self.code_blocks = [GraphConvBlock('code', dim, self.code_patient_adj, name='code_graph_block_%d' % layer)\n",
    "                            for layer, dim in enumerate(code_hidden_dims)]\n",
    "\n",
    "        c2p_dims = ([patient_dim] + patient_hidden_dims)[:-1]\n",
    "        p2c_dims = ([code_dim] + code_hidden_dims)[:-1]\n",
    "        self.c2p_denses = [Dense(dim, activation=None, name='code_to_patient_dense_%d' % layer)\n",
    "                           for layer, dim in enumerate(c2p_dims)]\n",
    "        self.p2c_denses = [Dense(dim, activation=None, name='patient_to_code_dense_%d' % layer)\n",
    "                           for layer, dim in enumerate(p2c_dims)]\n",
    "\n",
    "        code_num = code_code_adj.shape[0]\n",
    "        self.miu = self.add_weight(name='miu', shape=(code_num,), trainable=True)\n",
    "        self.theta = self.add_weight(name='theta', shape=(code_num,), trainable=True)\n",
    "\n",
    "    def call(self, patient_embeddings, code_embeddings):\n",
    "        weight_decay = tf.nn.sigmoid(self.miu * self.code_code_adj + self.theta)\n",
    "        weight_decay = norm_no_nan(weight_decay)\n",
    "        # weight_decay = None\n",
    "        for c2p_dense, p2c_dense, patient_block, code_block in zip(self.c2p_denses, self.p2c_denses,\n",
    "                                                                   self.patient_blocks, self.code_blocks):\n",
    "            code_embeddings_p = c2p_dense(code_embeddings)\n",
    "            patient_embeddings_new = patient_block(patient_embeddings, code_embeddings_p)\n",
    "            patient_embeddings_c = p2c_dense(patient_embeddings)\n",
    "            code_embeddings = code_block(code_embeddings, patient_embeddings_c, weight_decay)\n",
    "            patient_embeddings = patient_embeddings_new\n",
    "        patient_embeddings_c = self.p2c_denses[-1](patient_embeddings)\n",
    "        code_embeddings = self.code_blocks[-1](code_embeddings, patient_embeddings_c, weight_decay)\n",
    "        return patient_embeddings, code_embeddings\n",
    "\n",
    "\n",
    "class VisitEmbedding(Layer):\n",
    "    def __init__(self, max_seq_len, name='visit_embedding'):\n",
    "        super().__init__(name=name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def call(self, code_embeddings, visit_codes, visit_lens):\n",
    "        \"\"\"\n",
    "            visit_codes: (batch_size, max_seq_len, max_code_num_in_a_visit)\n",
    "        \"\"\"\n",
    "        visit_codes_embedding = tf.nn.embedding_lookup(code_embeddings, visit_codes)  # (batch_size, max_seq_len, max_code_num_in_a_visit, code_dim)\n",
    "        visit_codes_mask = tf.expand_dims(visit_codes > 0, axis=-1)\n",
    "        visit_codes_mask = tf.cast(visit_codes_mask, visit_codes_embedding.dtype)\n",
    "        visit_codes_embedding *= visit_codes_mask  # (batch_size, max_seq_len, max_code_num_in_a_visit, code_dim)\n",
    "        visit_codes_num = tf.expand_dims(tf.reduce_sum(tf.cast(visit_codes > 0, visit_codes_embedding.dtype), axis=-1), axis=-1)\n",
    "        visits_embeddings = tf.math.divide_no_nan(tf.reduce_sum(visit_codes_embedding, axis=-2), visit_codes_num)  # (batch_size, max_seq_len, code_dim)\n",
    "        visit_mask = tf.expand_dims(tf.sequence_mask(visit_lens, self.max_seq_len, dtype=visits_embeddings.dtype), axis=-1)  # (batch_size, max_seq_len, 1)\n",
    "        visits_embeddings *= visit_mask  # (batch_size, max_seq_len, code_dim)\n",
    "        return visits_embeddings\n",
    "\n",
    "\n",
    "def masked_softmax(inputs, mask):\n",
    "    inputs = inputs - tf.reduce_max(inputs, keepdims=True, axis=-1)\n",
    "    exp = tf.exp(inputs) * mask\n",
    "    result = tf.math.divide_no_nan(exp, tf.reduce_sum(exp, keepdims=True, axis=-1))\n",
    "    return result\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, attention_dim, name='attention'):\n",
    "        super().__init__(name=name)\n",
    "        self.attention_dim = attention_dim\n",
    "        self.u_omega = self.add_weight(name=name + '_u', shape=(attention_dim,), initializer=GlorotUniform())\n",
    "        self.w_omega = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        hidden_size = input_shape[-1]\n",
    "        self.w_omega = self.add_weight(name=self.name + '_w', shape=(hidden_size, self.attention_dim), initializer=GlorotUniform())\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \"\"\"\n",
    "            x: (batch_size, max_seq_len, rnn_dim[-1] / hidden_size)\n",
    "        \"\"\"\n",
    "        t = tf.matmul(x, self.w_omega)\n",
    "        vu = tf.tensordot(t, self.u_omega, axes=1)  # (batch_size, max_seq_len)\n",
    "        if mask is not None:\n",
    "            vu *= mask\n",
    "            alphas = masked_softmax(vu, mask)\n",
    "        else:\n",
    "            alphas = tf.nn.softmax(vu)  # (batch_size, max_seq_len)\n",
    "        output = tf.reduce_sum(x * tf.expand_dims(alphas, -1), axis=-2)  # (batch_size, rnn_dim[-1] / hidden_size)\n",
    "        return output, alphas\n",
    "\n",
    "\n",
    "class TemporalEmbedding(Layer):\n",
    "    def __init__(self, rnn_dims, attention_dim, max_seq_len, cell_type=GRUCell, name='code_ra'):\n",
    "        super().__init__(name=name)\n",
    "        rnn_cells = [cell_type(rnn_dim) for rnn_dim in rnn_dims]\n",
    "        stacked_rnn = StackedRNNCells(rnn_cells)\n",
    "        self.rnn_layers = RNN(stacked_rnn, return_sequences=True, name=name + 'rnn')\n",
    "        self.attention = Attention(attention_dim, name=name + '_attention')\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def call(self, embeddings, lens):\n",
    "        seq_mask = tf.sequence_mask(lens, self.max_seq_len, dtype=embeddings.dtype)\n",
    "        outputs = self.rnn_layers(embeddings) * tf.expand_dims(seq_mask, axis=-1)  # (batch_size, max_seq_len, rnn_dim[-1])\n",
    "        outputs, alphas = self.attention(outputs, seq_mask)  # (batch_size, rnn_dim[-1])\n",
    "        return outputs, alphas\n",
    "\n",
    "\n",
    "def log_no_nan(x):\n",
    "    mask = tf.cast(x == 0, dtype=x.dtype)\n",
    "    x = x + mask\n",
    "    return tf.math.log(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.layers import Layer, Dense\n",
    "\n",
    "\n",
    "class CGLFeatureExtractor(Layer):\n",
    "    def __init__(self, config, hyper_params, name='cgl_feature'):\n",
    "        super().__init__(name=name)\n",
    "        self.config = config\n",
    "        self.hyper_params = hyper_params\n",
    "        self.hierarchical_embedding_layer = HierarchicalEmbedding(\n",
    "            code_levels=config['code_levels'],\n",
    "            code_num_in_levels=config['code_num_in_levels'],\n",
    "            code_dims=hyper_params['code_dims'])\n",
    "        self.patient_embedding_layer = PatientEmbedding(\n",
    "            patient_num=config['patient_num'],\n",
    "            patient_dim=hyper_params['patient_dim'])\n",
    "        self.graph_convolution_layer = GraphConvolution(\n",
    "            patient_dim=hyper_params['patient_dim'],\n",
    "            code_dim=np.sum(hyper_params['code_dims']),\n",
    "            patient_code_adj=config['patient_code_adj'],\n",
    "            code_code_adj=config['code_code_adj'],\n",
    "            patient_hidden_dims=hyper_params['patient_hidden_dims'],\n",
    "            code_hidden_dims=hyper_params['code_hidden_dims'])\n",
    "        self.visit_embedding_layer = VisitEmbedding(\n",
    "            max_seq_len=config['max_visit_seq_len'])\n",
    "        self.visit_temporal_embedding_layer = TemporalEmbedding(\n",
    "            rnn_dims=hyper_params['visit_rnn_dims'],\n",
    "            attention_dim=hyper_params['visit_attention_dim'],\n",
    "            max_seq_len=config['max_visit_seq_len'],\n",
    "            name='visit_temporal')\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        visit_codes = inputs['visit_codes']  # (batch_size, max_seq_len, max_code_num_in_a_visit)\n",
    "        visit_lens = tf.reshape(inputs['visit_lens'], (-1, ))  # (batch_size, )\n",
    "        code_embeddings = self.hierarchical_embedding_layer(None)\n",
    "        patient_embddings = self.patient_embedding_layer(None)\n",
    "\n",
    "        patient_embddings, code_embeddings = self.graph_convolution_layer(\n",
    "            patient_embeddings=patient_embddings, code_embeddings=code_embeddings)\n",
    "        visits_embeddings = self.visit_embedding_layer(\n",
    "            code_embeddings=code_embeddings,\n",
    "            visit_codes=visit_codes,\n",
    "            visit_lens=visit_lens)\n",
    "        visit_output, alpha_visit = self.visit_temporal_embedding_layer(visits_embeddings, visit_lens)\n",
    "        output = visit_output\n",
    "        return output\n",
    "\n",
    "\n",
    "class Classifier(Layer):\n",
    "    def __init__(self, output_dim, activation=None, name='classifier'):\n",
    "        super().__init__(name=name)\n",
    "        self.dense = Dense(output_dim, activation=activation)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dropout(x)\n",
    "        output = self.dense(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CGL(Model):\n",
    "    def __init__(self, config, hyper_params, name='cgl'):\n",
    "        super().__init__(name=name)\n",
    "        self.cgl_feature_extractor = CGLFeatureExtractor(config, hyper_params)\n",
    "        self.classifier = Classifier(config['output_dim'], activation=config['activation'])\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        output = self.cgl_feature_extractor(inputs, training=training)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "def medical_codes_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred), axis=1))\n",
    "\n",
    "def f1(y_true_hot, y_pred, metrics='weighted'):\n",
    "    result = np.zeros_like(y_true_hot)\n",
    "    for i in range(len(result)):\n",
    "        true_number = np.sum(y_true_hot[i] == 1)\n",
    "        result[i][y_pred[i][:true_number]] = 1\n",
    "    return f1_score(y_true=y_true_hot, y_pred=result, average=metrics)\n",
    "\n",
    "\n",
    "def top_k_prec_recall(y_true_hot, y_pred, ks):\n",
    "    a = np.zeros((len(ks), ))\n",
    "    r = np.zeros((len(ks), ))\n",
    "    for pred, true_hot in zip(y_pred, y_true_hot):\n",
    "        true = np.where(true_hot == 1)[0].tolist()\n",
    "        t = set(true)\n",
    "        for i, k in enumerate(ks):\n",
    "            p = set(pred[:k])\n",
    "            it = p.intersection(t)\n",
    "            a[i] += len(it) / k\n",
    "            r[i] += len(it) / len(t)\n",
    "    return a / len(y_true_hot), r / len(y_true_hot)\n",
    "\n",
    "\n",
    "def calculate_occurred(historical, y, preds, ks):\n",
    "    r1 = np.zeros((len(ks), ))\n",
    "    r2 = np.zeros((len(ks),))\n",
    "    n = np.sum(y, axis=-1)\n",
    "    for i, k in enumerate(ks):\n",
    "        n_k = n\n",
    "        pred_k = np.zeros_like(y)\n",
    "        for T in range(len(pred_k)):\n",
    "            pred_k[T][preds[T][:k]] = 1\n",
    "        pred_occurred = np.logical_and(historical, pred_k)\n",
    "        pred_not_occurred = np.logical_and(np.logical_not(historical), pred_k)\n",
    "        pred_occurred_true = np.logical_and(pred_occurred, y)\n",
    "        pred_not_occurred_true = np.logical_and(pred_not_occurred, y)\n",
    "        r1[i] = np.mean(np.sum(pred_occurred_true, axis=-1) / n_k)\n",
    "        r2[i] = np.mean(np.sum(pred_not_occurred_true, axis=-1) / n_k)\n",
    "    return r1, r2\n",
    "\n",
    "\n",
    "class EvaluateCodesCallBack(Callback):\n",
    "    def __init__(self, data_gen, y, historical=None):\n",
    "        super().__init__()\n",
    "        self.data_gen = data_gen\n",
    "        self.y = y\n",
    "        self.historical = historical\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        step_size = len(self.data_gen)\n",
    "        preds = []\n",
    "        for i in range(step_size):\n",
    "            batch_codes_x, batch_visit_lens, batch_note_x, batch_note_lens = self.data_gen[i]\n",
    "            output = self.model(inputs={\n",
    "                'visit_codes': batch_codes_x,\n",
    "                'visit_lens': batch_visit_lens,\n",
    "                'word_ids': batch_note_x,\n",
    "                'word_lens': batch_note_lens\n",
    "            }, training=False)\n",
    "            logits = tf.math.sigmoid(output)\n",
    "            pred = tf.argsort(logits, axis=-1, direction='DESCENDING')\n",
    "            preds.append(pred.numpy())\n",
    "        preds = np.vstack(preds)\n",
    "        f1_score = f1(self.y, preds)\n",
    "        prec, recall = top_k_prec_recall(self.y, preds, ks=[10, 20, 30, 40])\n",
    "        if self.historical is not None:\n",
    "            r1, r2 = calculate_occurred(self.historical, self.y, preds, ks=[10, 20, 30, 40])\n",
    "            print('\\t', 'f1_score:', f1_score, '\\t', 'top_k_recall:', recall, '\\t', 'occurred:', r1, '\\t', 'not occurred:', r2)\n",
    "        else:\n",
    "            print('\\t', 'f1_score:', f1_score, '\\t', 'top_k_recall:', recall)\n",
    "\n",
    "\n",
    "class EvaluateHFCallBack(Callback):\n",
    "    def __init__(self, data_gen, y):\n",
    "        super().__init__()\n",
    "        self.data_gen = data_gen\n",
    "        self.y = y\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        step_size = len(self.data_gen)\n",
    "        preds, outputs = [], []\n",
    "        for i in range(step_size):\n",
    "            batch_codes_x, batch_visit_lens, batch_note_x, batch_note_lens = self.data_gen[i]\n",
    "            output = self.model(inputs={\n",
    "                'visit_codes': batch_codes_x,\n",
    "                'visit_lens': batch_visit_lens,\n",
    "                'word_ids': batch_note_x,\n",
    "                'word_lens': batch_note_lens\n",
    "            }, training=False)\n",
    "            outputs.append(tf.squeeze(output).numpy())\n",
    "            pred = tf.squeeze(tf.cast(output > 0.5, tf.int32))\n",
    "            preds.append(pred.numpy())\n",
    "        outputs = np.concatenate(outputs)\n",
    "        preds = np.concatenate(preds)\n",
    "        auc = roc_auc_score(self.y, outputs)\n",
    "        f1_score_ = f1_score(self.y, preds)\n",
    "        print('\\t', 'auc:', auc, '\\t', 'f1_score:', f1_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data generation and loading\n",
    "\n",
    "### 3.1. Generate data unique to CGL\n",
    "\n",
    "We need additional functions to generate data for the CGL model that was\n",
    "neither used nor generated in Chet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, inputs, shuffle=True, batch_size=32):\n",
    "        assert len(inputs) > 0\n",
    "        self.inputs = inputs\n",
    "        self.idx = np.arange(len(inputs[0]))\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def data_length(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        n = self.data_length()\n",
    "        len_ = n // self.batch_size\n",
    "        return len_ if n % self.batch_size == 0 else len_ + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "        index = self.idx[start:end]\n",
    "        data = []\n",
    "        for x in self.inputs:\n",
    "            data.append(x[start:end])\n",
    "        return data\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idx)\n",
    "\n",
    "    def set_batch_size(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "def generate_patient_code_adjacent(code_x, code_num):\n",
    "    print('generating patient code adjacent matrix ...')\n",
    "    result = np.zeros((len(code_x), code_num), dtype=int)\n",
    "    for i, codes in enumerate(code_x):\n",
    "        adj_codes = codes[codes > 0]\n",
    "        result[i][adj_codes] = 1\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Loading and generating data\n",
    "\n",
    "Load the same data used in training Chet and generate the additional data needed\n",
    "for CGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating patient code adjacent matrix ...\n",
      "generating patient code adjacent matrix ...\n",
      "*** data loaded ***\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_sparse(path):\n",
    "    data = np.load(path)\n",
    "    idx, values = data['idx'], data['values']\n",
    "    mat = np.zeros(data['shape'], dtype=values.dtype)\n",
    "    mat[tuple(idx)] = values\n",
    "    return mat\n",
    "\n",
    "all_data = {}\n",
    "datasets = ['mimic3', 'mimic4']\n",
    "\n",
    "for dataset in datasets:\n",
    "    data_path = os.path.join('data', dataset)\n",
    "    parsed_path = os.path.join(data_path, 'parsed')\n",
    "    encoded_path = os.path.join(data_path, 'encoded')\n",
    "    standard_path = os.path.join(data_path, 'standard')\n",
    "    train_path = os.path.join(standard_path, 'train')\n",
    "    valid_path = os.path.join(standard_path, 'valid')\n",
    "    test_path = os.path.join(standard_path, 'test')\n",
    "\n",
    "    train_codes_x = load_sparse(os.path.join(train_path, 'code_x_cgl.npz'))\n",
    "    train_codes_y = load_sparse(os.path.join(train_path, 'code_y.npz'))\n",
    "    train_hf_y = np.load(os.path.join(train_path, 'hf_y.npz'))['hf_y']\n",
    "    train_visit_lens = np.load(os.path.join(train_path, 'visit_lens.npz'))['lens']\n",
    "    valid_codes_x = load_sparse(os.path.join(valid_path, 'code_x_cgl.npz'))\n",
    "    valid_codes_y = load_sparse(os.path.join(valid_path, 'code_y.npz'))\n",
    "    valid_hf_y = np.load(os.path.join(valid_path, 'hf_y.npz'))['hf_y']\n",
    "    valid_visit_lens = np.load(os.path.join(valid_path, 'visit_lens.npz'))['lens']\n",
    "    test_codes_x = load_sparse(os.path.join(test_path, 'code_x_cgl.npz'))\n",
    "    test_codes_y = load_sparse(os.path.join(test_path, 'code_y.npz'))\n",
    "    test_hf_y = np.load(os.path.join(test_path, 'hf_y.npz'))['hf_y']\n",
    "    test_visit_lens = np.load(os.path.join(test_path, 'visit_lens.npz'))['lens']\n",
    "    code_map = pickle.load(open(os.path.join(encoded_path, 'code_map.pkl'), 'rb'))\n",
    "    code_levels = pickle.load(open(os.path.join(parsed_path, 'code_levels.pkl'), 'rb'))['code_levels']\n",
    "    code_code_adj = load_sparse(os.path.join(standard_path, 'code_adj.npz'))\n",
    "    code_num = len(code_map)\n",
    "    patient_code_adj = generate_patient_code_adjacent(train_codes_x, code_num)\n",
    "\n",
    "    all_data[dataset] = {\n",
    "        'train_codes_x': train_codes_x,\n",
    "        'train_codes_y': train_codes_y,\n",
    "        'train_hf_y': train_hf_y,\n",
    "        'train_visit_lens': train_visit_lens,\n",
    "        'valid_codes_x': valid_codes_x,\n",
    "        'valid_codes_y': valid_codes_y,\n",
    "        'valid_hf_y': valid_hf_y,\n",
    "        'valid_visit_lens': valid_visit_lens,\n",
    "        'test_codes_x': test_codes_x,\n",
    "        'test_codes_y': test_codes_y,\n",
    "        'test_hf_y': test_hf_y,\n",
    "        'test_visit_lens': test_visit_lens,\n",
    "        'code_map': code_map,\n",
    "        'code_num': code_num,\n",
    "        'patient_code_adj': patient_code_adj,\n",
    "        'code_levels': code_levels,\n",
    "        'code_code_adj': code_code_adj\n",
    "    }\n",
    "\n",
    "print(\"*** data loaded ***\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and evaluation\n",
    "\n",
    "We train the CGL model on the same train/valid datasets as the one we used to\n",
    "train Chet, with the same seed and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle as pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# use the same seed and number of epochs as our Chet experiment\n",
    "seed = 6669\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "num_epochs = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Train and evaluate the model for diagnosis prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************Training diagnosis prediction task on mimic3******************\n",
      "\n",
      "Epoch 1/20\n",
      " 17/188 [=>............................] - ETA: 7:03 - loss: 519.3796"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m cgl_model \u001b[39m=\u001b[39m CGL(config, hyper_params)\n\u001b[1;32m     52\u001b[0m cgl_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39mmedical_codes_loss)\n\u001b[0;32m---> 53\u001b[0m cgl_model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     54\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mvisit_codes\u001b[39;49m\u001b[39m'\u001b[39;49m: all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mtrain_codes_x\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     55\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mvisit_lens\u001b[39;49m\u001b[39m'\u001b[39;49m: all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mtrain_visit_lens\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     56\u001b[0m }, y\u001b[39m=\u001b[39;49mall_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mtrain_codes_y\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39;49m), validation_data\u001b[39m=\u001b[39;49m({\n\u001b[1;32m     57\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mvisit_codes\u001b[39;49m\u001b[39m'\u001b[39;49m: all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mvalid_codes_x\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     58\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mvisit_lens\u001b[39;49m\u001b[39m'\u001b[39;49m: all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mvalid_visit_lens\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     59\u001b[0m     }, all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mvalid_codes_y\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39;49m)), epochs\u001b[39m=\u001b[39;49mnum_epochs, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[lr_scheduler, test_callback])\n\u001b[1;32m     60\u001b[0m cgl_model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def historical_hot(code_x, code_num):\n",
    "    result = np.zeros((len(code_x), code_num), dtype=int)\n",
    "    for i, x in enumerate(code_x):\n",
    "        for code in x:\n",
    "            result[i][code - 1] = 1\n",
    "    return result\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"\\n******************Training diagnosis prediction task on {}******************\\n\".format(dataset))\n",
    "    config = {\n",
    "        'patient_code_adj': tf.constant(all_data[dataset]['patient_code_adj'], dtype=tf.float32),\n",
    "        'code_code_adj': tf.constant(all_data[dataset]['code_code_adj'], dtype=tf.float32),\n",
    "        'code_levels': tf.constant(all_data[dataset]['code_levels'], dtype=tf.int32),\n",
    "        'code_num_in_levels': np.max(all_data[dataset]['code_levels'], axis=0) + 1,\n",
    "        'patient_num': all_data[dataset]['train_codes_x'].shape[0],\n",
    "        'max_visit_seq_len': all_data[dataset]['train_codes_x'].shape[1],\n",
    "        'output_dim': len(all_data[dataset]['code_map']),\n",
    "        'lambda': 0.3,\n",
    "        'activation': None\n",
    "    }\n",
    "\n",
    "    test_historical = historical_hot(all_data[dataset]['test_codes_x'], config['output_dim'])\n",
    "\n",
    "    visit_rnn_dims = [200]\n",
    "    hyper_params = {\n",
    "        'code_dims': [32, 32, 32, 32],\n",
    "        'patient_dim': 16,\n",
    "        'word_dim': 16,\n",
    "        'patient_hidden_dims': [32],\n",
    "        'code_hidden_dims': [64, 128],\n",
    "        'visit_rnn_dims': visit_rnn_dims,\n",
    "        'visit_attention_dim': 32,\n",
    "    }\n",
    "\n",
    "    test_codes_gen = DataGenerator([all_data[dataset]['test_codes_x'], all_data[dataset]['test_visit_lens']], shuffle=False)\n",
    "\n",
    "    def lr_schedule_fn(epoch, lr):\n",
    "        if epoch < 20:\n",
    "            lr = 0.01\n",
    "        elif epoch < 100:\n",
    "            lr = 0.001\n",
    "        elif epoch < 200:\n",
    "            lr = 0.0001\n",
    "        else:\n",
    "            lr = 0.00001\n",
    "        return lr\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule_fn)\n",
    "    test_callback = EvaluateCodesCallBack(test_codes_gen, all_data[dataset]['test_codes_y'], historical=test_historical)\n",
    "\n",
    "    cgl_model = CGL(config, hyper_params)\n",
    "    cgl_model.compile(optimizer='adam', loss=medical_codes_loss)\n",
    "    cgl_model.fit(x={\n",
    "        'visit_codes': all_data[dataset]['train_codes_x'],\n",
    "        'visit_lens': all_data[dataset]['train_visit_lens'],\n",
    "    }, y=all_data[dataset]['train_codes_y'].astype(float), validation_data=({\n",
    "        'visit_codes': all_data[dataset]['valid_codes_x'],\n",
    "        'visit_lens': all_data[dataset]['valid_visit_lens'],\n",
    "        }, all_data[dataset]['valid_codes_y'].astype(float)), epochs=num_epochs, batch_size=32, callbacks=[lr_scheduler, test_callback])\n",
    "    cgl_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Train and evaluate the model for heart failure prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************Training heart failure prediction task on mimic3******************\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 6000, 6000\n  y sizes: 1000\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m cgl_model \u001b[39m=\u001b[39m CGL(config, hyper_params)\n\u001b[1;32m     45\u001b[0m cgl_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[tf\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mAUC()])\n\u001b[0;32m---> 46\u001b[0m cgl_model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     47\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mvisit_codes\u001b[39;49m\u001b[39m'\u001b[39;49m: all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mtrain_codes_x\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     48\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mvisit_lens\u001b[39;49m\u001b[39m'\u001b[39;49m: all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mtrain_visit_lens\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     49\u001b[0m }, y\u001b[39m=\u001b[39;49m all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mtest_hf_y\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39;49m), validation_data\u001b[39m=\u001b[39;49m({\n\u001b[1;32m     50\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mvisit_codes\u001b[39;49m\u001b[39m'\u001b[39;49m: all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mvalid_codes_x\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     51\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mvisit_lens\u001b[39;49m\u001b[39m'\u001b[39;49m: all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mvalid_visit_lens\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     52\u001b[0m }, all_data[dataset][\u001b[39m'\u001b[39;49m\u001b[39mvalid_hf_y\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39;49m)), epochs\u001b[39m=\u001b[39;49mnum_epochs, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[lr_scheduler, test_callback])\n\u001b[1;32m     53\u001b[0m cgl_model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/data_adapter.py:1852\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1846\u001b[0m         label,\n\u001b[1;32m   1847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m   1848\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[1;32m   1849\u001b[0m         ),\n\u001b[1;32m   1850\u001b[0m     )\n\u001b[1;32m   1851\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1852\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 6000, 6000\n  y sizes: 1000\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(\"\\n******************Training heart failure prediction task on {}******************\\n\".format(dataset))\n",
    "\n",
    "    config = {\n",
    "            'patient_code_adj': tf.constant(all_data[dataset]['patient_code_adj'], dtype=tf.float32),\n",
    "            'code_code_adj': tf.constant(all_data[dataset]['code_code_adj'], dtype=tf.float32),\n",
    "            'code_levels': tf.constant(all_data[dataset]['code_levels'], dtype=tf.int32),\n",
    "            'code_num_in_levels': np.max(all_data[dataset]['code_levels'], axis=0) + 1,\n",
    "            'patient_num': all_data[dataset]['train_codes_x'].shape[0],\n",
    "            'max_visit_seq_len': all_data[dataset]['train_codes_x'].shape[1],\n",
    "            'output_dim': 1,\n",
    "            'lambda': 0.1,\n",
    "            'activation': 'sigmoid'\n",
    "        }\n",
    "\n",
    "    visit_rnn_dims = [200]\n",
    "    hyper_params = {\n",
    "        'code_dims': [32, 32, 32, 32],\n",
    "        'patient_dim': 16,\n",
    "        'word_dim': 16,\n",
    "        'patient_hidden_dims': [32],\n",
    "        'code_hidden_dims': [64, 128],\n",
    "        'visit_rnn_dims': visit_rnn_dims,\n",
    "        'visit_attention_dim': 32,\n",
    "        'note_attention_dim': visit_rnn_dims[-1]\n",
    "    }\n",
    "\n",
    "    test_codes_gen = DataGenerator([ all_data[dataset]['test_codes_x'], all_data[dataset]['test_visit_lens']], shuffle=False)\n",
    "\n",
    "    def lr_schedule_fn(epoch, lr):\n",
    "        if epoch < 8:\n",
    "            lr = 0.1\n",
    "        elif epoch < 20:\n",
    "            lr = 0.01\n",
    "        elif epoch < 50:\n",
    "            lr = 0.001\n",
    "        else:\n",
    "            lr = 0.0001\n",
    "        return lr\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule_fn)\n",
    "    test_callback = EvaluateHFCallBack(test_codes_gen, all_data[dataset]['test_hf_y'])\n",
    "\n",
    "    cgl_model = CGL(config, hyper_params)\n",
    "    cgl_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[tf.metrics.AUC()])\n",
    "    cgl_model.fit(x={\n",
    "        'visit_codes': all_data[dataset]['train_codes_x'],\n",
    "        'visit_lens': all_data[dataset]['train_visit_lens'],\n",
    "    }, y= all_data[dataset]['train_hf_y'].astype(float), validation_data=({\n",
    "        'visit_codes': all_data[dataset]['valid_codes_x'],\n",
    "        'visit_lens': all_data[dataset]['valid_visit_lens'],\n",
    "    }, all_data[dataset]['valid_hf_y'].astype(float)), epochs=num_epochs, batch_size=32, callbacks=[lr_scheduler, test_callback])\n",
    "    cgl_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
