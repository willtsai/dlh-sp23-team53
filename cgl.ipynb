{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of Chet baseline model for comparison: CGL model (Lu et al., 2021)\n",
    "\n",
    "**UIUC, CS598 DL4H, Spring 2023**\n",
    "\n",
    "**Authors:** Shiyu (Sherry) Li and Wei-Lun (Will) Tsai; {shiyuli2, wltsai2}@illinois.edu\n",
    "\n",
    "**Original paper:** Chang Lu, Chandan K. Reddy, Prithwish Chakraborty, Samantha\n",
    "Kleinberg, and Yue Ning. 2021. [Collaborative Graph Learning with Auxiliary Text\n",
    "for Temporal Event Prediction in Healthcare](https://arxiv.org/pdf/2105.07542.pdf) In \n",
    "*Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21*,\n",
    "pages 3529â€“3535. Interna- tional Joint Conferences on Artificial Intelligence\n",
    "Organization. Main Track.\n",
    "\n",
    "**Original codebase:** [github.com/LuChang-CS/CGL](https://github.com/LuChang-CS/CGL)\n",
    "\n",
    "We re-used the CLG model code from the original repository, but with the following modifications:\n",
    "1. Combined the code from the original classes `layers.py`, `model.py`, `loss.py`, `metrics.py`, `utils.py`, `train_codes.py`, and `train_codes.py` into a single notebook `cgl.ipynb` for ease of sequential execution.\n",
    "1. Modified the model definition and training code to remove the use of clinical notes in the CGL baseline to ensure a fair baseline comparison, as specified in the Chet paper (Lu et al., 2022).\n",
    "1. Modified the training code to use the same preprocessed data and train/valid/test splits as the Chet paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model architecture\n",
    "\n",
    "### 1.1. Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Dense, BatchNormalization, GRUCell\n",
    "from keras.layers import Concatenate, Activation, RNN, StackedRNNCells\n",
    "from keras.initializers import GlorotUniform\n",
    "\n",
    "\n",
    "class HierarchicalEmbedding(Layer):\n",
    "    def __init__(self, code_levels, code_num_in_levels, code_dims, name='hierarchical_embedding'):\n",
    "        super().__init__(name=name)\n",
    "        self.level_num = len(code_num_in_levels)\n",
    "        self.code_levels = code_levels  # (leaf code num * level_num)\n",
    "        self.level_embeddings = [self.add_weight(name='hier_emb_level_%d' % level,\n",
    "                                                 shape=(code_num, code_dim),\n",
    "                                                 initializer=GlorotUniform(),\n",
    "                                                 trainable=True)\n",
    "                                 for level, (code_num, code_dim) in enumerate(zip(code_num_in_levels, code_dims))]\n",
    "\n",
    "    def call(self, inputs=None):\n",
    "        \"\"\"\n",
    "            return: (code_num, embedding_size)\n",
    "        \"\"\"\n",
    "        embeddings = [tf.nn.embedding_lookup(self.level_embeddings[level], self.code_levels[:, level])\n",
    "                      for level in range(self.level_num)]\n",
    "        embeddings = Concatenate()(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class PatientEmbedding(Layer):\n",
    "    def __init__(self, patient_num, patient_dim, name='patient_embedding'):\n",
    "        super().__init__(name=name)\n",
    "        self.patient_embeddings = self.add_weight(name='p_emb',\n",
    "                                                  shape=(patient_num, patient_dim),\n",
    "                                                  initializer=GlorotUniform(),\n",
    "                                                  trainable=True)\n",
    "\n",
    "    def call(self, inputs=None):\n",
    "        return self.patient_embeddings\n",
    "\n",
    "\n",
    "class GraphConvBlock(Layer):\n",
    "    def __init__(self, node_type, dim, adj, name='graph_conv_block'):\n",
    "        super().__init__(name=name)\n",
    "        self.node_type = node_type\n",
    "        self.adj = adj\n",
    "        self.dense = Dense(dim, activation=None, name=name + '_dense')\n",
    "        self.activation = Activation('relu', name=name + '_activation')\n",
    "        self.bn = BatchNormalization(name=name + 'bn')\n",
    "\n",
    "    def call(self, embedding, embedding_neighbor, weight_decay=None):\n",
    "        output = embedding + tf.matmul(self.adj, embedding_neighbor)\n",
    "        if self.node_type == 'code':\n",
    "            assert weight_decay is not None\n",
    "            output += tf.matmul(weight_decay, embedding)\n",
    "        output = self.dense(output)\n",
    "        output = self.bn(output)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def norm_no_nan(x):\n",
    "    return tf.math.divide_no_nan(x, tf.reduce_sum(x, axis=-1, keepdims=True))\n",
    "\n",
    "\n",
    "class GraphConvolution(Layer):\n",
    "    def __init__(self, patient_dim, code_dim,\n",
    "                 patient_code_adj, code_code_adj,\n",
    "                 patient_hidden_dims, code_hidden_dims, name='graph_convolution'):\n",
    "        super().__init__(name=name)\n",
    "        self.patient_code_adj = norm_no_nan(patient_code_adj)  # (patient_num, code_num)\n",
    "        self.code_patient_adj = norm_no_nan(tf.transpose(patient_code_adj))  # (code_num, patient_num)\n",
    "        self.code_code_adj = code_code_adj  # (code_num, code_num)\n",
    "\n",
    "        self.patient_blocks = [\n",
    "            GraphConvBlock('patient', dim, self.patient_code_adj, name='patient_graph_block_%d' % layer)\n",
    "            for layer, dim in enumerate(patient_hidden_dims)]\n",
    "        self.code_blocks = [GraphConvBlock('code', dim, self.code_patient_adj, name='code_graph_block_%d' % layer)\n",
    "                            for layer, dim in enumerate(code_hidden_dims)]\n",
    "\n",
    "        c2p_dims = ([patient_dim] + patient_hidden_dims)[:-1]\n",
    "        p2c_dims = ([code_dim] + code_hidden_dims)[:-1]\n",
    "        self.c2p_denses = [Dense(dim, activation=None, name='code_to_patient_dense_%d' % layer)\n",
    "                           for layer, dim in enumerate(c2p_dims)]\n",
    "        self.p2c_denses = [Dense(dim, activation=None, name='patient_to_code_dense_%d' % layer)\n",
    "                           for layer, dim in enumerate(p2c_dims)]\n",
    "\n",
    "        code_num = code_code_adj.shape[0]\n",
    "        self.miu = self.add_weight(name='miu', shape=(code_num,), trainable=True)\n",
    "        self.theta = self.add_weight(name='theta', shape=(code_num,), trainable=True)\n",
    "\n",
    "    def call(self, patient_embeddings, code_embeddings):\n",
    "        weight_decay = tf.nn.sigmoid(self.miu * self.code_code_adj + self.theta)\n",
    "        weight_decay = norm_no_nan(weight_decay)\n",
    "        # weight_decay = None\n",
    "        for c2p_dense, p2c_dense, patient_block, code_block in zip(self.c2p_denses, self.p2c_denses,\n",
    "                                                                   self.patient_blocks, self.code_blocks):\n",
    "            code_embeddings_p = c2p_dense(code_embeddings)\n",
    "            patient_embeddings_new = patient_block(patient_embeddings, code_embeddings_p)\n",
    "            patient_embeddings_c = p2c_dense(patient_embeddings)\n",
    "            code_embeddings = code_block(code_embeddings, patient_embeddings_c, weight_decay)\n",
    "            patient_embeddings = patient_embeddings_new\n",
    "        patient_embeddings_c = self.p2c_denses[-1](patient_embeddings)\n",
    "        code_embeddings = self.code_blocks[-1](code_embeddings, patient_embeddings_c, weight_decay)\n",
    "        return patient_embeddings, code_embeddings\n",
    "\n",
    "\n",
    "class VisitEmbedding(Layer):\n",
    "    def __init__(self, max_seq_len, name='visit_embedding'):\n",
    "        super().__init__(name=name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def call(self, code_embeddings, visit_codes, visit_lens):\n",
    "        \"\"\"\n",
    "            visit_codes: (batch_size, max_seq_len, max_code_num_in_a_visit)\n",
    "        \"\"\"\n",
    "        visit_codes_embedding = tf.nn.embedding_lookup(code_embeddings, visit_codes)  # (batch_size, max_seq_len, max_code_num_in_a_visit, code_dim)\n",
    "        visit_codes_mask = tf.expand_dims(visit_codes > 0, axis=-1)\n",
    "        visit_codes_mask = tf.cast(visit_codes_mask, visit_codes_embedding.dtype)\n",
    "        visit_codes_embedding *= visit_codes_mask  # (batch_size, max_seq_len, max_code_num_in_a_visit, code_dim)\n",
    "        visit_codes_num = tf.expand_dims(tf.reduce_sum(tf.cast(visit_codes > 0, visit_codes_embedding.dtype), axis=-1), axis=-1)\n",
    "        visits_embeddings = tf.math.divide_no_nan(tf.reduce_sum(visit_codes_embedding, axis=-2), visit_codes_num)  # (batch_size, max_seq_len, code_dim)\n",
    "        visit_mask = tf.expand_dims(tf.sequence_mask(visit_lens, self.max_seq_len, dtype=visits_embeddings.dtype), axis=-1)  # (batch_size, max_seq_len, 1)\n",
    "        visits_embeddings *= visit_mask  # (batch_size, max_seq_len, code_dim)\n",
    "        return visits_embeddings\n",
    "\n",
    "\n",
    "def masked_softmax(inputs, mask):\n",
    "    inputs = inputs - tf.reduce_max(inputs, keepdims=True, axis=-1)\n",
    "    exp = tf.exp(inputs) * mask\n",
    "    result = tf.math.divide_no_nan(exp, tf.reduce_sum(exp, keepdims=True, axis=-1))\n",
    "    return result\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, attention_dim, name='attention'):\n",
    "        super().__init__(name=name)\n",
    "        self.attention_dim = attention_dim\n",
    "        self.u_omega = self.add_weight(name=name + '_u', shape=(attention_dim,), initializer=GlorotUniform())\n",
    "        self.w_omega = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        hidden_size = input_shape[-1]\n",
    "        self.w_omega = self.add_weight(name=self.name + '_w', shape=(hidden_size, self.attention_dim), initializer=GlorotUniform())\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \"\"\"\n",
    "            x: (batch_size, max_seq_len, rnn_dim[-1] / hidden_size)\n",
    "        \"\"\"\n",
    "        t = tf.matmul(x, self.w_omega)\n",
    "        vu = tf.tensordot(t, self.u_omega, axes=1)  # (batch_size, max_seq_len)\n",
    "        if mask is not None:\n",
    "            vu *= mask\n",
    "            alphas = masked_softmax(vu, mask)\n",
    "        else:\n",
    "            alphas = tf.nn.softmax(vu)  # (batch_size, max_seq_len)\n",
    "        output = tf.reduce_sum(x * tf.expand_dims(alphas, -1), axis=-2)  # (batch_size, rnn_dim[-1] / hidden_size)\n",
    "        return output, alphas\n",
    "\n",
    "\n",
    "class TemporalEmbedding(Layer):\n",
    "    def __init__(self, rnn_dims, attention_dim, max_seq_len, cell_type=GRUCell, name='code_ra'):\n",
    "        super().__init__(name=name)\n",
    "        rnn_cells = [cell_type(rnn_dim) for rnn_dim in rnn_dims]\n",
    "        stacked_rnn = StackedRNNCells(rnn_cells)\n",
    "        self.rnn_layers = RNN(stacked_rnn, return_sequences=True, name=name + 'rnn')\n",
    "        self.attention = Attention(attention_dim, name=name + '_attention')\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def call(self, embeddings, lens):\n",
    "        seq_mask = tf.sequence_mask(lens, self.max_seq_len, dtype=embeddings.dtype)\n",
    "        outputs = self.rnn_layers(embeddings) * tf.expand_dims(seq_mask, axis=-1)  # (batch_size, max_seq_len, rnn_dim[-1])\n",
    "        outputs, alphas = self.attention(outputs, seq_mask)  # (batch_size, rnn_dim[-1])\n",
    "        return outputs, alphas\n",
    "\n",
    "\n",
    "def log_no_nan(x):\n",
    "    mask = tf.cast(x == 0, dtype=x.dtype)\n",
    "    x = x + mask\n",
    "    return tf.math.log(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.layers import Layer, Dense\n",
    "\n",
    "\n",
    "class CGLFeatureExtractor(Layer):\n",
    "    def __init__(self, config, hyper_params, name='cgl_feature'):\n",
    "        super().__init__(name=name)\n",
    "        self.config = config\n",
    "        self.hyper_params = hyper_params\n",
    "        self.hierarchical_embedding_layer = HierarchicalEmbedding(\n",
    "            code_levels=config['code_levels'],\n",
    "            code_num_in_levels=config['code_num_in_levels'],\n",
    "            code_dims=hyper_params['code_dims'])\n",
    "        self.patient_embedding_layer = PatientEmbedding(\n",
    "            patient_num=config['patient_num'],\n",
    "            patient_dim=hyper_params['patient_dim'])\n",
    "        self.graph_convolution_layer = GraphConvolution(\n",
    "            patient_dim=hyper_params['patient_dim'],\n",
    "            code_dim=np.sum(hyper_params['code_dims']),\n",
    "            patient_code_adj=config['patient_code_adj'],\n",
    "            code_code_adj=config['code_code_adj'],\n",
    "            patient_hidden_dims=hyper_params['patient_hidden_dims'],\n",
    "            code_hidden_dims=hyper_params['code_hidden_dims'])\n",
    "        self.visit_embedding_layer = VisitEmbedding(\n",
    "            max_seq_len=config['max_visit_seq_len'])\n",
    "        self.visit_temporal_embedding_layer = TemporalEmbedding(\n",
    "            rnn_dims=hyper_params['visit_rnn_dims'],\n",
    "            attention_dim=hyper_params['visit_attention_dim'],\n",
    "            max_seq_len=config['max_visit_seq_len'],\n",
    "            name='visit_temporal')\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        visit_codes = inputs['visit_codes']  # (batch_size, max_seq_len, max_code_num_in_a_visit)\n",
    "        visit_lens = tf.reshape(inputs['visit_lens'], (-1, ))  # (batch_size, )\n",
    "        code_embeddings = self.hierarchical_embedding_layer(None)\n",
    "        patient_embddings = self.patient_embedding_layer(None)\n",
    "\n",
    "        patient_embddings, code_embeddings = self.graph_convolution_layer(\n",
    "            patient_embeddings=patient_embddings, code_embeddings=code_embeddings)\n",
    "        visits_embeddings = self.visit_embedding_layer(\n",
    "            code_embeddings=code_embeddings,\n",
    "            visit_codes=visit_codes,\n",
    "            visit_lens=visit_lens)\n",
    "        visit_output, alpha_visit = self.visit_temporal_embedding_layer(visits_embeddings, visit_lens)\n",
    "        output = visit_output\n",
    "        return output\n",
    "\n",
    "\n",
    "class Classifier(Layer):\n",
    "    def __init__(self, output_dim, activation=None, name='classifier'):\n",
    "        super().__init__(name=name)\n",
    "        self.dense = Dense(output_dim, activation=activation)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dropout(x)\n",
    "        output = self.dense(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CGL(Model):\n",
    "    def __init__(self, config, hyper_params, name='cgl'):\n",
    "        super().__init__(name=name)\n",
    "        self.cgl_feature_extractor = CGLFeatureExtractor(config, hyper_params)\n",
    "        self.classifier = Classifier(config['output_dim'], activation=config['activation'])\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        output = self.cgl_feature_extractor(inputs, training=training)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "def medical_codes_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred), axis=1))\n",
    "\n",
    "def f1(y_true_hot, y_pred, metrics='weighted'):\n",
    "    result = np.zeros_like(y_true_hot)\n",
    "    for i in range(len(result)):\n",
    "        true_number = np.sum(y_true_hot[i] == 1)\n",
    "        result[i][y_pred[i][:true_number]] = 1\n",
    "    return f1_score(y_true=y_true_hot, y_pred=result, average=metrics)\n",
    "\n",
    "\n",
    "def top_k_prec_recall(y_true_hot, y_pred, ks):\n",
    "    a = np.zeros((len(ks), ))\n",
    "    r = np.zeros((len(ks), ))\n",
    "    for pred, true_hot in zip(y_pred, y_true_hot):\n",
    "        true = np.where(true_hot == 1)[0].tolist()\n",
    "        t = set(true)\n",
    "        for i, k in enumerate(ks):\n",
    "            p = set(pred[:k])\n",
    "            it = p.intersection(t)\n",
    "            a[i] += len(it) / k\n",
    "            r[i] += len(it) / len(t)\n",
    "    return a / len(y_true_hot), r / len(y_true_hot)\n",
    "\n",
    "\n",
    "def calculate_occurred(historical, y, preds, ks):\n",
    "    r1 = np.zeros((len(ks), ))\n",
    "    r2 = np.zeros((len(ks),))\n",
    "    n = np.sum(y, axis=-1)\n",
    "    for i, k in enumerate(ks):\n",
    "        n_k = n\n",
    "        pred_k = np.zeros_like(y)\n",
    "        for T in range(len(pred_k)):\n",
    "            pred_k[T][preds[T][:k]] = 1\n",
    "        pred_occurred = np.logical_and(historical, pred_k)\n",
    "        pred_not_occurred = np.logical_and(np.logical_not(historical), pred_k)\n",
    "        pred_occurred_true = np.logical_and(pred_occurred, y)\n",
    "        pred_not_occurred_true = np.logical_and(pred_not_occurred, y)\n",
    "        r1[i] = np.mean(np.sum(pred_occurred_true, axis=-1) / n_k)\n",
    "        r2[i] = np.mean(np.sum(pred_not_occurred_true, axis=-1) / n_k)\n",
    "    return r1, r2\n",
    "\n",
    "\n",
    "class EvaluateCodesCallBack(Callback):\n",
    "    def __init__(self, data_gen, y, historical=None):\n",
    "        super().__init__()\n",
    "        self.data_gen = data_gen\n",
    "        self.y = y\n",
    "        self.historical = historical\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        step_size = len(self.data_gen)\n",
    "        preds = []\n",
    "        for i in range(step_size):\n",
    "            batch_codes_x, batch_visit_lens, batch_note_x, batch_note_lens = self.data_gen[i]\n",
    "            output = self.model(inputs={\n",
    "                'visit_codes': batch_codes_x,\n",
    "                'visit_lens': batch_visit_lens,\n",
    "                'word_ids': batch_note_x,\n",
    "                'word_lens': batch_note_lens\n",
    "            }, training=False)\n",
    "            logits = tf.math.sigmoid(output)\n",
    "            pred = tf.argsort(logits, axis=-1, direction='DESCENDING')\n",
    "            preds.append(pred.numpy())\n",
    "        preds = np.vstack(preds)\n",
    "        f1_score = f1(self.y, preds)\n",
    "        prec, recall = top_k_prec_recall(self.y, preds, ks=[10, 20, 30, 40])\n",
    "        if self.historical is not None:\n",
    "            r1, r2 = calculate_occurred(self.historical, self.y, preds, ks=[10, 20, 30, 40])\n",
    "            print('\\t', 'f1_score:', f1_score, '\\t', 'top_k_recall:', recall, '\\t', 'occurred:', r1, '\\t', 'not occurred:', r2)\n",
    "        else:\n",
    "            print('\\t', 'f1_score:', f1_score, '\\t', 'top_k_recall:', recall)\n",
    "\n",
    "\n",
    "class EvaluateHFCallBack(Callback):\n",
    "    def __init__(self, data_gen, y):\n",
    "        super().__init__()\n",
    "        self.data_gen = data_gen\n",
    "        self.y = y\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        step_size = len(self.data_gen)\n",
    "        preds, outputs = [], []\n",
    "        for i in range(step_size):\n",
    "            batch_codes_x, batch_visit_lens, batch_note_x, batch_note_lens = self.data_gen[i]\n",
    "            output = self.model(inputs={\n",
    "                'visit_codes': batch_codes_x,\n",
    "                'visit_lens': batch_visit_lens,\n",
    "                'word_ids': batch_note_x,\n",
    "                'word_lens': batch_note_lens\n",
    "            }, training=False)\n",
    "            outputs.append(tf.squeeze(output).numpy())\n",
    "            pred = tf.squeeze(tf.cast(output > 0.5, tf.int32))\n",
    "            preds.append(pred.numpy())\n",
    "        outputs = np.concatenate(outputs)\n",
    "        preds = np.concatenate(preds)\n",
    "        auc = roc_auc_score(self.y, outputs)\n",
    "        f1_score_ = f1_score(self.y, preds)\n",
    "        print('\\t', 'auc:', auc, '\\t', 'f1_score:', f1_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data generation and loading\n",
    "\n",
    "### 3.1. Generate data unique to CGL\n",
    "\n",
    "We need additional functions to generate data for the CGL model that was\n",
    "neither used nor generated in Chet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, inputs, shuffle=True, batch_size=32):\n",
    "        assert len(inputs) > 0\n",
    "        self.inputs = inputs\n",
    "        self.idx = np.arange(len(inputs[0]))\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def data_length(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        n = self.data_length()\n",
    "        len_ = n // self.batch_size\n",
    "        return len_ if n % self.batch_size == 0 else len_ + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "        index = self.idx[start:end]\n",
    "        data = []\n",
    "        for x in self.inputs:\n",
    "            data.append(x[start:end])\n",
    "        return data\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idx)\n",
    "\n",
    "    def set_batch_size(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "def generate_patient_code_adjacent(code_x, code_num):\n",
    "    print('generating patient code adjacent matrix ...')\n",
    "    result = np.zeros((len(code_x), code_num), dtype=int)\n",
    "    for i, codes in enumerate(code_x):\n",
    "        adj_codes = codes[codes > 0]\n",
    "        result[i][adj_codes] = 1\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Loading and generating data\n",
    "\n",
    "Load the same data used in training Chet and generate the additional data needed\n",
    "for CGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_sparse(path):\n",
    "    data = np.load(path)\n",
    "    idx, values = data['idx'], data['values']\n",
    "    mat = np.zeros(data['shape'], dtype=values.dtype)\n",
    "    mat[tuple(idx)] = values\n",
    "    return mat\n",
    "\n",
    "all_data = {}\n",
    "datasets = ['mimic3', 'mimic4']\n",
    "\n",
    "for dataset in datasets:\n",
    "    data_path = os.path.join('data', dataset)\n",
    "    parsed_path = os.path.join(data_path, 'parsed')\n",
    "    encoded_path = os.path.join(data_path, 'encoded')\n",
    "    standard_path = os.path.join(data_path, 'standard')\n",
    "    train_path = os.path.join(standard_path, 'train')\n",
    "    valid_path = os.path.join(standard_path, 'valid')\n",
    "    test_path = os.path.join(standard_path, 'test')\n",
    "\n",
    "    train_codes_x = load_sparse(os.path.join(train_path, 'code_x_cgl.npz'))\n",
    "    train_codes_y = load_sparse(os.path.join(train_path, 'code_y.npz'))\n",
    "    train_hf_y = np.load(os.path.join(train_path, 'hf_y.npz'))['hf_y']\n",
    "    train_visit_lens = np.load(os.path.join(train_path, 'visit_lens.npz'))['lens']\n",
    "    valid_codes_x = load_sparse(os.path.join(valid_path, 'code_x_cgl.npz'))\n",
    "    valid_codes_y = load_sparse(os.path.join(valid_path, 'code_y.npz'))\n",
    "    valid_hf_y = np.load(os.path.join(valid_path, 'hf_y.npz'))['hf_y']\n",
    "    valid_visit_lens = np.load(os.path.join(valid_path, 'visit_lens.npz'))['lens']\n",
    "    test_codes_x = load_sparse(os.path.join(test_path, 'code_x_cgl.npz'))\n",
    "    test_codes_y = load_sparse(os.path.join(test_path, 'code_y.npz'))\n",
    "    test_hf_y = np.load(os.path.join(test_path, 'hf_y.npz'))['hf_y']\n",
    "    test_visit_lens = np.load(os.path.join(test_path, 'visit_lens.npz'))['lens']\n",
    "    code_map = pickle.load(open(os.path.join(encoded_path, 'code_map.pkl'), 'rb'))\n",
    "    code_levels = pickle.load(open(os.path.join(parsed_path, 'code_levels.pkl'), 'rb'))['code_levels']\n",
    "    code_code_adj = load_sparse(os.path.join(standard_path, 'code_adj.npz'))\n",
    "    code_num = len(code_map)\n",
    "    patient_code_adj = generate_patient_code_adjacent(train_codes_x, code_num)\n",
    "\n",
    "    all_data[dataset] = {\n",
    "        'train_codes_x': train_codes_x,\n",
    "        'train_codes_y': train_codes_y,\n",
    "        'train_hf_y': train_hf_y,\n",
    "        'train_visit_lens': train_visit_lens,\n",
    "        'valid_codes_x': valid_codes_x,\n",
    "        'valid_codes_y': valid_codes_y,\n",
    "        'valid_hf_y': valid_hf_y,\n",
    "        'valid_visit_lens': valid_visit_lens,\n",
    "        'test_codes_x': test_codes_x,\n",
    "        'test_codes_y': test_codes_y,\n",
    "        'test_hf_y': test_hf_y,\n",
    "        'test_visit_lens': test_visit_lens,\n",
    "        'code_map': code_map,\n",
    "        'code_num': code_num,\n",
    "        'patient_code_adj': patient_code_adj,\n",
    "        'code_levels': code_levels,\n",
    "        'code_code_adj': code_code_adj\n",
    "    }\n",
    "\n",
    "print(\"*** data loaded ***\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and evaluation\n",
    "\n",
    "We train the CGL model on the same train/valid datasets as the one we used to\n",
    "train Chet, with the same seed and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle as pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# use the same seed and number of epochs as our Chet experiment\n",
    "seed = 6669\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "num_epochs = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Train and evaluate the model for diagnosis prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_hot(code_x, code_num):\n",
    "    result = np.zeros((len(code_x), code_num), dtype=int)\n",
    "    for i, x in enumerate(code_x):\n",
    "        for code in x:\n",
    "            result[i][code - 1] = 1\n",
    "    return result\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"\\n******************Training diagnosis prediction task on {}******************\\n\".format(dataset))\n",
    "    config = {\n",
    "        'patient_code_adj': tf.constant(all_data[dataset]['patient_code_adj'], dtype=tf.float32),\n",
    "        'code_code_adj': tf.constant(all_data[dataset]['code_code_adj'], dtype=tf.float32),\n",
    "        'code_levels': tf.constant(all_data[dataset]['code_levels'], dtype=tf.int32),\n",
    "        'code_num_in_levels': np.max(all_data[dataset]['code_levels'], axis=0) + 1,\n",
    "        'patient_num': all_data[dataset]['train_codes_x'].shape[0],\n",
    "        'max_visit_seq_len': all_data[dataset]['train_codes_x'].shape[1],\n",
    "        'output_dim': len(all_data[dataset]['code_map']),\n",
    "        'lambda': 0.3,\n",
    "        'activation': None\n",
    "    }\n",
    "\n",
    "    test_historical = historical_hot(all_data[dataset]['test_codes_x'], config['output_dim'])\n",
    "\n",
    "    visit_rnn_dims = [200]\n",
    "    hyper_params = {\n",
    "        'code_dims': [32, 32, 32, 32],\n",
    "        'patient_dim': 16,\n",
    "        'word_dim': 16,\n",
    "        'patient_hidden_dims': [32],\n",
    "        'code_hidden_dims': [64, 128],\n",
    "        'visit_rnn_dims': visit_rnn_dims,\n",
    "        'visit_attention_dim': 32,\n",
    "    }\n",
    "\n",
    "    test_codes_gen = DataGenerator([all_data[dataset]['test_codes_x'], all_data[dataset]['test_visit_lens']], shuffle=False)\n",
    "\n",
    "    def lr_schedule_fn(epoch, lr):\n",
    "        if epoch < 20:\n",
    "            lr = 0.01\n",
    "        elif epoch < 100:\n",
    "            lr = 0.001\n",
    "        elif epoch < 200:\n",
    "            lr = 0.0001\n",
    "        else:\n",
    "            lr = 0.00001\n",
    "        return lr\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule_fn)\n",
    "    test_callback = EvaluateCodesCallBack(test_codes_gen, all_data[dataset]['test_codes_y'], historical=test_historical)\n",
    "\n",
    "    cgl_model = CGL(config, hyper_params)\n",
    "    cgl_model.compile(optimizer='adam', loss=medical_codes_loss)\n",
    "    cgl_model.fit(x={\n",
    "        'visit_codes': all_data[dataset]['train_codes_x'],\n",
    "        'visit_lens': all_data[dataset]['train_visit_lens'],\n",
    "    }, y=all_data[dataset]['train_codes_y'].astype(float), validation_data=({\n",
    "        'visit_codes': all_data[dataset]['valid_codes_x'],\n",
    "        'visit_lens': all_data[dataset]['valid_visit_lens'],\n",
    "        }, all_data[dataset]['valid_codes_y'].astype(float)), epochs=num_epochs, batch_size=32, callbacks=[lr_scheduler, test_callback])\n",
    "    cgl_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Train and evaluate the model for heart failure prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(\"\\n******************Training heart failure prediction task on {}******************\\n\".format(dataset))\n",
    "\n",
    "    config = {\n",
    "            'patient_code_adj': tf.constant(all_data[dataset]['patient_code_adj'], dtype=tf.float32),\n",
    "            'code_code_adj': tf.constant(all_data[dataset]['code_code_adj'], dtype=tf.float32),\n",
    "            'code_levels': tf.constant(all_data[dataset]['code_levels'], dtype=tf.int32),\n",
    "            'code_num_in_levels': np.max(all_data[dataset]['code_levels'], axis=0) + 1,\n",
    "            'patient_num': all_data[dataset]['train_codes_x'].shape[0],\n",
    "            'max_visit_seq_len': all_data[dataset]['train_codes_x'].shape[1],\n",
    "            'output_dim': 1,\n",
    "            'lambda': 0.1,\n",
    "            'activation': 'sigmoid'\n",
    "        }\n",
    "\n",
    "    visit_rnn_dims = [200]\n",
    "    hyper_params = {\n",
    "        'code_dims': [32, 32, 32, 32],\n",
    "        'patient_dim': 16,\n",
    "        'word_dim': 16,\n",
    "        'patient_hidden_dims': [32],\n",
    "        'code_hidden_dims': [64, 128],\n",
    "        'visit_rnn_dims': visit_rnn_dims,\n",
    "        'visit_attention_dim': 32,\n",
    "        'note_attention_dim': visit_rnn_dims[-1]\n",
    "    }\n",
    "\n",
    "    test_codes_gen = DataGenerator([ all_data[dataset]['test_codes_x'], all_data[dataset]['test_visit_lens']], shuffle=False)\n",
    "\n",
    "    def lr_schedule_fn(epoch, lr):\n",
    "        if epoch < 8:\n",
    "            lr = 0.1\n",
    "        elif epoch < 20:\n",
    "            lr = 0.01\n",
    "        elif epoch < 50:\n",
    "            lr = 0.001\n",
    "        else:\n",
    "            lr = 0.0001\n",
    "        return lr\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule_fn)\n",
    "    test_callback = EvaluateHFCallBack(test_codes_gen, all_data[dataset]['test_hf_y'])\n",
    "\n",
    "    cgl_model = CGL(config, hyper_params)\n",
    "    cgl_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[tf.metrics.AUC()])\n",
    "    cgl_model.fit(x={\n",
    "        'visit_codes': all_data[dataset]['train_codes_x'],\n",
    "        'visit_lens': all_data[dataset]['train_visit_lens'],\n",
    "    }, y= all_data[dataset]['train_hf_y'].astype(float), validation_data=({\n",
    "        'visit_codes': all_data[dataset]['valid_codes_x'],\n",
    "        'visit_lens': all_data[dataset]['valid_visit_lens'],\n",
    "    }, all_data[dataset]['valid_hf_y'].astype(float)), epochs=num_epochs, batch_size=32, callbacks=[lr_scheduler, test_callback])\n",
    "    cgl_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
