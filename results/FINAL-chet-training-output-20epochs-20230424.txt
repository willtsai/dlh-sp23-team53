training for h task on mimic3 dataset:
486845
Epoch 1 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m13.3s, loss: 0.494444423
    Evaluation: loss: 0.4734 --- auc: 0.8433 --- f1_score: 0.7277
Epoch 2 / 20:
    Step 188 / 188, LR: 0.001, time cost: 4m17.8s, loss: 0.406060307
    Evaluation: loss: 0.4616 --- auc: 0.8514 --- f1_score: 0.7248
Epoch 3 / 20:
    Step 188 / 188, LR: 0.0001, time cost: 4m20.6s, loss: 0.381717121
    Evaluation: loss: 0.4649 --- auc: 0.8507 --- f1_score: 0.7158
Epoch 4 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m15.7s, loss: 0.378989022
    Evaluation: loss: 0.4653 --- auc: 0.8504 --- f1_score: 0.7158
Epoch 5 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m15.6s, loss: 0.378686629
    Evaluation: loss: 0.4656 --- auc: 0.8503 --- f1_score: 0.7158
Epoch 6 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m16.6s, loss: 0.378383753
    Evaluation: loss: 0.4659 --- auc: 0.8502 --- f1_score: 0.7123
Epoch 7 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m14.9s, loss: 0.378080664
    Evaluation: loss: 0.4662 --- auc: 0.8501 --- f1_score: 0.7123
Epoch 8 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m12.6s, loss: 0.377777966
    Evaluation: loss: 0.4666 --- auc: 0.8500 --- f1_score: 0.7143
Epoch 9 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m14.6s, loss: 0.377575927
    Evaluation: loss: 0.4668 --- auc: 0.8500 --- f1_score: 0.7143
Epoch 10 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m16.6s, loss: 0.377272651
    Evaluation: loss: 0.4671 --- auc: 0.8498 --- f1_score: 0.7143
Epoch 11 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m15.7s, loss: 0.376969009
    Evaluation: loss: 0.4673 --- auc: 0.8497 --- f1_score: 0.7143
Epoch 12 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m20.0s, loss: 0.376666954
    Evaluation: loss: 0.4676 --- auc: 0.8496 --- f1_score: 0.7143
Epoch 13 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m19.6s, loss: 0.376363001
    Evaluation: loss: 0.4679 --- auc: 0.8495 --- f1_score: 0.7143
Epoch 14 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m18.4s, loss: 0.376161117
    Evaluation: loss: 0.4681 --- auc: 0.8493 --- f1_score: 0.7143
Epoch 15 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m16.7s, loss: 0.375858374
    Evaluation: loss: 0.4685 --- auc: 0.8492 --- f1_score: 0.7143
Epoch 16 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m18.1s, loss: 0.375555644
    Evaluation: loss: 0.4687 --- auc: 0.8490 --- f1_score: 0.7143
Epoch 17 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m16.3s, loss: 0.375353696
    Evaluation: loss: 0.4688 --- auc: 0.8490 --- f1_score: 0.7143
Epoch 18 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m15.9s, loss: 0.375050225
    Evaluation: loss: 0.4693 --- auc: 0.8489 --- f1_score: 0.7143
Epoch 19 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m15.5s, loss: 0.374747090
    Evaluation: loss: 0.4694 --- auc: 0.8489 --- f1_score: 0.7143
Epoch 20 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m15.4s, loss: 0.374545795
    Evaluation: loss: 0.4697 --- auc: 0.8488 --- f1_score: 0.7143
Evaluating model on test data...
    Evaluation: loss: 0.4608 --- auc: 0.8509 --- f1_score: 0.7199
Test loss: 0.4608249053955078, F1 score: 0.7198879551820728, AUC or TopK: 0.8508809311655496

****************************************************************************************************

training for h task on mimic4 dataset:
579405
Epoch 1 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m21.7s, loss: 0.27622614
    Evaluation: loss: 0.2717 --- auc: 0.9119 --- f1_score: 0.7357
Epoch 2 / 20:
    Step 250 / 250, LR: 0.001, time cost: 15m30.0s, loss: 0.21655150
    Evaluation: loss: 0.2540 --- auc: 0.9226 --- f1_score: 0.7632
Epoch 3 / 20:
    Step 250 / 250, LR: 0.0001, time cost: 15m30.0s, loss: 0.20855679
    Evaluation: loss: 0.2537 --- auc: 0.9225 --- f1_score: 0.7632
Epoch 4 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m35.3s, loss: 0.20700020
    Evaluation: loss: 0.2538 --- auc: 0.9225 --- f1_score: 0.7632
Epoch 5 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m29.4s, loss: 0.20699931
    Evaluation: loss: 0.2538 --- auc: 0.9225 --- f1_score: 0.7632
Epoch 6 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m25.7s, loss: 0.20677155
    Evaluation: loss: 0.2537 --- auc: 0.9225 --- f1_score: 0.7632
Epoch 7 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m29.6s, loss: 0.20666727
    Evaluation: loss: 0.2538 --- auc: 0.9224 --- f1_score: 0.7632
Epoch 8 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m35.7s, loss: 0.20644360
    Evaluation: loss: 0.2538 --- auc: 0.9224 --- f1_score: 0.7632
Epoch 9 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m27.0s, loss: 0.20622458
    Evaluation: loss: 0.2538 --- auc: 0.9224 --- f1_score: 0.7632
Epoch 10 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m28.3s, loss: 0.20611829
    Evaluation: loss: 0.2538 --- auc: 0.9224 --- f1_score: 0.7632
Epoch 11 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m25.7s, loss: 0.20600753
    Evaluation: loss: 0.2538 --- auc: 0.9224 --- f1_score: 0.7632
Epoch 12 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m26.9s, loss: 0.20588614
    Evaluation: loss: 0.2538 --- auc: 0.9224 --- f1_score: 0.7632
Epoch 13 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m27.3s, loss: 0.20566840
    Evaluation: loss: 0.2538 --- auc: 0.9224 --- f1_score: 0.7632
Epoch 14 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m27.9s, loss: 0.20555697
    Evaluation: loss: 0.2538 --- auc: 0.9223 --- f1_score: 0.7632
Epoch 15 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m26.6s, loss: 0.20533119
    Evaluation: loss: 0.2538 --- auc: 0.9223 --- f1_score: 0.7632
Epoch 16 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m28.0s, loss: 0.20522024
    Evaluation: loss: 0.2538 --- auc: 0.9222 --- f1_score: 0.7632
Epoch 17 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m28.2s, loss: 0.20511016
    Evaluation: loss: 0.2538 --- auc: 0.9222 --- f1_score: 0.7632
Epoch 18 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m27.7s, loss: 0.20499505
    Evaluation: loss: 0.2538 --- auc: 0.9223 --- f1_score: 0.7632
Epoch 19 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m26.9s, loss: 0.20488390
    Evaluation: loss: 0.2538 --- auc: 0.9223 --- f1_score: 0.7632
Epoch 20 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m26.1s, loss: 0.20466009
    Evaluation: loss: 0.2538 --- auc: 0.9223 --- f1_score: 0.7632
Evaluating model on test data...
    Evaluation: loss: 0.2467 --- auc: 0.9296 --- f1_score: 0.7437
Test loss: 0.24667868456244468, F1 score: 0.743661971830986, AUC or TopK: 0.9296305957421193

****************************************************************************************************

training for m task on mimic3 dataset:
1223574
Epoch 1 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m10.6s, loss: 1606.968181802
    Evaluation: loss: 150.5803 --- f1_score: 0.1066 --- top_k_recall: 0.1850, 0.2698, 0.3342, 0.3703  --- occurred: 0.0987, 0.1256, 0.1475, 0.1598  --- not occurred: 0.0863, 0.1442, 0.1867, 0.2105
Epoch 2 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m7.0s, loss: 1559.1020020831
    Evaluation: loss: 122.7962 --- f1_score: 0.1092 --- top_k_recall: 0.1961, 0.2711, 0.3313, 0.3761  --- occurred: 0.1041, 0.1302, 0.1454, 0.1592  --- not occurred: 0.0920, 0.1409, 0.1859, 0.2169
Epoch 3 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m5.8s, loss: 1558.9287287256
    Evaluation: loss: 128.0770 --- f1_score: 0.1250 --- top_k_recall: 0.2011, 0.2861, 0.3521, 0.4009  --- occurred: 0.1031, 0.1326, 0.1568, 0.1710  --- not occurred: 0.0980, 0.1535, 0.1953, 0.2299
Epoch 4 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m6.6s, loss: 1558.5080080403
    Evaluation: loss: 129.9187 --- f1_score: 0.1337 --- top_k_recall: 0.2116, 0.3004, 0.3629, 0.4085  --- occurred: 0.1059, 0.1392, 0.1620, 0.1784  --- not occurred: 0.1057, 0.1611, 0.2009, 0.2301
Epoch 5 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m5.9s, loss: 1557.7344344852
    Evaluation: loss: 135.1442 --- f1_score: 0.1393 --- top_k_recall: 0.2166, 0.3068, 0.3722, 0.4201  --- occurred: 0.1132, 0.1455, 0.1681, 0.1837  --- not occurred: 0.1034, 0.1613, 0.2041, 0.2364
Epoch 6 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m6.2s, loss: 1557.4142142167
    Evaluation: loss: 123.7903 --- f1_score: 0.1440 --- top_k_recall: 0.2234, 0.3139, 0.3731, 0.4243  --- occurred: 0.1216, 0.1495, 0.1710, 0.1894  --- not occurred: 0.1018, 0.1644, 0.2021, 0.2349
Epoch 7 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m6.4s, loss: 1556.9843843037
    Evaluation: loss: 114.4666 --- f1_score: 0.1473 --- top_k_recall: 0.2223, 0.3167, 0.3783, 0.4262  --- occurred: 0.1185, 0.1507, 0.1711, 0.1914  --- not occurred: 0.1037, 0.1661, 0.2072, 0.2348
Epoch 8 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m8.7s, loss: 1556.6573573014
    Evaluation: loss: 119.1582 --- f1_score: 0.1653 --- top_k_recall: 0.2374, 0.3318, 0.3913, 0.4368  --- occurred: 0.1333, 0.1644, 0.1841, 0.1989  --- not occurred: 0.1041, 0.1674, 0.2073, 0.2379
Epoch 9 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m11.8s, loss: 1555.296262438
    Evaluation: loss: 113.4357 --- f1_score: 0.1689 --- top_k_recall: 0.2395, 0.3337, 0.3992, 0.4488  --- occurred: 0.1312, 0.1628, 0.1857, 0.2027  --- not occurred: 0.1083, 0.1708, 0.2135, 0.2461
Epoch 10 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m11.6s, loss: 1555.219191398
    Evaluation: loss: 124.6995 --- f1_score: 0.1821 --- top_k_recall: 0.2518, 0.3461, 0.4123, 0.4562  --- occurred: 0.1402, 0.1745, 0.2000, 0.2135  --- not occurred: 0.1116, 0.1716, 0.2123, 0.2426
Epoch 11 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m10.8s, loss: 1555.041414759
    Evaluation: loss: 119.2452 --- f1_score: 0.1891 --- top_k_recall: 0.2553, 0.3518, 0.4129, 0.4636  --- occurred: 0.1458, 0.1815, 0.2012, 0.2140  --- not occurred: 0.1096, 0.1703, 0.2117, 0.2496
Epoch 12 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m10.2s, loss: 1553.967171540
    Evaluation: loss: 109.1906 --- f1_score: 0.2003 --- top_k_recall: 0.2606, 0.3601, 0.4224, 0.4674  --- occurred: 0.1520, 0.1853, 0.2058, 0.2187  --- not occurred: 0.1085, 0.1747, 0.2167, 0.2487
Epoch 13 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m8.5s, loss: 1553.1476476155
    Evaluation: loss: 101.2076 --- f1_score: 0.2002 --- top_k_recall: 0.2557, 0.3540, 0.4223, 0.4661  --- occurred: 0.1527, 0.1894, 0.2081, 0.2184  --- not occurred: 0.1030, 0.1646, 0.2142, 0.2477
Epoch 14 / 20:
    Step 188 / 188, LR: 0.01, time cost: 4m6.3s, loss: 1552.1923923688
    Evaluation: loss: 108.7771 --- f1_score: 0.2082 --- top_k_recall: 0.2642, 0.3605, 0.4167, 0.4629  --- occurred: 0.1590, 0.1922, 0.2086, 0.2218  --- not occurred: 0.1052, 0.1683, 0.2081, 0.2410
Epoch 15 / 20:
    Step 188 / 188, LR: 0.001, time cost: 4m6.4s, loss: 1550.5330330270
    Evaluation: loss: 102.1754 --- f1_score: 0.2181 --- top_k_recall: 0.2741, 0.3713, 0.4360, 0.4809  --- occurred: 0.1628, 0.1966, 0.2155, 0.2278  --- not occurred: 0.1112, 0.1747, 0.2206, 0.2531
Epoch 16 / 20:
    Step 188 / 188, LR: 0.001, time cost: 4m6.3s, loss: 1549.7667667548
    Evaluation: loss: 102.7756 --- f1_score: 0.2191 --- top_k_recall: 0.2741, 0.3709, 0.4366, 0.4807  --- occurred: 0.1645, 0.1978, 0.2165, 0.2275  --- not occurred: 0.1096, 0.1731, 0.2201, 0.2532
Epoch 17 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m7.0s, loss: 1549.7517517976
    Evaluation: loss: 102.1625 --- f1_score: 0.2187 --- top_k_recall: 0.2745, 0.3703, 0.4370, 0.4807  --- occurred: 0.1645, 0.1975, 0.2166, 0.2273  --- not occurred: 0.1100, 0.1728, 0.2204, 0.2534
Epoch 18 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m7.1s, loss: 1549.7251251158
    Evaluation: loss: 101.7437 --- f1_score: 0.2184 --- top_k_recall: 0.2749, 0.3702, 0.4369, 0.4808  --- occurred: 0.1644, 0.1974, 0.2165, 0.2273  --- not occurred: 0.1105, 0.1728, 0.2204, 0.2535
Epoch 19 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m5.6s, loss: 1549.4958958704
    Evaluation: loss: 101.4757 --- f1_score: 0.2187 --- top_k_recall: 0.2754, 0.3703, 0.4371, 0.4808  --- occurred: 0.1645, 0.1974, 0.2166, 0.2272  --- not occurred: 0.1109, 0.1729, 0.2205, 0.2536
Epoch 20 / 20:
    Step 188 / 188, LR: 1e-05, time cost: 4m6.2s, loss: 1549.1008008210
    Evaluation: loss: 101.2007 --- f1_score: 0.2187 --- top_k_recall: 0.2749, 0.3703, 0.4375, 0.4807  --- occurred: 0.1642, 0.1975, 0.2168, 0.2272  --- not occurred: 0.1107, 0.1728, 0.2207, 0.2535
Evaluating model on test data...
    Evaluation: loss: 100.6438 --- f1_score: 0.2145 --- top_k_recall: 0.2656, 0.3633, 0.4238, 0.4720  --- occurred: 0.1643, 0.1980, 0.2137, 0.2248  --- not occurred: 0.1013, 0.1653, 0.2102, 0.2472
Test loss: 100.6438017642498, F1 score: 0.2144932085663336, AUC or TopK: [0.26557106286013665, 0.3632958057721857, 0.42383426859477064, 0.4719875133364834]

****************************************************************************************************

training for m task on mimic4 dataset:
1490841
Epoch 1 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m4.2s, loss: 1962.976868445
    Evaluation: loss: 160.9560 --- f1_score: 0.1086 --- top_k_recall: 0.1844, 0.2654, 0.3186, 0.3559  --- occurred: 0.1149, 0.1550, 0.1767, 0.1906  --- not occurred: 0.0695, 0.1104, 0.1419, 0.1653
Epoch 2 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m12.4s, loss: 1919.14766500
    Evaluation: loss: 139.8890 --- f1_score: 0.1237 --- top_k_recall: 0.1952, 0.2707, 0.3231, 0.3671  --- occurred: 0.1196, 0.1556, 0.1771, 0.1954  --- not occurred: 0.0756, 0.1151, 0.1460, 0.1717
Epoch 3 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m12.6s, loss: 1918.49788809
    Evaluation: loss: 129.8944 --- f1_score: 0.1377 --- top_k_recall: 0.2135, 0.2885, 0.3471, 0.3928  --- occurred: 0.1348, 0.1691, 0.1929, 0.2101  --- not occurred: 0.0787, 0.1194, 0.1542, 0.1827
Epoch 4 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m13.0s, loss: 1917.64566296
    Evaluation: loss: 127.3922 --- f1_score: 0.1564 --- top_k_recall: 0.2253, 0.3075, 0.3620, 0.4077  --- occurred: 0.1396, 0.1821, 0.2039, 0.2217  --- not occurred: 0.0856, 0.1254, 0.1581, 0.1861
Epoch 5 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m13.9s, loss: 1916.75955832
    Evaluation: loss: 134.5491 --- f1_score: 0.1684 --- top_k_recall: 0.2375, 0.3197, 0.3728, 0.4148  --- occurred: 0.1514, 0.1893, 0.2100, 0.2255  --- not occurred: 0.0860, 0.1304, 0.1629, 0.1893
Epoch 6 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m14.5s, loss: 1915.62411713
    Evaluation: loss: 125.1694 --- f1_score: 0.1751 --- top_k_recall: 0.2462, 0.3308, 0.3860, 0.4240  --- occurred: 0.1570, 0.1933, 0.2153, 0.2290  --- not occurred: 0.0892, 0.1375, 0.1707, 0.1950
Epoch 7 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m13.8s, loss: 1915.49100587
    Evaluation: loss: 117.3137 --- f1_score: 0.1829 --- top_k_recall: 0.2460, 0.3353, 0.3904, 0.4306  --- occurred: 0.1567, 0.2000, 0.2209, 0.2338  --- not occurred: 0.0893, 0.1352, 0.1695, 0.1968
Epoch 8 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m13.0s, loss: 1915.13611109
    Evaluation: loss: 131.8138 --- f1_score: 0.1926 --- top_k_recall: 0.2581, 0.3445, 0.3999, 0.4434  --- occurred: 0.1662, 0.2085, 0.2293, 0.2449  --- not occurred: 0.0920, 0.1360, 0.1706, 0.1985
Epoch 9 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m15.1s, loss: 1914.08233358
    Evaluation: loss: 119.0297 --- f1_score: 0.1992 --- top_k_recall: 0.2635, 0.3519, 0.4063, 0.4464  --- occurred: 0.1705, 0.2091, 0.2287, 0.2420  --- not occurred: 0.0930, 0.1427, 0.1776, 0.2043
Epoch 10 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m15.1s, loss: 1912.80344116
    Evaluation: loss: 111.2528 --- f1_score: 0.2128 --- top_k_recall: 0.2661, 0.3561, 0.4116, 0.4518  --- occurred: 0.1770, 0.2140, 0.2335, 0.2453  --- not occurred: 0.0892, 0.1421, 0.1782, 0.2065
Epoch 11 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m15.1s, loss: 1912.66899716
    Evaluation: loss: 109.7216 --- f1_score: 0.2178 --- top_k_recall: 0.2683, 0.3605, 0.4175, 0.4559  --- occurred: 0.1767, 0.2160, 0.2355, 0.2477  --- not occurred: 0.0916, 0.1446, 0.1821, 0.2082
Epoch 12 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m13.8s, loss: 1911.95688905
    Evaluation: loss: 106.8417 --- f1_score: 0.2203 --- top_k_recall: 0.2704, 0.3600, 0.4118, 0.4538  --- occurred: 0.1784, 0.2172, 0.2341, 0.2466  --- not occurred: 0.0920, 0.1428, 0.1777, 0.2072
Epoch 13 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m14.8s, loss: 1911.07411248
    Evaluation: loss: 103.1915 --- f1_score: 0.2294 --- top_k_recall: 0.2789, 0.3672, 0.4162, 0.4537  --- occurred: 0.1862, 0.2217, 0.2392, 0.2512  --- not occurred: 0.0928, 0.1455, 0.1770, 0.2025
Epoch 14 / 20:
    Step 250 / 250, LR: 0.01, time cost: 15m15.8s, loss: 1911.30800077
    Evaluation: loss: 95.8660 --- f1_score: 0.2268 --- top_k_recall: 0.2724, 0.3596, 0.4109, 0.4530  --- occurred: 0.1843, 0.2214, 0.2370, 0.2509  --- not occurred: 0.0881, 0.1382, 0.1740, 0.2022
Epoch 15 / 20:
    Step 250 / 250, LR: 0.001, time cost: 15m15.3s, loss: 1908.66222684
    Evaluation: loss: 92.4118 --- f1_score: 0.2394 --- top_k_recall: 0.2887, 0.3736, 0.4276, 0.4644  --- occurred: 0.1946, 0.2284, 0.2448, 0.2563  --- not occurred: 0.0941, 0.1452, 0.1828, 0.2081
Epoch 16 / 20:
    Step 250 / 250, LR: 0.001, time cost: 15m14.7s, loss: 1907.61311094
    Evaluation: loss: 90.7932 --- f1_score: 0.2423 --- top_k_recall: 0.2915, 0.3740, 0.4302, 0.4682  --- occurred: 0.1956, 0.2281, 0.2471, 0.2592  --- not occurred: 0.0959, 0.1459, 0.1832, 0.2090
Epoch 17 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m14.6s, loss: 1907.99900575
    Evaluation: loss: 90.8262 --- f1_score: 0.2424 --- top_k_recall: 0.2914, 0.3734, 0.4302, 0.4684  --- occurred: 0.1956, 0.2281, 0.2471, 0.2593  --- not occurred: 0.0958, 0.1453, 0.1831, 0.2091
Epoch 18 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m14.0s, loss: 1907.74611695
    Evaluation: loss: 90.8824 --- f1_score: 0.2421 --- top_k_recall: 0.2916, 0.3736, 0.4301, 0.4683  --- occurred: 0.1958, 0.2280, 0.2468, 0.2593  --- not occurred: 0.0958, 0.1455, 0.1833, 0.2090
Epoch 19 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m17.1s, loss: 1908.16500822
    Evaluation: loss: 90.8481 --- f1_score: 0.2425 --- top_k_recall: 0.2915, 0.3733, 0.4299, 0.4672  --- occurred: 0.1959, 0.2278, 0.2465, 0.2582  --- not occurred: 0.0956, 0.1455, 0.1833, 0.2090
Epoch 20 / 20:
    Step 250 / 250, LR: 1e-05, time cost: 15m17.1s, loss: 1907.37044049
    Evaluation: loss: 90.9456 --- f1_score: 0.2427 --- top_k_recall: 0.2915, 0.3738, 0.4297, 0.4671  --- occurred: 0.1959, 0.2279, 0.2464, 0.2583  --- not occurred: 0.0956, 0.1459, 0.1832, 0.2088
Evaluating model on test data...
    Evaluation: loss: 92.6969 --- f1_score: 0.2447 --- top_k_recall: 0.2854, 0.3750, 0.4311, 0.4679  --- occurred: 0.1936, 0.2279, 0.2441, 0.2545  --- not occurred: 0.0918, 0.1471, 0.1871, 0.2134
Test loss: 92.69687088603527, F1 score: 0.24469900736051192, AUC or TopK: [0.2854095805863638, 0.3749995602431907, 0.4311128905307906, 0.4678522696796106]

****************************************************************************************************

## Python dictionaries to store the results

test_results =  {'mimic3-h': {'test_loss': 0.4608249053955078, 'f1_score': 0.7198879551820728, 'auc_or_topk': 0.8508809311655496}, 'mimic4-h': {'test_loss': 0.24667868456244468, 'f1_score': 0.743661971830986, 'auc_or_topk': 0.9296305957421193}, 'mimic3-m': {'test_loss': 100.6438017642498, 'f1_score': 0.2144932085663336, 'auc_or_topk': [0.26557106286013665, 0.3632958057721857, 0.42383426859477064, 0.4719875133364834]}, 'mimic4-m': {'test_loss': 92.69687088603527, 'f1_score': 0.24469900736051192, 'auc_or_topk': [0.2854095805863638, 0.3749995602431907, 0.4311128905307906, 0.4678522696796106]}}
train_results =  {'mimic3-h': {'model': Model(
  (graph_layer): GraphLayer(
    (fc): Linear(in_features=48, out_features=32, bias=True)
    (LeakyReLU): LeakyReLU(negative_slope=0.01)
  )
  (transition_layer): TransitionLayer(
    (gru): GRUCell(32, 150)
    (attention): SingleHeadAttentionLayer(
      (query_dense): Linear(in_features=32, out_features=32, bias=True)
      (key_dense): Linear(in_features=32, out_features=32, bias=True)
      (value_dense): Linear(in_features=32, out_features=150, bias=True)
    )
    (activation): Tanh()
  )
  (attention): EmbeddingWithAttentionLayer(
    (linear): Linear(in_features=150, out_features=32, bias=True)
  )
  (classifier): Classifier(
    (linear): Linear(in_features=150, out_features=1, bias=True)
    (activation): Sigmoid()
    (dropout): Dropout(p=0.0, inplace=False)
  )
), 'epoch_lrs': [0.01, 0.001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05], 'valid_losses': [0.4733937790742027, 0.461564230145112, 0.46489917603274145, 0.4652823386883881, 0.46558182422094363, 0.4658739956711659, 0.46617114658771613, 0.46656549666039115, 0.46676631504575333, 0.4670846025793838, 0.4673480882364161, 0.4676196547357354, 0.46790375054245303, 0.4681460249859228, 0.46847287600229043, 0.46871372470991124, 0.46884415146061664, 0.4692546948337168, 0.46943467208628237, 0.4697221991498378], 'mean_losses': [0.49443248923619587, 0.4060215106010437, 0.38171500142415365, 0.3789307129383087, 0.3786280857721965, 0.37833402411142986, 0.378040034532547, 0.37774401672681174, 0.3774589500427246, 0.37718067614237466, 0.3769075496991475, 0.3766193824609121, 0.3763437847296397, 0.37606630754470827, 0.375798388560613, 0.3755302503903707, 0.37527356926600136, 0.37501328309377036, 0.3747467569510142, 0.37447391311327616], 'time_costs': ['4m13.3s', '4m17.8s', '4m20.6s', '4m15.7s', '4m15.6s', '4m16.6s', '4m14.9s', '4m12.6s', '4m14.6s', '4m16.6s', '4m15.7s', '4m20.0s', '4m19.6s', '4m18.4s', '4m16.7s', '4m18.1s', '4m16.3s', '4m15.9s', '4m15.5s', '4m15.4s'], 'f1_scores': [0.7277486910994764, 0.7247956403269754, 0.7158469945355191, 0.7158469945355191, 0.7158469945355191, 0.7123287671232877, 0.7123287671232877, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142, 0.7142857142857142], 'auc_or_topks': [0.8433323077974908, 0.8513656718968994, 0.8506648890712063, 0.8504255973746283, 0.8502546747342152, 0.8502204902061328, 0.850066659829761, 0.8500153830376372, 0.8499641062455132, 0.8497760913410589, 0.8497077222848939, 0.8495538919085223, 0.849485522852357, 0.8493316924759854, 0.8491778620996138, 0.849024031723242, 0.8489898471951594, 0.8488702013468705, 0.8488531090828292, 0.8487505554985812]}, 'mimic4-h': {'model': Model(
  (graph_layer): GraphLayer(
    (fc): Linear(in_features=48, out_features=32, bias=True)
    (LeakyReLU): LeakyReLU(negative_slope=0.01)
  )
  (transition_layer): TransitionLayer(
    (gru): GRUCell(32, 150)
    (attention): SingleHeadAttentionLayer(
      (query_dense): Linear(in_features=32, out_features=32, bias=True)
      (key_dense): Linear(in_features=32, out_features=32, bias=True)
      (value_dense): Linear(in_features=32, out_features=150, bias=True)
    )
    (activation): Tanh()
  )
  (attention): EmbeddingWithAttentionLayer(
    (linear): Linear(in_features=150, out_features=32, bias=True)
  )
  (classifier): Classifier(
    (linear): Linear(in_features=150, out_features=1, bias=True)
    (activation): Sigmoid()
    (dropout): Dropout(p=0.0, inplace=False)
  )
), 'epoch_lrs': [0.01, 0.001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05], 'valid_losses': [0.27167638850212095, 0.2539888452887535, 0.25374845468997953, 0.2537652896046638, 0.25375805610418317, 0.25373742669820787, 0.25377253532409666, 0.25376347357034684, 0.25376740682125093, 0.2537695310115814, 0.25379463100433347, 0.2537713150978088, 0.2537611572742462, 0.25380685305595396, 0.2537700163722038, 0.253771558880806, 0.25377387619018554, 0.2537980700135231, 0.2537809707522392, 0.2537877061367035], 'mean_losses': [0.27623526041209695, 0.21648624527454377, 0.2085123190432787, 0.2070304554402828, 0.20687417013943196, 0.20671618841588496, 0.20656067740917206, 0.20640064631402494, 0.20624495625495912, 0.20609396366775035, 0.20595038358867168, 0.20579131297767161, 0.20564388199150563, 0.20549914832413196, 0.20534743958711624, 0.20520352049916982, 0.205052520185709, 0.20490029300749302, 0.2047527652680874, 0.2046004485487938], 'time_costs': ['15m21.7s', '15m30.0s', '15m30.0s', '15m35.3s', '15m29.4s', '15m25.7s', '15m29.6s', '15m35.7s', '15m27.0s', '15m28.3s', '15m25.7s', '15m26.9s', '15m27.3s', '15m27.9s', '15m26.6s', '15m28.0s', '15m28.2s', '15m27.7s', '15m26.9s', '15m26.1s'], 'f1_scores': [0.7356948228882834, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421], 'auc_or_topks': [0.911935021500239, 0.9226437330785157, 0.9225163242554547, 0.9224844720496894, 0.9224526198439241, 0.9224589902850773, 0.922427138079312, 0.922433508520465, 0.922408026755853, 0.9223761745500878, 0.9223761745500876, 0.9223952858735467, 0.9223825449912406, 0.9223315814620163, 0.9223124701385571, 0.9222360248447206, 0.9222423952858737, 0.9222742474916388, 0.9222869883739448, 0.922286988373945]}, 'mimic3-m': {'model': Model(
  (graph_layer): GraphLayer(
    (fc): Linear(in_features=48, out_features=32, bias=True)
    (LeakyReLU): LeakyReLU(negative_slope=0.01)
  )
  (transition_layer): TransitionLayer(
    (gru): GRUCell(32, 150)
    (attention): SingleHeadAttentionLayer(
      (query_dense): Linear(in_features=32, out_features=32, bias=True)
      (key_dense): Linear(in_features=32, out_features=32, bias=True)
      (value_dense): Linear(in_features=32, out_features=150, bias=True)
    )
    (activation): Tanh()
  )
  (attention): EmbeddingWithAttentionLayer(
    (linear): Linear(in_features=150, out_features=32, bias=True)
  )
  (classifier): Classifier(
    (linear): Linear(in_features=150, out_features=4880, bias=True)
    (activation): Sigmoid()
    (dropout): Dropout(p=0.45, inplace=False)
  )
), 'epoch_lrs': [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001, 1e-05, 1e-05, 1e-05, 1e-05], 'valid_losses': [150.5803250281129, 122.79623432587417, 128.07700597097374, 129.91866132127345, 135.14418216070828, 123.7902942686729, 114.46661823087241, 119.15818141210393, 113.43570765175878, 124.69949782015586, 119.24515922703075, 109.1905706528476, 101.20760144465594, 108.77713400893231, 102.17536613345146, 102.77563452297485, 102.16253500530009, 101.74371410123474, 101.47566234545582, 101.20070388635081], 'mean_losses': [1606.968063545227, 1559.102038008372, 1558.9287372652689, 1558.508045978546, 1557.7343695640564, 1557.4141500473022, 1556.9843180529276, 1556.6573476219178, 1555.2961632665, 1555.2191143417358, 1555.041386159261, 1553.9671243858338, 1553.1476323509216, 1552.1923347409565, 1550.5329773139954, 1549.7667382685343, 1549.7517394256593, 1549.7251352310182, 1549.4958112462361, 1549.1007980219524], 'time_costs': ['4m10.6s', '4m7.0s', '4m5.8s', '4m6.6s', '4m5.9s', '4m6.2s', '4m6.4s', '4m8.7s', '4m11.8s', '4m11.6s', '4m10.8s', '4m10.2s', '4m8.5s', '4m6.3s', '4m6.4s', '4m6.3s', '4m7.0s', '4m7.1s', '4m5.6s', '4m6.2s'], 'f1_scores': [0.1065766855904969, 0.10924369575577428, 0.1250248059704996, 0.13369697861470534, 0.13931598426368932, 0.14396466889717804, 0.147325080408674, 0.1652928900322197, 0.1688870530698359, 0.182052341854104, 0.18912546636023136, 0.20033823562328967, 0.20022436454301998, 0.20820300378259302, 0.218147733912218, 0.21906230462274054, 0.21865172176383968, 0.21839717326391986, 0.21868304965997956, 0.21871317842362462], 'auc_or_topks': [[0.18501352598961224, 0.269824333766842, 0.3342150343208554, 0.37032603166392475], [0.19614430191064913, 0.27111814511502996, 0.3312789896184987, 0.37610144823860314], [0.20109597419726596, 0.28614210393439904, 0.352087117577174, 0.4009100885471256], [0.21159490335571005, 0.3003618831751454, 0.36290334562509996, 0.4084923515951998], [0.21659427930020875, 0.306805380965858, 0.37220024809570923, 0.4201483221631019], [0.2233581413892659, 0.3139471537507803, 0.3731169918492402, 0.4243008324448532], [0.22225657972294413, 0.3167424790474979, 0.37832271683768437, 0.4261695662956757], [0.23739423808781301, 0.33183332836608526, 0.39133491191691405, 0.43677504285296853], [0.23953889609205137, 0.33365937486822234, 0.39924127909112006, 0.44882693642779964], [0.2517955446492571, 0.3460889398262897, 0.4122792554221274, 0.456162281920708], [0.2553342118621814, 0.3517543863093084, 0.41290493567421366, 0.46359636112527247], [0.2605528501968531, 0.360078859919222, 0.42243921335814066, 0.46741113750063273], [0.2556914880980049, 0.35403775301858253, 0.4222997552360832, 0.46605197811414734], [0.26417745689220923, 0.3604618236220642, 0.41670710647907144, 0.462866959310854], [0.2740932039909263, 0.3712906422800752, 0.43603837670829726, 0.48089608940897677], [0.27409240313071664, 0.3708832161433985, 0.4365865463384607, 0.4806918246969954], [0.27451344929212795, 0.3703445849923703, 0.4369809569762791, 0.4807019021003614], [0.27487998429088245, 0.3701720189565421, 0.43688196956547387, 0.48075797817063626], [0.27535319539774733, 0.3703102154853842, 0.43711942086783384, 0.4807630491645511], [0.27491827046992184, 0.37025588340772553, 0.43745996744689614, 0.4807425237129912]]}, 'mimic4-m': {'model': Model(
  (graph_layer): GraphLayer(
    (fc): Linear(in_features=48, out_features=32, bias=True)
    (LeakyReLU): LeakyReLU(negative_slope=0.01)
  )
  (transition_layer): TransitionLayer(
    (gru): GRUCell(32, 150)
    (attention): SingleHeadAttentionLayer(
      (query_dense): Linear(in_features=32, out_features=32, bias=True)
      (key_dense): Linear(in_features=32, out_features=32, bias=True)
      (value_dense): Linear(in_features=32, out_features=150, bias=True)
    )
    (activation): Tanh()
  )
  (attention): EmbeddingWithAttentionLayer(
    (linear): Linear(in_features=150, out_features=32, bias=True)
  )
  (classifier): Classifier(
    (linear): Linear(in_features=150, out_features=6037, bias=True)
    (activation): Sigmoid()
    (dropout): Dropout(p=0.45, inplace=False)
  )
), 'epoch_lrs': [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001, 1e-05, 1e-05, 1e-05, 1e-05], 'valid_losses': [160.95599694010616, 139.8890400054306, 129.8943788460344, 127.3922211047113, 134.54913792552054, 125.16942313815653, 117.31374596768617, 131.8137692923993, 119.02972826060653, 111.25276632070542, 109.7215694616586, 106.84171725735068, 103.19149896802008, 95.86595830848069, 92.41182174292207, 90.79322854144127, 90.82622115649284, 90.88243093188852, 90.8480828699991, 90.94564967408031], 'mean_losses': [1962.976824410796, 1919.1475733208656, 1918.4977591078282, 1917.6455861949921, 1916.759496120572, 1915.6241330081225, 1915.4910256547928, 1915.1360919036865, 1914.082250451207, 1912.8033635327815, 1912.6689319931268, 1911.9567925038339, 1911.0741042928696, 1911.3080095728635, 1908.6622039730548, 1907.6131101603507, 1907.9990133191347, 1907.746083655715, 1908.1649993553162, 1907.370383928299], 'time_costs': ['15m4.2s', '15m12.4s', '15m12.6s', '15m13.0s', '15m13.9s', '15m14.5s', '15m13.8s', '15m13.0s', '15m15.1s', '15m15.1s', '15m15.1s', '15m13.8s', '15m14.8s', '15m15.8s', '15m15.3s', '15m14.7s', '15m14.6s', '15m14.0s', '15m17.1s', '15m17.1s'], 'f1_scores': [0.10864706944774748, 0.12374794087442668, 0.13774722526423883, 0.15639938501125886, 0.16836069698471706, 0.17511367628561358, 0.18292966964530435, 0.1925871473553963, 0.19917229342987963, 0.21276185882830828, 0.2177825407180179, 0.22025936672580984, 0.2293909489845707, 0.22676544908354287, 0.23939621050043988, 0.242263427542122, 0.2423664428962191, 0.24209397882244044, 0.2424989940175763, 0.2426648849669053], 'auc_or_topks': [[0.18440736880296038, 0.2654150598557971, 0.3185806671240179, 0.3558906713647651], [0.19516079928197208, 0.27065126820770313, 0.323143674681456, 0.3671320306804223], [0.21348997952601012, 0.28845604035454886, 0.3471206084596191, 0.3928211887037399], [0.22526520726829632, 0.30746885334705104, 0.361973622532461, 0.407733580362353], [0.23747548148315667, 0.3197162437293243, 0.37283585287469684, 0.41481927583334754], [0.24622782055574993, 0.33081678791304603, 0.38600458472755755, 0.42395561201360155], [0.24602876242736607, 0.3352607434714567, 0.3903935574036224, 0.4305998249922487], [0.25814455980425277, 0.3444726417319647, 0.39985439883201335, 0.44340685395160245], [0.26349699488595296, 0.3518628580039432, 0.40628111939233547, 0.4463588206578527], [0.2661377121650967, 0.3561235293274369, 0.4116376817496744, 0.45180354528984784], [0.26828512803456644, 0.3605221680170668, 0.4175189899633225, 0.4558737343743995], [0.27037467139174814, 0.36004635204647417, 0.41180368579091725, 0.45376338121299375], [0.27892293336811785, 0.367185640407212, 0.4161947536655495, 0.45365659619241355], [0.2724259511203495, 0.35961843952310685, 0.41093449927631376, 0.4530354007045916], [0.28867497111062757, 0.37358875822686183, 0.42759471517092273, 0.46442946069605234], [0.29153020809683733, 0.3740121614123463, 0.4302356795088945, 0.468223191948585], [0.2914022713376505, 0.37336940931150586, 0.43016251784006515, 0.46840925495964797], [0.29159043497140236, 0.3735514101022528, 0.4300741049386729, 0.4683041267573836], [0.29153085894801056, 0.37329436370420216, 0.4298505826208606, 0.46720457378842284], [0.29148800180515344, 0.3738392572093898, 0.42966308156380567, 0.46710073084773773]]}}
