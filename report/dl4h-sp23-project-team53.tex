%
% File dl4h-sp23-project-team53.tex
%
%% Based on the style files for EMNLP 2020, which were % Based on the style
%files for ACL 2020, which were % Based on the style files for ACL 2018, NAACL
%2018/19, which were % Based on the style files for ACL-2015, with some
%improvements %  taken from the NAACL-2016 style % Based on the style files for
%ACL-2014, which were, in turn, % based on ACL-2013, ACL-2012, ACL-2011,
%ACL-2010, ACL-IJCNLP-2009, % EACL-2009, IJCNLP-2008... % Based on the style
%files for EACL 2006 by %e.agirre@ehu.es or Sergi.Balari@uab.es % and that of
%ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper,fleqn]{article}
\usepackage[hyperref]{acl2021}
\usepackage{hyperref}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{float}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out, but it will improve
% the layout of the manuscript, and will typically save some space.
\usepackage{microtype}

\aclfinalcopy
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm} You can expand the titlebox if you need extra space to
% show all the authors. Please do not make the titlebox smaller than 5cm (the
% original size); we will check this in the camera-ready version and ask you to
% change it back.

% Content lightly modified from original work by Jesse Dodge and Noah Smith


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{ Reproducing Context-aware Health Event Prediction via \\
  Transition Functions on Dynamic Disease Graphs \cite{chet} \\
  CS598 DL4H Spring 2023 }

\author{Shiyu (Sherry) Li and Wei-Lun (Will) Tsai \\
  \texttt{\{shiyuli2, wltsai2\}@illinois.edu} \\[2em]
  Group ID: 53\\
  Paper ID: 28\\
  Presentation link: \url{\#TODO} \\
  Code link: \url{https://github.com/willtsai/dlh-sp23-team53}} 

\begin{document}
\maketitle

% All sections are mandatory. Keep in mind that your page limit is 8, excluding
% references. For specific grading rubrics, please see the project instruction.

\section{Introduction}
In this report, we will focus on our reproduction study for
\textit{Context-aware Health Event Prediction via Transition Functions on
Dynamic Disease Graphs} \cite{chet}. This paper propose a new deep learning
model called Chet (\textbf{c}ontext-aware \textbf{h}ealth \textbf{e}vent
prediction via \textbf{t}ransition functions on dynamic disease graphs) that
leverages the relationship between diseases and how they develop over time to
predict future outcomes and diagnoses. Existing research on deep learning models
for classification and prediction of diseases based on longitudinal EHR data
have modeled disease diagnoses as independent events in their respective visits.
However, intuition and data indicate that there are in fact hidden patterns
within the combinations of disease diagnoses that may be useful for predicting
future outcomes for patients, but yet have not been leveraged in existing
best-in-class healthcare deep learning models. The Chet model is able to learn
how diagnosed diseases develop over the course of each patient's doctor visits
and then utilize this learned disease combination context to predict future
outcomes and diagnoses. The most innovative part of the approach is the design
to include both disease combinational information and the dynamic scheme of
disease into the model. To include disease combinational information, the paper
constructed a weighted disease combination based on the entire longitudinal EHR
data globally and also a disease subgraph based on the specific visit locally.
To include dynamic scheme of diseases, the paper utilized a disease-level
temporal learning with multiple diagnosis roles and corresponding transition
functions to extract historical contexts.

\section{Scope of reproducibility}
In our reproduction study, we will use the same methodology as proposed by the
authors for data selection, cleaning, and preprocessing. Specifically, we will
join the MIMIC-III \cite{mimic3} and MIMIC-IV \cite{mimic4} datasets along the
same overlapping time ranges that the authors describe and split
training/validation/test sets randomly using the same ratios they used. We will
build the diagnosis graphs and calculate the adjacency matrices for their
corresponding subgraphs using the same methodology described by the authors. We
will train the model and at least one listed baseline model for diagnosis
prediction and hearth failure prediction respectively and compare the
performance.
\subsection{Addressed claims from the original paper}
% \# TODO: Clearly itemize the claims you are testing:
\begin{itemize}
    \item By utilizing disease combinational information and the dynamic scheme
    of diseases, the Chet model has higher accuracy for diagnosis prediction and
    hearth failure prediction than CGL \cite{cgl}, which is the most performant
    of the baseline models. 
    \item The designed global disease graph and visit subgraphs can integrate
    global and local context from disease combinations to inform the deep
    learning model, so the Chet model has higher accuracy for diagnosis
    prediction and hearth failure prediction than $Chet_{d-}$ model where
    dynamic part of GNN is removed in Chet.
    \item The proposed three diagnosis roles and corresponding transition
    functions can extract historical context and learn the disease development
    schemes, so the Chet model has higher accuracy for diagnosis prediction and
    heart failure prediction than $Chet_{t-}$ model where transaction functions
    are removed in Chet.
    \item The Chet model has higher accuracy for diagnosis prediction and heart
    failure prediction than $Chet_{att-}$ model where attention from the final
    patient embedding layer is removed in Chet. This is an additional ablation
    study we proposed.
\end{itemize}

\section{Methodology}
In this section , we demonstrate the details of the model used in the original
paper, our implementation approach as well as necessary computational resource.
\subsection{Model descriptions}
The Chet model can be decomposed into three layers: graph layer, transition
layer and embedding layer.

\subsubsection{Graph Layer}
The first layer is a dynamic graph learning layer to extract both local and
global contexts for diagnosis and neighbors in visit t using a memory-efficient
calculation:
\begin{equation}
  Z_D^t =m^t \odot (M + A(m^t\odot M) + A(n^t\odot N))
\end{equation}
\begin{equation}
  Z_N^t =n^t \odot (N + A(n^t\odot N) + A(m^t\odot M))
\end{equation}
Where M,N represent embedding matrices for diagnoses and neighbors, A is the
static adjacency matrix, $m^t$ and $n^t$ represent diagnoses code and neighbors
code in t visit, $Z_D^t$ is aggregated diagnosis local context and diagnosis
global context and $Z_N^t$ is aggregated neighbor global context. Finally, the
GNN outputs are calculated with a fully connected layer with LeakyReLU as the
activation function from $Z_D^t$ and $Z_N^t$.
\begin{equation}
  H^t_{D,N}=\textrm{LeakyReLU}(Z^t_{D,N}W)\in\mathcal{R}^{d\times s'}
\end{equation}

\subsubsection{Transition Layer}
The Transition (second) Layer is to learn the disease development schemes, it
takes the vector of diagnosis codes $m^t$ per visit as input and partitions it
into three disjoint vectors: (1) persistent diseases $m_p^t$ representing
diagnoses in visit $t$ that are also present in visit $t-1$, (2) emerging
neighbors $m_{en}^t$ representing diagnoses in visit $t$ that are neighbors in
visit $t-1$, (3) emerging unrelated diseases $m_{eu}^t$ representing diagnoses
in visit $t$ that are unrelated diseases in visit $t-1$. The layer is composed
of three transition functions corresponding to each partition of $m^t$ and are
designed to extract historical context from previous visits to compute the
hidden values. The transition function for calculating the hidden values for
both $m_{en}^t$ and $m_{eu}^t$ is a scaled dot-product attention
\cite{dp_attention}: 

\begin{equation}
\begin{tabular}{l}
Attn \\ (Q, K, V) 
\end{tabular}
=
\begin{tabular}{l}
  soft \\
  max
\end{tabular}\left(\frac{QW_q(KW_k)^T}{\sqrt{a}}\right)VW_v 
\end{equation}
\normalsize
Where $a$ is the attention size, $W_q$, $W_k$, $W_v$ are the weight matrices.
For $h_{en}^t$, $Q$ and $K$ are the hidden neighbor embeddings $H_N^{t-1}$. For
$h_{eu}^t$, $Q$ and $K$ are the universal embeddings of unrelated diseases $R$.
For both $h_{en}^t$ and $h_{eu}^t$, $V$ is the diagnosis embeddings $H^{t}_D$.
The transition function for calculating the hidden values for $m_p^t$ is a
modified gated recurrent unit (M-GRU) \cite{gru}:

\begin{equation}
  h_p^t = {M\verb|-|GRU}(m_p^t \odot H_D^t , h_{en}^t, h_{eu}^t, h_p^{t-1})
\end{equation}

Finally, to calculate the visit embedding $v^t$, we apply max pooling to the
transition output of the three partitions, which are all contained in $h_p^t$:
\begin{equation}
  v^t = \verb|max_pooling|(h_p^t).
\end{equation}

\subsubsection{Embedding Layer}
The third layer is an embedding layer with a location-based attention to
calculate the final hidden representation of all visits embeddings.
\begin{equation}
  \alpha=\textrm{softmax}([v^1,v^2,\dots, v^T]W_\alpha)\in \mathcal{R}^T
\end{equation}
\begin{equation}
  o=\alpha[v^1,v^2,\dots, v^T]^T\in\mathcal{R}^p
\end{equation}
Where $W_\alpha$ is a context vector for attention, $\alpha$ is the attention
score for visits and o represents the final patient embedding.

\subsection{Data descriptions}
For our reproduction study, we will use the MIMIC-III \cite{mimic3} and
MIMIC-IV \cite{mimic4} datasets downloaded from PhysioNet \cite{physionet} for
training/validation/testing, same as the original paper. In MIMIC-III data,
there are 7493 patients in total from 2001 to 2012 with an average of 2.6 visits
per patient and an average of 13.06 diagnose codes per visit. We randomly split
the data into training set, validation set and test set with a size of 6000, 493
and 1000 respectively. In MIMIC-IV data, there are 10000 patients in total from
2013 to 2019 with an average of 3.79 visits per patient and an average of 13.51
diagnose codes per visit. We split the data into training set, validation set
and test set with a size of 8000, 1000 and 1000 respectively. 

\subsection{Hyperparameters}
% Describe how you set the hyperparameters and what the source was for
% their value (e.g. paper, code or your guess). 
The authors had randomly initialized model parameters and tuned on the
validation dataset to arrive at the optimal values described in their paper.
Thus, we set most of the hyperparameters to the same values as the original 
paper for our reproduction experiments$: batch size = 32, hidden size = 150,
dropout rate = 0.45$ (diagnosis prediction), and $dropout rate = 0.0$ (heart
failure prediction). The only hyperparameters we modified were the number of
epochs and learning rate, the latter of which the authors had set as a step
function change at specific epochs. Given that we ran just a fraction of the
epochs (20 instead of 200), we had to adjust the learning rates accordingly: 
\begin{itemize}
  \item $LR_{diag} = 0.01$ for epochs 1 through 14,
  $1e-3$ for epochs 15 through 17, and $1e-5$ for epochs 18 through 20
  \item $LR_{hf} = 0.01$ for epochs 1 through
  1, $1e-3$ for epochs 2 through 2, $1e-4$ for epochs 3 through 3, and $1e-$ 4
  through 20
\end{itemize}

\subsection{Implementation}
In our preliminary reproduction implementation, we built Python notebooks
\href{https://github.com/willtsai/dlh-sp23-team53/blob/main/chet.ipynb}{chet.ipynb} 
for reproducing the main Chet model and 
\href{https://github.com/willtsai/dlh-sp23-team53/blob/main/cgl.ipynb}{cgl.ipynb}
for reproducing the baseline CGL model. The notebooks contain complete model
flows, including hyperparameters setting, data preprocessing, data loading,
model building, model training and evaluation. We reused the
\href{https://github.com/LuChang-CS/Chet}{author's code} in data preprocessing.
Our major coding efforts went into model rebuilding and the training/validation
flow. In training/validation part, we built our own training and validation
method to streamline the training, validation and test process while reusing the
existing schedulers and metrics. In model rebuilding part, we tried to follow
closely with the model structure and all the equations in the paper step by
step.

\subsection{Computational requirements}
All code are implemented with Python and PyTorch. For additional package and
version details, please refer to
\href{https://github.com/willtsai/dlh-sp23-team53/blob/main/requirements.txt}{requirements.txt}.
In our initial investigation, it took around 10 minutes for data preprocessing
and approximately 96 hours to complete total 200 epoches of training for a
combined MIMIC-III/MIMIC-IV training set on our local machine with 16GB memory
and Apple M1 PRO chip. In our reproduction implementation, it took 10 hours to
actual finish 10 epoches of training for a combined MIMIC-III/MIMIC-IV training
set on the same machine to get an initial result. In order to unblock the
computational constraints we have with the CPUs on our local machine, we would
like to further explore the following computational resources:
\begin{itemize}
  \item We will try to use the available GPUs(16 cores) on one of our local
  machines for the model training.
  \item We will explore with Google Colab to utilize their free standard
  GPUs(NVIDIA T4 Tensor Core GPUs) for the model training.
  \item We will explore with Microsoft Azure for available Virtual Machines with
  GPU using the available credit we have.
\end{itemize}

\begin{table*}[ht]
  \centering
  \begin{tabular}{lcccc|cccc} \hline \hline
    {\bf Models} & \multicolumn{4}{c}{\bf MIMIC-III} & \multicolumn{4}{c}{\bf MIMIC-IV} \\
    \cline{2-9} & \bf w-F1 & \bf R@10 & \bf R@20 & \# \bf Params & \bf w-F1 & \bf R@10 & \bf R@20 & \#
    \bf Params \\ \hline
    {Orig CGL} & 21.92 & 26.64 & 36.72 & 1.53M & 25.41 & 28.52 &
    37.15 & 1.83M \\
    {Orig Chet} & 22.63 & 28.64 & 37.87 & 2.12M 
    & 26.35 & 30.28 & 38.69 & 2.59M \\
    \hline
    {Repro CGL} & 20.69 & 24.91 & 34.69 & 1.51M & 23.53 & 26.97 &
    36.21 & 1.81M \\
    {Repro Chet} & \bf 21.45 & \bf 26.56 & \bf 36.33 & \bf 1.22M
    & \bf 24.47 & \bf 28.54 & \bf 37.50 & \bf 1.49MM \\ 
    \hline \hline
  \end{tabular}
  \caption{Diagnosis prediction results on MIMIC-III and MIMIC-IV using w-F1 (\%) and R@k (\%).}
  \label{tab:diag}
  \end{table*}

\begin{table*}[ht]
  \centering
  \begin{tabular}{lccc|ccc}
  \hline\hline
  {\bf Models} & \multicolumn{3}{c}{\bf MIMIC-III} & \multicolumn{3}{c}{\bf MIMIC-IV} \\
   \cline{2-7} & \bf AUC & \bf F1 & \# \bf Params & \bf AUC & \bf F1 & \# \bf Params \\ \hline
  {Orig CGL} & 84.19 & 71.77 & 0.55M & 89.05 & 69.36 & 0.60M \\
  {Orig Chet} & 86.14 & 73.08 & 0.68M & 90.83 & 71.14 & 0.88M \\
  \hline
  {Repro CGL} & 82.33 & 69.08 & 0.53M & 93.14 & 73.29 & 0.60M \\
  {Repro Chet} & \bf 85.09 & \bf 71.99 & \bf 0.47M & \bf 92.96 & \bf 74.37 & \bf 0.58M \\
  \hline\hline
  \end{tabular}
  \caption{Heart failure prediction results on MIMIC-III and MIMIC-IV using AUC (\%) and F1 (\%).}
  \label{tab:hf}
  \end{table*}

\section{Results}
We evaluated prediction performance for our models against the test datasets.
Our reproduction experiment results are from training each of our reproduced
Chet models once with 20 epochs instead of the 200 epochs originally used in the
paper. Our reproduced Chet model performance directionally aligns with that of
the original paper across both diagnosis and heart failure prediction tasks. We
make the same observation as the original paper that MIMIC-IV trained models
outperform MIMIC-III trained models in terms of AUC and F1 score. Our experiment
results indicate that our work supports the claims from the original paper that
Chet outperforms the most performant baseline model (CGL), with our results
falling within the same order of magnitude as the original paper for Chet model
performance improvement over CGL. Interestingly, the number of parameters in our
reproduction of Chet model were about 0.9M and 0.3M smaller than the originals
for diagnosis and heart failure prediction tasks, respectfully, which suggests
that our Chet model has reduced space complexity.

\subsection{Diagnosis Prediction Results}
Model performance results for the diagnosis prediction task from our preliminary
experiments are summarized in Table~\ref{tab:diag}. Compared to the 200-epoch
Chet model performance results from the original paper, our reproduced 20-epoch
Chet model performed only approximately 93-95\% as well in terms of F1 score,
R@10, and R@20; across both MIMIC-III and MIMIC-IV datasets. We observe similar
patterns for the original vs. reproduced CGL model performance. Thus, our
experiment results for the diagnosis prediction task confirm the original
paper's claim that Chet outperforms CGL, and by the same order of magnitude. 

\subsection{Heart Failure Prediction Results}
Model performance results for the heart failure prediction task from our
preliminary experiments are summarized in Table~\ref{tab:hf}. Compared to the
200-epoch Chet model performance results from the original paper, our reproduced
20-epoch Chet model performed about 99\% as well as the original in terms of AUC
and F1 score for the MIMIC-III dataset. Surprisingly, our Chet model
outperformed the original Chet model by a magnitude of 2\% in terms of AUC and
5\% in terms of F1 score for the MIMIC-IV dataset. We observe similar patterns
for the original vs. reproduced CGL model performance. We suspect that the
higher prediction performance can be attributed to the fact that the heart
failure prediction task is a much easier task than the diagnosis prediction
task. Intuitively, predicting a general diagnosis is much more ambiguous and
involves more complexity than predicting a single specific condition such as
heart failure. Indeed, we observe in the original paper that the prediction
performance for heart failure is much better than that of diagnosis prediction,
across all baseline models and Chet, for both MIMIC-III and MIMIC-IV datasets.
Our repro Chet models outperform our repro CGL models in all scenarios, with the
lone exception of AUC for MIMIC-IV, where CGL and Chet have similar performance
(CGL slightly outperforms by 0.19\%). Thus, we conclude our experiment results
for the heart failure prediction task confirm the original paper's claim that
Chet outperforms CGL by the same order of magnitude as the original paper, with
the exception of when measured by AUC on MIMIC-IV data. We suspect that this
might be due to random noise or fluctuations in the model training runs, which
the original paper normalizes by averaging over 5 training runs while we opted to 
only train each model once due to time constraints caused by compute capacity.

\subsection{Additional results not present in the original paper}
We plan to complete at least one ablation study from the original paper as well
as an additional ablation experiment of removing the attention from the final
patient embedding layer (not part of the original paper). We will include our
results for these additional experiments in the final report.
% Describe any additional experiments beyond the original paper. This could
% include experimenting with additional datasets, exploring different methods,
% running more ablations, or tuning the hyperparameters. For each additional
% experiment, clearly describe which experiment you conducted, its result, and
% discussions (e.g. what is the indication of the result).

\section{Discussion}
\#TODO
% Describe larger implications of the experimental results, whether the original
% paper was reproducible, and if it wasnâ€™t, what factors made it irreproducible. 

% Give your judgement on if you feel the evidence you got from running the code
% supports the claims of the paper. Discuss the strengths and weaknesses of your
% approach -- perhaps you didn't have time to run all the experiments, or
% perhaps you did additional experiments that further strengthened the claims in
% the paper.

% \subsection{What was easy \#TODO}
% % Describe which parts of your reproduction study were easy. E.g. was it easy to
% % run the author's code, or easy to re-implement their method based on the
% % description in the paper. The goal of this section is to summarize to the
% % reader which parts of the original paper they could easily apply to their
% % problem. 

% % Tips: Be careful not to give sweeping generalizations. Something that is easy
% % for you might be difficult to others. Put what was easy in context and explain
% % why it was easy (e.g. code had extensive API documentation and a lot of
% % examples that matched experiments in papers). 

% \subsection{What was difficult \#TODO}
% % Describe which parts of your reproduction study were difficult or took much
% % more time than you expected. Perhaps the data was not available and you
% % couldn't verify some experiments, or the author's code was broken and had to
% % be debugged first. Or, perhaps some experiments just take too much
% % time/resources to run and you couldn't verify them. The purpose of this
% % section is to indicate to the reader which parts of the original paper are
% % either difficult to re-use, or require a significant amount of work and
% % resources to verify. 

% % Tips: Be careful to put your discussion in context. For example, don't say
% % ``the math was difficult to follow,'' say ``the math requires advanced
% % knowledge of calculus to follow.'' 

% \subsection{Recommendations for reproducibility \#TODO}
% % Describe a set of recommendations to the original authors or others who work
% % in this area for improving reproducibility.

\section{Communication with original authors}
We communicated via email with one of the original authors, Chang Lu, who is
also the owner of the original Chet and CGL experiment code repos. We learned
through our correspondence that they had experimented with different LRs and
observed where the F1 scores began to degrade in their 200-epoch runs and
lowered the LR at those respective epochs. We conducted the same experiments to
determine the optimal LR step function scheduler for our own 20-epoch training
runs. Additionally, we confirmed with them the needed modifications we had to
make on the preprocessing code for CGL to exclude clinical notes from the input
data so that Chet and CGL can both be trained on the same cuts of MIMIC-IV data
for fair comparison.
% Document the extent of (or lack of) communication with the original authors.
% To make sure the reproducibility report is a fair assessment of the original
% research we recommend getting in touch with the original authors. You can ask
% authors specific questions, or if you don't have any questions you can send
% them the full report to get their feedback.


\bibliographystyle{acl_natbib}
\bibliography{dl4h-sp23-project-team53}

%\appendix



\end{document}
